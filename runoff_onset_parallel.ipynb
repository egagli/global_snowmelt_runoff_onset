{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create global snowmelt runoff onset product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"f135ce58-ff56-4b8f-84fa-31c5deeae8ce\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"851aed5d-ba94-454a-a0f4-83f3f6e70e03\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"79f45cfaa9fa45969442b7f8ccccb8e4\",\"client_comm_id\":\"987d69704d6c425cbd3758d4ebf334d3\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"851aed5d-ba94-454a-a0f4-83f3f6e70e03\",\"roots\":{\"p1002\":\"f135ce58-ff56-4b8f-84fa-31c5deeae8ce\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import easysnowdata\n",
    "import pystac_client\n",
    "from sys import getsizeof\n",
    "import tqdm\n",
    "import planetary_computer\n",
    "import adlfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import odc.stac\n",
    "import pathlib\n",
    "import configparser\n",
    "import shapely\n",
    "import time\n",
    "import gc\n",
    "import dask\n",
    "import dask.distributed\n",
    "import coiled\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import traceback\n",
    "import hvplot.xarray\n",
    "#odc.stac.configure_rio(cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_token = pathlib.Path('sas_token.txt').read_text()\n",
    "\n",
    "valid_tiles_gdf = gpd.read_file('valid_tiles.geojson')\n",
    "valid_tiles_gdf = valid_tiles_gdf.sort_values(by='percent_valid_snow_pixels',ascending=False) \n",
    "#valid_tiles_gdf.explore(column='percent_valid_snow_pixels')\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read('global_config.txt')\n",
    "resolution = config.getfloat('VALUES', 'resolution')\n",
    "zarr_chunk_size = (config.getint('VALUES', 'chunk_size'),config.getint('VALUES', 'chunk_size'))\n",
    "bbox_left = config.getfloat('VALUES', 'bbox_left')\n",
    "bbox_right = config.getfloat('VALUES', 'bbox_right')\n",
    "bbox_top = config.getfloat('VALUES', 'bbox_top')\n",
    "bbox_bottom = config.getfloat('VALUES', 'bbox_bottom')\n",
    "\n",
    "WY_start = config.getint('VALUES', 'WY_start') \n",
    "WY_end = config.getint('VALUES', 'WY_end') \n",
    "water_years = np.arange(WY_start, WY_end + 1)\n",
    "\n",
    "min_years_for_median_std = config.getint('VALUES', 'min_years_for_median_std')\n",
    "\n",
    "min_monthly_acquisitions = config.getint('VALUES', 'min_monthly_acquisitions')\n",
    "max_allowed_days_gap_per_orbit = config.getint('VALUES', 'max_allowed_days_gap_per_orbit')\n",
    "low_backscatter_threshold = config.getfloat('VALUES', 'low_backscatter_threshold')\n",
    "\n",
    "start_date = '2014-01-01'\n",
    "today = pd.Timestamp.today().strftime('%Y-%m-%d')\n",
    "end_date = today\n",
    "\n",
    "print(f'Config loaded: \\n{resolution=}, \\n{zarr_chunk_size=}, \\n{bbox_left=}, \\n{bbox_right=}, \\n{bbox_top=}, \\n{bbox_bottom=}, \\n{start_date=}, \\n{end_date=} \\n{water_years=}, \\n{min_years_for_median_std=}, \\n{low_backscatter_threshold=}, \\n{min_monthly_acquisitions=}, \\n{max_allowed_days_gap_per_orbit=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_tiles_gdf.explore(column='percent_valid_snow_pixels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_geobox = odc.geo.geobox.GeoBox.from_bbox((bbox_left, bbox_bottom,\n",
    "   bbox_right, bbox_top),crs=\"epsg:4326\", resolution=resolution)\n",
    "\n",
    "geobox_tiles = odc.geo.geobox.GeoboxTiles(global_geobox,zarr_chunk_size)\n",
    "\n",
    "global_geobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel1_rtc(geobox):\n",
    "\n",
    "    chunks_read = {\"x\": 2048, \"y\": 2048, \"time\": 1}\n",
    "\n",
    "    items = (\n",
    "        pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",modifier=planetary_computer.sign_inplace)\n",
    "        .search(\n",
    "            intersects=geobox.geographic_extent,\n",
    "            collections=[\"sentinel-1-rtc\"],\n",
    "            datetime=(start_date, end_date),\n",
    "        )\n",
    "        .item_collection()\n",
    "    )\n",
    "\n",
    "    load_params = {\n",
    "        \"items\": items,\n",
    "        \"nodata\": -32768,\n",
    "        \"chunks\": chunks_read,\n",
    "        \"groupby\": \"sat:absolute_orbit\",\n",
    "        \"geobox\":geobox,\n",
    "        \"resampling\": \"bilinear\",\n",
    "        #\"fail_on_error\":False\n",
    "    }\n",
    "\n",
    "\n",
    "    s1_rtc_ds = odc.stac.load(**load_params).sortby(\"time\")#.chunk(chunks_compute) # rechunk?\n",
    "\n",
    "    metadata = gpd.GeoDataFrame.from_features(items, \"epsg:4326\")\n",
    "\n",
    "    metadata_groupby_gdf = (\n",
    "        metadata.groupby([\"sat:absolute_orbit\"]).first().sort_values(\"datetime\")\n",
    "    )\n",
    "\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"sat:orbit_state\": (\"time\", metadata_groupby_gdf[\"sat:orbit_state\"]),\n",
    "        \"sat:relative_orbit\": (\"time\", metadata_groupby_gdf[\"sat:relative_orbit\"].astype(\"int16\"))\n",
    "    })\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.drop_vars(['hh','hv'],errors='ignore')\n",
    "\n",
    "    epsg = s1_rtc_ds.rio.estimate_utm_crs().to_epsg()\n",
    "    hemisphere = 'northern' if epsg < 32700 else 'southern'\n",
    "\n",
    "    s1_rtc_ds.attrs['hemisphere'] = hemisphere\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"water_year\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere))),\n",
    "        \"DOWY\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)))\n",
    "    })        \n",
    "\n",
    "    return s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_masks(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds):\n",
    "\n",
    "    s1_rtc_ds = remove_unwanted_water_years(s1_rtc_ds)\n",
    "\n",
    "    center_lat = (s1_rtc_ds.rio.bounds()[1]+s1_rtc_ds.rio.bounds()[3])/2\n",
    "    if np.absolute(center_lat) < 3:\n",
    "        s1_rtc_ds = remove_equator_crossing(s1_rtc_ds)\n",
    "        \n",
    "    s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry) # does this compute?\n",
    "\n",
    "    s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "    \n",
    "    return s1_rtc_masked_ds\n",
    "\n",
    "def remove_unwanted_water_years(s1_rtc_ds):\n",
    "    s1_rtc_ds = s1_rtc_ds.sel(time=s1_rtc_ds.water_year.isin(water_years))\n",
    "    return s1_rtc_ds\n",
    "\n",
    "\n",
    "def remove_equator_crossing(s1_rtc_ds):\n",
    "    if s1_rtc_ds.attrs['hemisphere'] == 'northern':\n",
    "        mask = s1_rtc_ds.latitude >= 0\n",
    "    else:\n",
    "        mask = s1_rtc_ds.latitude < 0\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.where(mask)\n",
    "    return s1_rtc_ds\n",
    "\n",
    "def get_gmba_mountain_inventory(bbox_gdf):\n",
    "    url = (f\"https://data.earthenv.org/mountains/standard/GMBA_Inventory_v2.0_standard_300.zip\")\n",
    "    gmba_gdf = gpd.read_file(\"zip+\" + url)\n",
    "    gmba_clipped_gdf = gpd.clip(gmba_gdf, bbox_gdf)\n",
    "    return gmba_clipped_gdf\n",
    "\n",
    "def get_custom_seasonal_snow_mask(s1_rtc_ds,bbox_gdf):\n",
    "    mask_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snow_mask_v2/global_modis_snow_mask.zarr\")\n",
    "    seasonal_snow_mask = xr.open_zarr(mask_store, consolidated=True, decode_coords='all') \n",
    "    seasonal_snow_mask_clip_ds = seasonal_snow_mask.rio.clip_box(*bbox_gdf.total_bounds,crs='EPSG:4326') # clip to correct box, maybe use total_bounds and then use crs \n",
    "    seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=0)).rename({'x':'longitude','y':'latitude'})\n",
    "    return seasonal_snow_mask_matched_ds\n",
    "\n",
    "\n",
    "def apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds):\n",
    "    s1_rtc_masked_ds = s1_rtc_ds.groupby('water_year').map(lambda group: apply_mask_for_year(group, seasonal_snow_mask_matched_ds))\n",
    "    s1_rtc_masked_ds.rio.write_crs(s1_rtc_ds.rio.crs,inplace=True)\n",
    "    return s1_rtc_masked_ds\n",
    "\n",
    "def apply_mask_for_year(group, seasonal_snow_mask_matched_ds):\n",
    "\n",
    "    year = group.water_year.values[0]\n",
    "\n",
    "\n",
    "    if year not in seasonal_snow_mask_matched_ds.water_year:\n",
    "        print(f\"Warning: water_year {year} not found in seasonal_snow_mask_matched_ds\")\n",
    "        return group.where(False) \n",
    "\n",
    "    sad_mask = group['DOWY'] >= seasonal_snow_mask_matched_ds['SAD_DOWY'].sel(water_year=year)\n",
    "    sdd_mask = group['DOWY'] <= seasonal_snow_mask_matched_ds['SDD_DOWY'].sel(water_year=year)\n",
    "    consec_mask = seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=year) >= 56\n",
    "    combined_mask = sad_mask & sdd_mask & consec_mask\n",
    "    return group.where(combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_datetime_to_DOWY(date_da, hemisphere=\"northern\"):\n",
    "    \"\"\"\n",
    "    Converts an xarray DataArray containing datetime objects to the Day of Water Year (DOWY).\n",
    "\n",
    "    Parameters:\n",
    "    date (xr.DataArray): An xarray DataArray with datetime64 data type.\n",
    "    hemisphere (str): 'northern' or 'southern'\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray: An xarray DataArray containing the DOWY for each datetime in the input DataArray.\n",
    "    \"\"\"\n",
    "\n",
    "    if date_da.attrs.get(\"any_valid_date\") is not None:\n",
    "        any_valid_date = pd.to_datetime(date_da.attrs[\"any_valid_date\"])\n",
    "    else:\n",
    "        any_valid_date = pd.to_datetime(date_da.sel(x=0, y=0, method=\"nearest\").values)\n",
    "\n",
    "    start_of_water_year = easysnowdata.utils.get_water_year_start(\n",
    "        any_valid_date, hemisphere=hemisphere\n",
    "    )\n",
    "\n",
    "    return xr.apply_ufunc(\n",
    "        lambda x: (x - start_of_water_year).days + 1,  # dt accessor?\n",
    "        date_da,\n",
    "        input_core_dims=[[]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",  # try allowed also\n",
    "        output_dtypes=[float],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runoff_onset(\n",
    "    s1_rtc_ds: xr.Dataset,\n",
    "    min_monthly_acquisitions: int,\n",
    "    max_allowed_days_gap_per_orbit: int,\n",
    "    consec_snow_days_da: xr.DataArray,\n",
    "    return_constituent_runoff_onsets: bool = False,\n",
    "    returned_dates_format: str = \"dowy\",\n",
    "    low_backscatter_threshold: float = 0.001,\n",
    "    report_temporal_res: bool = False,\n",
    "):\n",
    "\n",
    "\n",
    "    s1_rtc_ds = remove_bad_scenes_and_border_noise(s1_rtc_ds, low_backscatter_threshold)\n",
    "\n",
    "    #pixelwise_counts_per_orbit_and_polarization_ds = (count_acquisitions_per_orbit_and_polarization(s1_rtc_ds))  # this should be for melt, keeping general for now to integrate modis data\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds, max_days_gap_per_orbit_da = count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds)\n",
    "\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds = (calculate_backscatter_min_per_orbit(s1_rtc_ds))\n",
    "\n",
    "\n",
    "    if report_temporal_res:\n",
    "        constituent_runoff_onsets_da, temporal_resolution, pixel_count = ( # , temporal_resolution, pixel_count\n",
    "            filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "                backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "                pixelwise_counts_per_orbit_and_polarization_ds,\n",
    "                max_days_gap_per_orbit_da,\n",
    "                consec_snow_days_da,\n",
    "                min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit,\n",
    "                report_temporal_res\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        constituent_runoff_onsets_da = ( # , temporal_resolution, pixel_count\n",
    "            filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "                backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "                pixelwise_counts_per_orbit_and_polarization_ds,\n",
    "                max_days_gap_per_orbit_da,\n",
    "                consec_snow_days_da,\n",
    "                min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit,\n",
    "                report_temporal_res\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if return_constituent_runoff_onsets == False:\n",
    "        runoff_onset_da = calculate_runoff_onset_from_constituent_runoff_onsets(constituent_runoff_onsets_da)\n",
    "    else:\n",
    "        runoff_onset_da = constituent_runoff_onsets_da\n",
    "\n",
    "\n",
    "    if returned_dates_format == \"dowy\":\n",
    "\n",
    "        hemisphere = (\n",
    "            \"northern\"\n",
    "            if s1_rtc_ds.rio.estimate_utm_crs().to_epsg() < 32700\n",
    "            else \"southern\"\n",
    "        )\n",
    "        month_start = 10 if hemisphere == \"northern\" else 4\n",
    "        print(\n",
    "            f\"Area is in the {hemisphere} hemisphere. Water year starts in month {month_start}.\"\n",
    "        )\n",
    "        runoff_onset_da.attrs[\"any_valid_date\"] = s1_rtc_ds.time[0].values\n",
    "        runoff_onset_da = xr_datetime_to_DOWY(runoff_onset_da, hemisphere=hemisphere)\n",
    "\n",
    "    elif returned_dates_format == \"doy\":\n",
    "        runoff_onset_da = runoff_onset_da.dt.dayofyear\n",
    "    elif returned_dates_format == \"datetime64\":\n",
    "        runoff_onset_da = runoff_onset_da\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'returned_dates_format must be either \"doy\", \"dowy\", or \"datetime64\".'\n",
    "        )\n",
    "\n",
    "    if report_temporal_res:\n",
    "        return runoff_onset_da, temporal_resolution, pixel_count  \n",
    "    else:\n",
    "        return runoff_onset_da\n",
    "\n",
    "def remove_bad_scenes_and_border_noise(da, threshold):\n",
    "    cutoff_date = np.datetime64('2018-03-14')\n",
    "    \n",
    "    original_crs = da.rio.crs\n",
    "    \n",
    "    result = xr.where(\n",
    "        da.time < cutoff_date,\n",
    "        da.where(da > threshold),\n",
    "        da.where(da > 0)\n",
    "    )\n",
    "    \n",
    "    result.rio.write_crs(original_crs, inplace=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# def count_acquisitions_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "#     print(\"Calculating pixelwise counts per orbit and polarization...\")\n",
    "#     pixelwise_counts_per_orbit_and_polarization = s1_rtc_ds.groupby(\n",
    "#         \"sat:relative_orbit\"\n",
    "#     ).count(dim=\"time\", engine='flox')\n",
    "#     return pixelwise_counts_per_orbit_and_polarization\n",
    "\n",
    "def count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "    print(\"Calculating pixelwise counts and maximum gaps per orbit and polarization...\")\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds = s1_rtc_ds.groupby(\"sat:relative_orbit\").count(dim=\"time\", engine='flox')\n",
    "    \n",
    "    def calc_max_gap(group):\n",
    "            times = group.time.sortby('time')\n",
    "            if len(times) == 1: # if only one scene in this group, set gap to very large number so it won't be calculated\n",
    "                return times.count()*9999\n",
    "            gaps = times.diff(dim='time').max().dt.days\n",
    "            return gaps\n",
    "\n",
    "    max_time_gap_per_orbit_days_da = s1_rtc_ds.groupby(\"sat:relative_orbit\").map(calc_max_gap)\n",
    "    \n",
    "    return pixelwise_counts_per_orbit_and_polarization_ds, max_time_gap_per_orbit_days_da\n",
    "\n",
    "\n",
    "def calculate_backscatter_min_per_orbit(s1_rtc_ds: xr.Dataset):\n",
    "    print(\"Calculating backscatter min per orbit...\")\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds = s1_rtc_ds.groupby(\n",
    "        \"sat:relative_orbit\"\n",
    "    ).map(lambda c: c.idxmin(dim=\"time\"))\n",
    "    return backscatter_min_timing_per_orbit_and_polarization_ds\n",
    "\n",
    "\n",
    "# def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#     backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "#     pixelwise_counts_per_orbit: xr.Dataset,\n",
    "#     consec_snow_days_da: xr.DataArray,\n",
    "#     min_monthly_acquisitions: int,\n",
    "# ):\n",
    "    \n",
    "#     print(f\"Filtering insufficient pixels per orbit and polarization, must have at least {min_monthly_acquisitions} per month...\")\n",
    "#     constituent_runoff_onsets_ds = (\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds.where(\n",
    "#             pixelwise_counts_per_orbit >= (min_monthly_acquisitions*(consec_snow_days_da/30))\n",
    "#         )\n",
    "#     )\n",
    "#     return constituent_runoff_onsets_ds\n",
    "\n",
    "def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "    max_days_gap_per_orbit_da: xr.DataArray,\n",
    "    consec_snow_days_da: xr.DataArray,\n",
    "    min_monthly_acquisitions: int,\n",
    "    max_allowed_days_gap_per_orbit: int,\n",
    "    report_temporal_res: bool,\n",
    "):\n",
    "    print(f\"Filtering insufficient pixels per orbit and polarization...\")\n",
    "\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds = pixelwise_counts_per_orbit_and_polarization_ds.persist()\n",
    "    insufficient_mask = (pixelwise_counts_per_orbit_and_polarization_ds >= (min_monthly_acquisitions*(consec_snow_days_da/30))) & (max_days_gap_per_orbit_da <= max_allowed_days_gap_per_orbit) & (pixelwise_counts_per_orbit_and_polarization_ds>0)\n",
    "    \n",
    "\n",
    "    constituent_runoff_onsets_ds = backscatter_min_timing_per_orbit_and_polarization_ds.where(insufficient_mask)\n",
    "    constituent_runoff_onsets_da = constituent_runoff_onsets_ds.to_dataarray(dim=\"polarization\")\n",
    "\n",
    "    if not report_temporal_res:\n",
    "        return constituent_runoff_onsets_da\n",
    "    else:\n",
    "        temporal_resolution_da = consec_snow_days_da / (pixelwise_counts_per_orbit_and_polarization_ds.where(insufficient_mask)['vv'].sum(dim='sat:relative_orbit').where(lambda x: x>0))\n",
    "        temporal_resolution = temporal_resolution_da.mean(dim=['latitude','longitude'],skipna=True)\n",
    "        pixel_count = temporal_resolution_da.count(dim=['latitude','longitude'])\n",
    "        return constituent_runoff_onsets_da, temporal_resolution, pixel_count\n",
    "\n",
    "\n",
    "def calculate_runoff_onset_from_constituent_runoff_onsets(constituent_runoff_onsets_da: xr.DataArray,):\n",
    "    print(\"Calculating runoff onset from constituent runoff onsets...\")\n",
    "    runoff_onset_da = (\n",
    "        constituent_runoff_onsets_da.astype(\"int64\")\n",
    "        .where(lambda x: x > 0)\n",
    "        .median(dim=[\"sat:relative_orbit\", \"polarization\"], skipna=True)\n",
    "        .astype(\"datetime64[ns]\")\n",
    "    )  #\n",
    "    return runoff_onset_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runoff_onset_wrapper(ds, consec_snow_days_da, min_monthly_acquisitions, max_allowed_days_gap_per_orbit, returned_dates_format, return_constituent_runoff_onsets, low_backscatter_threshold, report_temporal_res, tile):\n",
    "    \n",
    "    water_year = ds.water_year.values[0]\n",
    "\n",
    "    print(f'calculating for WY {water_year}...')\n",
    "\n",
    "    if water_year not in consec_snow_days_da.water_year:\n",
    "        print(f\"Warning: water_year {water_year} not found in consec_snow_days_da\")\n",
    "        consec_snow_days_slice = consec_snow_days_da.sel(water_year=water_year, method='nearest').where(False,other=9999) # if water year does not exist in the consec_snow_days_da, set to 9999 so no values are calculated in calculate_runoff_onset...\n",
    "    else:\n",
    "        consec_snow_days_slice = consec_snow_days_da.sel(water_year=water_year)\n",
    "    \n",
    "    if report_temporal_res:\n",
    "        runoff_onset_da, temporal_resolution, pixel_count = calculate_runoff_onset( #, temporal_resolution, pixel_count\n",
    "            ds,\n",
    "            consec_snow_days_da=consec_snow_days_slice,\n",
    "            min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "            max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "            returned_dates_format=returned_dates_format,\n",
    "            return_constituent_runoff_onsets=return_constituent_runoff_onsets,\n",
    "            low_backscatter_threshold=low_backscatter_threshold,\n",
    "            report_temporal_res=report_temporal_res,\n",
    "        )\n",
    "\n",
    "        setattr(tile, f'tr_{water_year}', round(float(temporal_resolution),3))\n",
    "        setattr(tile, f'pix_ct_{water_year}', int(pixel_count))\n",
    "    else:\n",
    "        runoff_onset_da = calculate_runoff_onset( #, temporal_resolution, pixel_count\n",
    "            ds,\n",
    "            consec_snow_days_da=consec_snow_days_slice,\n",
    "            min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "            max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "            returned_dates_format=returned_dates_format,\n",
    "            return_constituent_runoff_onsets=return_constituent_runoff_onsets,\n",
    "            low_backscatter_threshold=low_backscatter_threshold,\n",
    "            report_temporal_res=report_temporal_res,\n",
    "        )\n",
    "    \n",
    "    return runoff_onset_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataarrays_to_dataset(runoff_onsets_da, median_da, std_da):\n",
    "\n",
    "    runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset').round().astype('uint16')\n",
    "    runoff_onsets_ds = runoff_onsets_ds.reindex(water_year=water_years)\n",
    "    runoff_onsets_ds['runoff_onset_median'] = median_da.round().astype('uint16')\n",
    "    runoff_onsets_ds['runoff_onset_std'] = std_da\n",
    "    \n",
    "    return runoff_onsets_ds\n",
    "\n",
    "def median_and_std_with_min_obs(da, dim, min_count):\n",
    "    count_mask = da.notnull().sum(dim=dim) >= min_count\n",
    "    \n",
    "    median = da.where(count_mask).median(dim=dim)\n",
    "    std = da.where((count_mask) & (median>0)).std(dim=dim)\n",
    "    \n",
    "    return median, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tile:\n",
    "    def __init__(self, row, col):\n",
    "        self.row = row\n",
    "        self.col = col\n",
    "        self.index = row,col\n",
    "        self.percent_valid_snow_pixels = self.get_percent_valid_snow_pixels()\n",
    "        self.geobox = self.get_geobox()\n",
    "        self.bbox_gdf = self.get_bbox_gdf()\n",
    "        self.start_time = None\n",
    "        self.total_time = None\n",
    "        self.s1_rtc_ds = None\n",
    "        self.s1_rtc_ds_dims = None\n",
    "        self.s1_rtc_masked_ds_dims = None\n",
    "        self.runoff_onsets = None\n",
    "        self.runoff_onsets_dims = None\n",
    "        self.tr_2015 = None\n",
    "        self.tr_2016 = None\n",
    "        self.tr_2017 = None\n",
    "        self.tr_2018 = None\n",
    "        self.tr_2019 = None\n",
    "        self.tr_2020 = None\n",
    "        self.tr_2021 = None\n",
    "        self.tr_2022 = None\n",
    "        self.tr_2023 = None\n",
    "        self.tr_2024 = None\n",
    "        self.pix_ct_2015 = None\n",
    "        self.pix_ct_2016 = None\n",
    "        self.pix_ct_2017 = None\n",
    "        self.pix_ct_2018 = None\n",
    "        self.pix_ct_2019 = None\n",
    "        self.pix_ct_2020 = None\n",
    "        self.pix_ct_2021 = None\n",
    "        self.pix_ct_2022 = None\n",
    "        self.pix_ct_2023 = None\n",
    "        self.pix_ct_2024 = None\n",
    "        self.error_messages = []\n",
    "        self.success = False\n",
    "\n",
    "    \n",
    "    def get_geobox(self):\n",
    "        return geobox_tiles[self.index]\n",
    "    \n",
    "    def get_bbox_gdf(self):\n",
    "        bbox = self.geobox.boundingbox\n",
    "        bbox_geometry = shapely.geometry.box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "        bbox_gdf = gpd.GeoDataFrame(geometry=[bbox_geometry], crs=self.geobox.crs)\n",
    "        return bbox_gdf\n",
    "    \n",
    "    def get_percent_valid_snow_pixels(self):\n",
    "        return float(valid_tiles_gdf['percent_valid_snow_pixels'].loc[(valid_tiles_gdf['row'] == self.row) & (valid_tiles_gdf['col'] == self.col)].values[0])\n",
    "    \n",
    "    def get_info(self):\n",
    "        return f\"Processing Tile {self.index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile: Tile):\n",
    "    @dask.delayed\n",
    "    def _process():\n",
    "        tile.start_time = time.time()\n",
    "        geobox = tile.geobox\n",
    "        bbox_gdf = tile.bbox_gdf\n",
    "        \n",
    "        try:\n",
    "\n",
    "            s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "            tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "            seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds, bbox_gdf)\n",
    "\n",
    "            gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "\n",
    "            s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds, gmba_clipped_gdf, seasonal_snow_mask_matched_ds)\n",
    "\n",
    "            runoff_onsets_da = (\n",
    "                s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "                .apply(\n",
    "                    calculate_runoff_onset_wrapper,\n",
    "                    consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "                    min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "                    max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "                    returned_dates_format=\"dowy\",\n",
    "                    return_constituent_runoff_onsets=False,\n",
    "                    low_backscatter_threshold=low_backscatter_threshold,\n",
    "                    report_temporal_res=False,\n",
    "                    tile=tile,\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            tile.runoff_onsets_dims = dict(runoff_onsets_da.sizes)\n",
    "            median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', min_years_for_median_std)\n",
    "            runoff_onsets_ds = dataarrays_to_dataset(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "            global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "            \n",
    "            global_ds = xr.open_zarr(global_store, consolidated=True)\n",
    "            global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude, longitude=runoff_onsets_ds.longitude, method='nearest')\n",
    "            \n",
    "            runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude, longitude=global_subset_ds.longitude)\n",
    "            \n",
    "            # Write to Zarr\n",
    "            runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(\n",
    "                global_store, region=\"auto\", mode=\"r+\", consolidated=True\n",
    "            )\n",
    "            \n",
    "            tile.total_time = time.time() - tile.start_time\n",
    "            tile.success = True\n",
    "        except Exception as e:\n",
    "            tile.error_messages.append(str(e))\n",
    "            tile.error_messages.append(traceback.format_exc())\n",
    "            tile.total_time = time.time() - tile.start_time\n",
    "            tile.success = False\n",
    "        \n",
    "        return tile\n",
    "    \n",
    "    return _process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile: Tile):\n",
    "    tile.start_time = time.time()\n",
    "    geobox = tile.geobox\n",
    "    bbox_gdf = tile.bbox_gdf\n",
    "    \n",
    "    try:\n",
    "\n",
    "        s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "        s1_rtc_ds = s1_rtc_ds.persist()\n",
    "        \n",
    "        tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "        if tile.s1_rtc_ds_dims['time'] > 1700:\n",
    "            tile.error_messages.append('Over 1700 scenes, process with more worker RAM')\n",
    "            tile.total_time = time.time() - tile.start_time\n",
    "            tile.success = False\n",
    "            return tile\n",
    "\n",
    "        seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds, bbox_gdf)\n",
    "\n",
    "        gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "\n",
    "        s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds, gmba_clipped_gdf, seasonal_snow_mask_matched_ds)\n",
    "\n",
    "\n",
    "        runoff_onsets_da = (\n",
    "            s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "            .apply(\n",
    "                calculate_runoff_onset_wrapper,\n",
    "                consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "                min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "                returned_dates_format=\"dowy\",\n",
    "                return_constituent_runoff_onsets=False,\n",
    "                low_backscatter_threshold=low_backscatter_threshold,\n",
    "                report_temporal_res=False,\n",
    "                tile=tile,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        tile.runoff_onsets_dims = dict(runoff_onsets_da.sizes)\n",
    "        median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', min_years_for_median_std)\n",
    "        runoff_onsets_ds = dataarrays_to_dataset(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "        global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "        \n",
    "        global_ds = xr.open_zarr(global_store, consolidated=True)\n",
    "        global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude, longitude=runoff_onsets_ds.longitude, method='nearest')\n",
    "        \n",
    "        runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude, longitude=global_subset_ds.longitude)\n",
    "        \n",
    "        # Write to Zarr\n",
    "        runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(\n",
    "            global_store, region=\"auto\", mode=\"r+\", consolidated=True\n",
    "        )\n",
    "        \n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = True\n",
    "    except Exception as e:\n",
    "        tile.error_messages.append(str(e))\n",
    "        tile.error_messages.append(traceback.format_exc())\n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = False\n",
    "    \n",
    "    return tile\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                         #shutdown_on_close=False,\n",
    "                         #wait_for_workers=True,\n",
    "                         #n_workers=[41,170], # 170\n",
    "                         #n_workers=[31,86],\n",
    "                         n_workers=41,\n",
    "                         #n_workers=8,\n",
    "                         #n_workers=10,\n",
    "                         worker_memory=\"32 GB\", #coiled.list_instance_types(backend=\"azure\")\n",
    "                         worker_cpu=4,\n",
    "                         #worker_options={\"nthreads\": 8},# 16 8 4 oversubscribe?\n",
    "                         #scheduler_memory=\"128 GB\",\n",
    "                         scheduler_memory=\"128 GB\",\n",
    "                         spot_policy=\"spot\", # spot usually\n",
    "                         #software=\"sar_snowmelt_timing\",\n",
    "                         environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                         #container=\"mcr.microsoft.com/planetary-computer/python:latest\",\n",
    "                         workspace=\"azure\",\n",
    "                         \n",
    "                         )\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "#use the following config for the problem tiles, otherwise 4 and 32\n",
    "                        #  worker_memory=\"64 GB\", \n",
    "                        #  worker_cpu=8,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK CLIP_BOX issue before running again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "odc.stac.configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_tiles = [Tile(row,col) for row,col in zip(valid_tiles_gdf.row,valid_tiles_gdf.col)]\n",
    "\n",
    "header_written = False\n",
    "\n",
    "if os.path.exists('tile_results.csv'): # make sure to delete tile-results.csv to reprocess\n",
    "    header_written = True\n",
    "\n",
    "    processed_tiles_df = pd.read_csv('tile_results.csv')\n",
    "    #processed_tiles_df = processed_tiles_df[processed_tiles_df['success'] == True] # uncomment this line to reprocess failed tiles\n",
    "\n",
    "    processed_tiles = set(zip(processed_tiles_df['row'], processed_tiles_df['col']))\n",
    "\n",
    "    \n",
    "    tiles = [tile for tile in initial_tiles if (tile.row, tile.col) not in processed_tiles]\n",
    "else:\n",
    "    tiles = initial_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\n",
    "            \"tr_2015\", \"tr_2016\", \"tr_2017\", \"tr_2018\", \"tr_2019\", \"tr_2020\", \"tr_2021\", \"tr_2022\", \"tr_2023\",\"tr_2024\",\n",
    "            \"pix_ct_2015\",\"pix_ct_2016\",\"pix_ct_2017\",\"pix_ct_2018\",\"pix_ct_2019\",\"pix_ct_2020\",\"pix_ct_2021\",\"pix_ct_2022\",\"pix_ct_2023\",\"pix_ct_2024\",\n",
    "            \"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "for tile_batch in tqdm.tqdm(tile_batches, total=len(tile_batches)):\n",
    "\n",
    "    futures = []\n",
    "    for tile in tile_batch:\n",
    "        futures.append(client.submit(process_tile, tile, retries=0))\n",
    "    computed_results = client.gather(futures,errors='skip')\n",
    "\n",
    "    for tile in tile_batch:\n",
    "        if tile.index not in [computed_tile.index for computed_tile in computed_results]:\n",
    "            computed_results.append(tile)\n",
    "    \n",
    "    # batch_results = [process_tile(tile) for tile in tile_batch]\n",
    "    # try:    \n",
    "    #     computed_results = dask.compute(*batch_results)\n",
    "    # except:\n",
    "    #     computed_results = tile_batch\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        [[getattr(r, f) for f in fields] for r in computed_results if r is not None],\n",
    "        columns=fields,\n",
    "    )\n",
    "\n",
    "    df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "    header_written = True\n",
    "    \n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import as_completed\n",
    "\n",
    "# fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\n",
    "#             \"tr_2015\", \"tr_2016\", \"tr_2017\", \"tr_2018\", \"tr_2019\", \"tr_2020\", \"tr_2021\", \"tr_2022\", \"tr_2023\",\"tr_2024\",\n",
    "#             \"pix_ct_2015\",\"pix_ct_2016\",\"pix_ct_2017\",\"pix_ct_2018\",\"pix_ct_2019\",\"pix_ct_2020\",\"pix_ct_2021\",\"pix_ct_2022\",\"pix_ct_2023\",\"pix_ct_2024\",\n",
    "#             \"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "# futures = [client.submit(process_tile, tile, retries=0) for tile in tiles]\n",
    "\n",
    "# with tqdm.tqdm(total=len(tiles)) as pbar:\n",
    "#     for future in as_completed(futures):\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             df = pd.DataFrame([[getattr(result, f) for f in fields]], columns=fields)\n",
    "#         except Exception as e:\n",
    "#             # Handle the error, possibly by adding the original tile to the results\n",
    "#             error_tile = next(tile for tile in tiles if tile.index == future.key.split('-')[1])\n",
    "#             error_tile.error_messages = str(e)\n",
    "#             df = pd.DataFrame([[getattr(error_tile, f) for f in fields]], columns=fields)\n",
    "\n",
    "#         df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#         header_written = True\n",
    "#         pbar.update(1)\n",
    "\n",
    "#     client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "global_ds = xr.open_zarr(global_store, consolidated=True,decode_coords='all')\n",
    "\n",
    "def view_tile(tile: Tile):\n",
    "\n",
    "\n",
    "    test_ds = global_ds.rio.clip_box(*tile.get_geobox().boundingbox,crs='EPSG:4326')\n",
    "\n",
    "    f,ax=plt.subplots(1,2,figsize=(10,10))\n",
    "    test_ds['runoff_onset_median'].plot.imshow(ax=ax[0],vmin=0,vmax=365)\n",
    "\n",
    "    test_ds['runoff_onset_std'].plot.imshow(ax=ax[1],cmap='Reds')\n",
    "\n",
    "    test_ds['runoff_onset'].plot.imshow(col='water_year',col_wrap=3,vmin=0,vmax=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_tile(initial_tiles[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code graveyeard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True,decode_coords='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile = Tile(16,204)\n",
    "# tile = Tile(16,203)\n",
    "# futures = []\n",
    "# futures.append(client.submit(process_tile, tile, retries=1))\n",
    "# futures\n",
    "# computed_results = client.gather(futures,errors='skip')\n",
    "# computed_results[0].error_messages\n",
    "# computed_results[0].runoff_onsets_dims\n",
    "# test_ds = global_ds.rio.clip_box(*tile.get_geobox().boundingbox,crs='EPSG:4326')\n",
    "# test_ds\n",
    "# test_ds['runoff_onset'].sum(dim=['latitude','longitude']).values\n",
    "# view_tile(tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_ds['runoff_onset_median'].odc.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles = [Tile(row,col) for row,col in zip(valid_tiles_gdf.row,valid_tiles_gdf.col)]\n",
    "# tile = tiles[6]\n",
    "# geobox = tile.get_geobox()\n",
    "# geobox.explore()\n",
    "# bbox = geobox.boundingbox\n",
    "# bbox_geometry = shapely.geometry.box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "# bbox_gdf = gpd.GeoDataFrame(geometry=[bbox_geometry], crs=geobox.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "# s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "# s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry)\n",
    "# s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_snow_mask_clip_ds = get_custom_seasonal_snow_mask(bbox_gdf)\n",
    "# seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=0)).rename({'x':'longitude','y':'latitude'})\n",
    "# #seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.odc.reproject(geobox).persist()#.rename({'x':'longitude','y':'latitude'})\n",
    "# seasonal_snow_mask_matched_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "# s1_rtc_masked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_masked_vv_WY2019_ds = s1_rtc_masked_ds['vv'].sel(time=slice('2018-10-01','2019-09-30')).compute()\n",
    "# s1_rtc_masked_vv_WY2019_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_rtc_masked_vv_WY2019_ds.plot.imshow(col='time',col_wrap=6,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_rtc_masked_vv_WY2019_ds.where(lambda x:x>0.001).plot.imshow(col='time',col_wrap=6,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runoff_onsets = (\n",
    "# s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "# .apply(\n",
    "#     calculate_runoff_onset_wrapper,\n",
    "#     consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "#     min_monthly_acquisitions=1, #one or two\n",
    "#     returned_dates_format=\"dowy\",\n",
    "#     return_constituent_runoff_onsets=True,\n",
    "#     low_backscatter_threshold=0.001#0.001\n",
    "# ))\n",
    "#runoff_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runoff_onsets.sel(water_year=2019).plot.imshow(col='sat:relative_orbit',row='polarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getsizeof(futures[0].result())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# runoff_onsets\n",
    "# runoff_onsets_computed = runoff_onsets.to_dataset(name='runoff_onset').persist()# add .persist here?\n",
    "# runoff_onsets_computed\n",
    "# runoff_onsets_computed['runoff_onset_median'] = median_with_min_obs(runoff_onsets_computed['runoff_onset'], 'water_year', 3)\n",
    "# runoff_onsets_computed\n",
    "# runoff_onsets_computed['runoff_onset'].plot.imshow(col='water_year',col_wrap=3)\n",
    "# f,ax=plt.subplots(figsize=(12,12))\n",
    "# runoff_onsets_computed['runoff_onset_median'].plot.imshow(ax=ax)\n",
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True) #consolidated=False if processed tiles not showing up\n",
    "# global_ds\n",
    "# global_subset = global_ds.sel(latitude=runoff_onsets_computed.latitude,longitude=runoff_onsets_computed.longitude,method='nearest')\n",
    "# global_subset\n",
    "# runoff_onsets_computed_reindexed = runoff_onsets_computed.round().astype('uint16').assign_coords(latitude=global_subset.latitude,longitude=global_subset.longitude)\n",
    "# runoff_onsets_computed_reindexed\n",
    "# runoff_onsets_computed_reindexed.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048})#.to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True)\n",
    "# cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing equator\n",
    "# # \n",
    "# # client.restart()\n",
    "\n",
    "# test_tiles_gdf = valid_tiles_gdf[(valid_tiles_gdf['row'] == 54) | (valid_tiles_gdf['row'] == 55)]\n",
    "\n",
    "# test_tiles = [Tile(row,col) for row,col in zip(test_tiles_gdf.row,test_tiles_gdf.col)]\n",
    "\n",
    "# futures = []\n",
    "\n",
    "# for tile in test_tiles:\n",
    "#     future = client.submit(process_tile,tile)\n",
    "#     #future = process_tile.submit(tile)\n",
    "#     futures.append(future)\n",
    "\n",
    "#     test_tiles_gdf.explore()\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "\n",
    "# for result in results:\n",
    "#     print(f'tile {result.index} success: {result.success}, time: {result.total_time}, errors: {result.error_messages}, s1_rtc_ds_dims: {result.s1_rtc_ds_dims}, s1_rtc_masked_ds_dims: {result.s1_rtc_masked_ds_dims}, runoff_onsets_dims: {result.runoff_onsets_dims}')\n",
    "\n",
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True,decode_coords='all')\n",
    "\n",
    "\n",
    "# def view_tile(tile: Tile, global_ds: xr.Dataset):\n",
    "\n",
    "\n",
    "#     test_ds = global_ds.rio.clip_box(*tile.get_geobox().boundingbox,crs='EPSG:4326')\n",
    "\n",
    "#     f,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "#     test_ds['runoff_onset_median'].plot.imshow(ax=ax)\n",
    "\n",
    "#     test_ds['runoff_onset'].plot.imshow(col='water_year',col_wrap=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@coiled.function(cpu=4, memory='32 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", threads_per_worker=-1)\n",
    "# #@coiled.function(cpu=4, spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\")\n",
    "# #, name=f\"process_tile_batch_{batch_number}\"\n",
    "# #@dask.delayed#, threads_per_worker=-1\n",
    "# #odc.stac.configure_rio(cloud_defaults=True)\n",
    "# #@coiled.function(n_workers=80, cpu=4, memory='32 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", name='mem_test')\n",
    "# def process_tile(tile : Tile):\n",
    "\n",
    "\n",
    "#     tile.start_time = time.time()\n",
    "\n",
    "#     geobox = tile.geobox\n",
    "#     bbox_gdf = tile.bbox_gdf\n",
    "    \n",
    "#     #odc.stac.configure_rio(cloud_defaults=True)\n",
    "\n",
    "#     try:\n",
    "\n",
    "\n",
    "#         #s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "#         s1_rtc_ds = dask.delayed(get_sentinel1_rtc)(geobox)\n",
    "\n",
    "\n",
    "#         #tile.s1_rtc_ds = s1_rtc_ds\n",
    "        \n",
    "#         #tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "#         #seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds,bbox_gdf)\n",
    "#         seasonal_snow_mask_matched_ds = dask.delayed(get_custom_seasonal_snow_mask)(s1_rtc_ds,bbox_gdf)\n",
    "\n",
    "#         #gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "#         gmba_clipped_gdf = dask.delayed(get_gmba_mountain_inventory)(bbox_gdf)\n",
    "\n",
    "\n",
    "#         #s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "#         s1_rtc_masked_ds = dask.delayed(apply_all_masks)(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "\n",
    "        \n",
    "#         #tile.s1_rtc_masked_ds_dims = dict(s1_rtc_masked_ds.sizes)\n",
    "\n",
    "#         runoff_onsets_da = (\n",
    "#         s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "#         .apply(\n",
    "#             calculate_runoff_onset_wrapper,\n",
    "#             consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "#             min_monthly_acquisitions=2, #one or two\n",
    "#             returned_dates_format=\"dowy\",\n",
    "#             return_constituent_runoff_onsets=False,\n",
    "#             low_backscatter_threshold=0.001,\n",
    "#         ))\n",
    "        \n",
    "#         #tile.runoff_onsets = runoff_onsets\n",
    "        \n",
    "#         #runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset')# add .persist here?\n",
    "\n",
    "\n",
    "#         median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', 5)\n",
    "\n",
    "#         runoff_onsets_ds = dask.delayed(dataarrays_to_dataset)(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "#         #runoff_onsets_ds['runoff_onset_median'] = median_da\n",
    "        \n",
    "#         #runoff_onsets_ds = runoff_onsets_ds.round().astype('uint16')\n",
    "\n",
    "#         #runoff_onsets_ds['runoff_onset_std'] = std_da.astype('float32')\n",
    "\n",
    "#         #tile.runoff_onsets_dims = dict(runoff_onsets_ds.sizes)\n",
    "\n",
    "#         #with dask.config.set(pool=ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         #    runoff_onsets_computed = runoff_onsets_computed.compute() # use compute instead, then if stilll sloe remove thread pool saturation\n",
    "        \n",
    "#         #del s1_rtc_ds, seasonal_snow_mask_clip_ds, seasonal_snow_mask_matched_ds, s1_rtc_masked_ds, runoff_onsets #gmba_clipped_gdf,\n",
    "#         #gc.collect()\n",
    "\n",
    "#         global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude,longitude=runoff_onsets_ds.longitude,method='nearest')\n",
    "\n",
    "#         runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude,longitude=global_subset_ds.longitude)\n",
    "        \n",
    "#         #with dask.config.set(pool=ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True, compute=True)\n",
    "\n",
    "#         #del runoff_onsets_computed, global_store, global_ds, global_subset, runoff_onsets_computed_reindexed\n",
    "#         #gc.collect()\n",
    "\n",
    "#         #del seasonal_snow_mask_matched_ds\n",
    "\n",
    "#         tile.total_time = time.time() - tile.start_time\n",
    "#         tile.success = True\n",
    "\n",
    "#     except Exception as e:\n",
    "#         #gc.collect()\n",
    "#         tile.error_messages.append(str(e))\n",
    "#         tile.error_messages.append(traceback.format_exc())\n",
    "#         tile.total_time = time.time() - tile.start_time\n",
    "#         tile.success = False\n",
    "\n",
    "#     return tile\n",
    "\n",
    "\n",
    "\n",
    "# def apply_all_masks(s1_rtc_ds,bbox_gdf):\n",
    "\n",
    "#     s1_rtc_ds = remove_unwanted_water_years(s1_rtc_ds)\n",
    "\n",
    "#     center_lat = (s1_rtc_ds.rio.bounds()[1]+s1_rtc_ds.rio.bounds()[3])/2\n",
    "\n",
    "#     if np.absolute(center_lat) < 3:\n",
    "#         s1_rtc_ds = remove_equator_crossing(s1_rtc_ds)\n",
    "        \n",
    "#     gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "#     s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry)\n",
    "    \n",
    "#     seasonal_snow_mask_clip_ds = get_custom_seasonal_snow_mask(bbox_gdf)\n",
    "#     seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=0)).rename({'x':'longitude','y':'latitude'})\n",
    "#     #seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.odc.reproject(geobox)\n",
    "\n",
    "\n",
    "#     s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "    \n",
    "#     return s1_rtc_masked_ds, seasonal_snow_mask_matched_ds\n",
    "\n",
    "# def remove_unwanted_water_years(s1_rtc_ds):\n",
    "#     s1_rtc_ds = s1_rtc_ds.sel(time=s1_rtc_ds.water_year.isin(water_years))\n",
    "#     return s1_rtc_ds\n",
    "\n",
    "\n",
    "# def remove_equator_crossing(s1_rtc_ds):\n",
    "#     if s1_rtc_ds.attrs['hemisphere'] == 'northern':\n",
    "#         mask = s1_rtc_ds.latitude >= 0\n",
    "#     else:\n",
    "#         mask = s1_rtc_ds.latitude < 0\n",
    "\n",
    "#     s1_rtc_ds = s1_rtc_ds.where(mask)\n",
    "#     return s1_rtc_ds\n",
    "\n",
    "# def get_gmba_mountain_inventory(bbox_gdf):\n",
    "#     url = (f\"https://data.earthenv.org/mountains/standard/GMBA_Inventory_v2.0_standard_300.zip\")\n",
    "#     gmba_gdf = gpd.read_file(\"zip+\" + url)\n",
    "#     return gpd.clip(gmba_gdf, bbox_gdf)\n",
    "\n",
    "# def get_custom_seasonal_snow_mask(bbox_gdf):\n",
    "#     #xmin, ymin, xmax, ymax = bbox_gdf.total_bounds\n",
    "#     mask_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snow_mask/global_modis_snow_mask.zarr\")\n",
    "#     seasonal_snow_mask = xr.open_zarr(mask_store, consolidated=True, decode_coords='all') \n",
    "#     seasonal_snow_mask_clip = seasonal_snow_mask.rio.clip_box(*bbox_gdf.total_bounds,crs='EPSG:4326') # clip to correct box, maybe use total_bounds and then use crs \n",
    "#     return seasonal_snow_mask_clip\n",
    "\n",
    "\n",
    "# def apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds):\n",
    "#     s1_rtc_masked_ds = s1_rtc_ds.groupby('water_year').map(lambda group: apply_mask_for_year(group, seasonal_snow_mask_matched_ds))\n",
    "#     s1_rtc_masked_ds.rio.write_crs(s1_rtc_ds.rio.crs,inplace=True)\n",
    "#     return s1_rtc_masked_ds\n",
    "\n",
    "# def apply_mask_for_year(group, seasonal_snow_mask_matched_ds):\n",
    "\n",
    "#     year = group.water_year.values[0]\n",
    "\n",
    "\n",
    "#     if year not in seasonal_snow_mask_matched_ds.water_year:\n",
    "#         print(f\"Warning: water_year {year} not found in seasonal_snow_mask_matched_ds\")\n",
    "#         return group.where(False) \n",
    "\n",
    "#     sad_mask = group['DOWY'] >= seasonal_snow_mask_matched_ds['SAD_DOWY'].sel(water_year=year)\n",
    "#     sdd_mask = group['DOWY'] <= seasonal_snow_mask_matched_ds['SDD_DOWY'].sel(water_year=year)\n",
    "#     consec_mask = seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=year) >= 56\n",
    "#     combined_mask = sad_mask & sdd_mask & consec_mask\n",
    "#     return group.where(combined_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def xr_datetime_to_DOWY_map_blocks(date_da, hemisphere=\"northern\"):\n",
    "#     \"\"\"\n",
    "#     Converts an xarray DataArray containing datetime objects to the Day of Water Year (DOWY).\n",
    "\n",
    "#     Parameters:\n",
    "#     date (xr.DataArray): An xarray DataArray with datetime64 data type.\n",
    "#     hemisphere (str): 'northern' or 'southern'\n",
    "\n",
    "#     Returns:\n",
    "#     xr.DataArray: An xarray DataArray containing the DOWY for each datetime in the input DataArray.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Determine any valid date\n",
    "#     if date_da.attrs.get(\"any_valid_date\") is not None:\n",
    "#         any_valid_date = pd.to_datetime(date_da.attrs[\"any_valid_date\"])\n",
    "#     else:\n",
    "#         any_valid_date = pd.to_datetime(date_da.sel(x=0, y=0, method=\"nearest\").values)\n",
    "\n",
    "#     # Calculate the start of the water year\n",
    "#     start_of_water_year = easysnowdata.utils.get_water_year_start(\n",
    "#         any_valid_date, hemisphere=hemisphere\n",
    "#     )\n",
    "\n",
    "#     # Define the function to calculate DOWY for a block\n",
    "#     def calculate_dowy_block(block, start_of_water_year):\n",
    "#         # Calculate DOWY for the block\n",
    "#         dowy_block = (block - np.datetime64(start_of_water_year)).astype('timedelta64[D]').astype(int) + 1\n",
    "#         return dowy_block\n",
    "\n",
    "#     # Apply the function using map_blocks\n",
    "#     return date_da.map_blocks(\n",
    "#         calculate_dowy_block,\n",
    "#         args=(start_of_water_year,),\n",
    "#         template=date_da.astype(int)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile = tiles[0]\n",
    "# geobox = tile.geobox\n",
    "# bbox_gdf = tile.bbox_gdf\n",
    "\n",
    "# s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "# s1_rtc_ds = dask.delayed(get_sentinel1_rtc)(geobox)\n",
    "# s1_rtc_ds\n",
    "\n",
    "\n",
    "# seasonal_snow_mask_matched_ds = dask.delayed(get_custom_seasonal_snow_mask)(s1_rtc_ds,bbox_gdf)\n",
    "# seasonal_snow_mask_matched_ds\n",
    "\n",
    "\n",
    "# gmba_clipped_gdf = dask.delayed(get_gmba_mountain_inventory)(bbox_gdf)\n",
    "# gmba_clipped_gdf\n",
    "\n",
    "\n",
    "# s1_rtc_masked_ds = dask.delayed(apply_all_masks)(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "# s1_rtc_masked_ds\n",
    "\n",
    "\n",
    "# runoff_onsets_da = (\n",
    "# s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "# .apply(\n",
    "# calculate_runoff_onset_wrapper,\n",
    "# consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "# min_monthly_acquisitions=2, #one or two\n",
    "# returned_dates_format=\"doy\",\n",
    "# return_constituent_runoff_onsets=False,\n",
    "# low_backscatter_threshold=0.001,\n",
    "# ))\n",
    "\n",
    "# runoff_onsets_da\n",
    "\n",
    "\n",
    "# median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', 5)\n",
    "\n",
    "# def dataarrays_to_dataset(runoff_onsets_da, median_da, std_da):\n",
    "\n",
    "#     runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset').round().astype('uint16')\n",
    "#     runoff_onsets_ds['runoff_onset_median'] = median_da.round().astype('uint16')\n",
    "#     runoff_onsets_ds['runoff_onset_std'] = std_da\n",
    "    \n",
    "#     return runoff_onsets_ds\n",
    "\n",
    "# runoff_onsets_ds = dask.delayed(dataarrays_to_dataset)(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "# runoff_onsets_ds\n",
    "\n",
    "# runoff_onsets_computed_ds = dask.compute(runoff_onsets_ds)\n",
    "\n",
    "# runoff_onsets_computed_ds\n",
    "\n",
    "# global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude,longitude=runoff_onsets_ds.longitude,method='nearest')\n",
    "# global_subset_ds\n",
    "\n",
    "# runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude,longitude=global_subset_ds.longitude)\n",
    "# runoff_onsets_reindexed_ds\n",
    "\n",
    "# dask.compute(runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True))\n",
    "\n",
    "\n",
    "# runoff_onsets_ds['runoff_onset_median'] = median_da\n",
    "\n",
    "# runoff_onsets_ds = runoff_onsets_ds.round().astype('uint16')\n",
    "\n",
    "# runoff_onsets_ds['runoff_onset_std'] = std_da.astype('float32')\n",
    "\n",
    "# computed_results = dask.compute(*results)\n",
    "\n",
    "\n",
    "# results = computed_results\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     for tile in tile_batch:\n",
    "#         result = process_tile(tile)\n",
    "#         results.append(result)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         break\n",
    "    \n",
    "    \n",
    "#     for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     futures = []\n",
    "#     for tile in tile_batch:\n",
    "#         future = client.submit(process_tile,tile)\n",
    "#         #future = process_tile.submit(tile) # when trying serverless coiled.function\n",
    "#         futures.append(future)\n",
    "\n",
    "#     fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "#     #results \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "#     df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#     header_written = True\n",
    "    \n",
    "#     client.restart()\n",
    "    \n",
    "#     if i == 1:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_results_list = []\n",
    "\n",
    "# batch_size = 10\n",
    "# tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "# process_tile_serverless = coiled.function(n_workers=11, cpu=8, memory='64 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", name='mem_test')(process_tile)\n",
    "\n",
    "\n",
    "# for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     futures = []\n",
    "#     for tile in tile_batch:\n",
    "#         #future = client.submit(process_tile,tile)\n",
    "#         future = process_tile_serverless.submit(tile) # when trying serverless coiled.function\n",
    "#         futures.append(future)\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "    \n",
    "#     full_results_list.extend(results)\n",
    "\n",
    "#     fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "    \n",
    "#     #results = client.gather(futures) \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "#     df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#     header_written = True\n",
    "    \n",
    "    \n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "#     print(\"Calculating pixelwise counts and maximum gaps per orbit and polarization...\")\n",
    "#     pixelwise_counts_per_orbit_ds = s1_rtc_ds.groupby(\"sat:relative_orbit\").count(dim=\"time\", engine='flox')\n",
    "    \n",
    "#     def calc_max_gap(group):\n",
    "#         times = group.time.sortby('time')\n",
    "#         gaps = times.diff(dim='time').max()\n",
    "#         return gaps\n",
    "\n",
    "#     max_time_gap_per_orbit_days_da = s1_rtc_ds.groupby(\"sat:relative_orbit\").map(calc_max_gap).dt.days\n",
    "    \n",
    "#     return pixelwise_counts_per_orbit_ds, max_time_gap_per_orbit_days_da\n",
    "\n",
    "\n",
    "# def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#     backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "#     pixelwise_counts_per_orbit_ds: xr.Dataset,\n",
    "#     max_days_gap_per_orbit_da: xr.Dataset,\n",
    "#     consec_snow_days_da: xr.DataArray,\n",
    "#     min_monthly_acquisitions: int,\n",
    "#     max_allowed_days_gap_per_orbit: int\n",
    "# ):\n",
    "#     print(f\"Filtering insufficient pixels per orbit and polarization...\")\n",
    "#     constituent_runoff_onsets_ds = (\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds.where(\n",
    "#             (pixelwise_counts_per_orbit_ds >= (min_monthly_acquisitions*(consec_snow_days_da/30))) &\n",
    "#             (max_days_gap_per_orbit_da <= max_allowed_days_gap_per_orbit)\n",
    "#         )\n",
    "#     )\n",
    "#     return constituent_runoff_onsets_ds\n",
    "\n",
    "\n",
    "# constituent_runoff_onsets_ds = (\n",
    "#     filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "#         pixelwise_counts_per_orbit_ds,\n",
    "#         max_days_gap_per_orbit_da,\n",
    "#         seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=2019),\n",
    "#         min_monthly_acquisitions,\n",
    "#         max_allowed_days_gap_per_orbit\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# constituent_runoff_onsets_da = constituent_runoff_onsets_ds.to_dataarray(dim=\"polarization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sar_snowmelt_timing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
