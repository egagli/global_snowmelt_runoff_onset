{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8f4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from global_snowmelt_runoff_onset.config import Config\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset_comparison(target_config_path, reference_config_path, n_tiles_to_check=10, tolerance=1e-6):\n",
    "    \"\"\"Compare two datasets for consistency\"\"\"\n",
    "    from global_snowmelt_runoff_onset.config import Config\n",
    "    import random\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Load both configs\n",
    "\n",
    "    print(\"target config:\")\n",
    "    config_v2 = Config(target_config_path)\n",
    "\n",
    "    print(\"reference config:\")\n",
    "    config_v1 = Config(reference_config_path)\n",
    "\n",
    "\n",
    "    # Get processed tiles from both versions\n",
    "    tiles_v1 = config_v1.get_list_of_tiles(which='processed')\n",
    "    tiles_v2 = config_v2.get_list_of_tiles(which='processed')\n",
    "    \n",
    "    # Find common tiles\n",
    "    tiles_v1_indices = set((tile.row, tile.col) for tile in tiles_v1)\n",
    "    tiles_v2_indices = set((tile.row, tile.col) for tile in tiles_v2)\n",
    "    common_indices = tiles_v1_indices.intersection(tiles_v2_indices)\n",
    "    \n",
    "    if len(common_indices) == 0:\n",
    "        return {\"error\": \"No common processed tiles found\"}\n",
    "    \n",
    "    # Sample tiles for detailed comparison\n",
    "    sampled_indices = random.sample(list(common_indices), min(n_tiles_to_check, len(common_indices)))\n",
    "    \n",
    "    results = {\n",
    "        'dataset_configs': {\n",
    "            'target_dataset': target_config_path,\n",
    "            'reference_dataset': reference_config_path,\n",
    "            'target_processed_tiles': len(tiles_v2),\n",
    "            'reference_processed_tiles': len(tiles_v1),\n",
    "            'common_tiles': len(common_indices),\n",
    "            'tiles_checked': len(sampled_indices)\n",
    "        },\n",
    "        'tile_comparisons': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Open with mask_and_scale=False for detailed encoding comparison\n",
    "        ds_v1_raw = xr.open_zarr(config_v1.global_runoff_store, consolidated=True, mask_and_scale=False)\n",
    "        ds_v2_raw = xr.open_zarr(config_v2.global_runoff_store, consolidated=True, mask_and_scale=False)\n",
    "        \n",
    "        # Open with mask_and_scale=True for aggregation validation\n",
    "        ds_v1_decoded = xr.open_zarr(config_v1.global_runoff_store, consolidated=True, mask_and_scale=True)\n",
    "        ds_v2_decoded = xr.open_zarr(config_v2.global_runoff_store, consolidated=True, mask_and_scale=True)\n",
    "        \n",
    "        expected_ranges = {\n",
    "            'runoff_onset': (1, 366),\n",
    "            'runoff_onset_median': (1, 366),\n",
    "            'runoff_onset_mad': (0, 100),\n",
    "            'temporal_resolution': (0.1, 100),\n",
    "            'temporal_resolution_median': (0.1, 100)\n",
    "        }\n",
    "        \n",
    "        for row, col in sampled_indices:\n",
    "            tile_v1 = config_v1.get_tile(row, col)\n",
    "            tile_v2 = config_v2.get_tile(row, col)\n",
    "            \n",
    "            # Get tile bounds\n",
    "            extent_v1 = tile_v1.geobox.extent.boundary.coords\n",
    "            extent_v2 = tile_v2.geobox.extent.boundary.coords\n",
    "            \n",
    "            lons_v1, lats_v1 = zip(*extent_v1)\n",
    "            min_lon_v1, max_lon_v1 = min(lons_v1), max(lons_v1)\n",
    "            min_lat_v1, max_lat_v1 = min(lats_v1), max(lats_v1)\n",
    "            \n",
    "            lons_v2, lats_v2 = zip(*extent_v2)\n",
    "            min_lon_v2, max_lon_v2 = min(lons_v2), max(lons_v2)\n",
    "            min_lat_v2, max_lat_v2 = min(lats_v2), max(lats_v2)\n",
    "            \n",
    "            # Get raw data for encoding comparison\n",
    "            tile_ds_v1_raw = ds_v1_raw.sel(\n",
    "                latitude=slice(max_lat_v1, min_lat_v1),\n",
    "                longitude=slice(min_lon_v1, max_lon_v1)\n",
    "            ).compute()\n",
    "            \n",
    "            tile_ds_v2_raw = ds_v2_raw.sel(\n",
    "                latitude=slice(max_lat_v2, min_lat_v2),\n",
    "                longitude=slice(min_lon_v2, max_lon_v2)\n",
    "            ).compute()\n",
    "            \n",
    "            # Get decoded data for aggregation validation\n",
    "            tile_ds_v1_decoded = ds_v1_decoded.sel(\n",
    "                latitude=slice(max_lat_v1, min_lat_v1),\n",
    "                longitude=slice(min_lon_v1, max_lon_v1)\n",
    "            ).compute()\n",
    "            \n",
    "            tile_ds_v2_decoded = ds_v2_decoded.sel(\n",
    "                latitude=slice(max_lat_v2, min_lat_v2),\n",
    "                longitude=slice(min_lon_v2, max_lon_v2)\n",
    "            ).compute()\n",
    "            \n",
    "            tile_id = f\"tile_{row}_{col}\"\n",
    "            tile_results = {'variables': {}}\n",
    "            \n",
    "            # Analyze each data variable in target dataset\n",
    "            analysis_vars = [var for var in tile_ds_v2_raw.data_vars if var != 'spatial_ref']\n",
    "            for var_name in analysis_vars:\n",
    "                if var_name in tile_ds_v1_raw.data_vars:\n",
    "                    var_comparison = analyze_variable_comparison(\n",
    "                        tile_ds_v1_raw[var_name], tile_ds_v2_raw[var_name], var_name, expected_ranges, tolerance,\n",
    "                        tile_ds_v1_decoded, tile_ds_v2_decoded\n",
    "                    )\n",
    "                    tile_results['variables'][var_name] = var_comparison\n",
    "            \n",
    "            results['tile_comparisons'][tile_id] = tile_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['error'] = f\"Error during comparison: {str(e)}\"\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_variable_comparison(var_ref, var_target, var_name, expected_ranges, tolerance=1e-6, \n",
    "                              tile_ds_ref=None, tile_ds_target=None):\n",
    "    \"\"\"Analyze a single variable comparison between reference and target datasets\"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    # Get encoding information for both datasets\n",
    "    ref_encoding = var_ref.encoding\n",
    "    ref_attrs = var_ref.attrs\n",
    "    target_encoding = var_target.encoding \n",
    "    target_attrs = var_target.attrs\n",
    "    \n",
    "    # Raw values\n",
    "    ref_raw = var_ref.values\n",
    "    target_raw = var_target.values\n",
    "    \n",
    "    # Process reference dataset\n",
    "    ref_fill = ref_encoding.get('_FillValue', ref_attrs.get('_FillValue'))\n",
    "    ref_scale = ref_encoding.get('scale_factor', ref_attrs.get('scale_factor', 1.0))\n",
    "    ref_offset = ref_encoding.get('add_offset', ref_attrs.get('add_offset', 0.0))\n",
    "    \n",
    "    ref_decoded = ref_raw.astype(np.float64)\n",
    "    if ref_fill is not None:\n",
    "        ref_valid_mask = ref_raw != ref_fill\n",
    "        ref_decoded[~ref_valid_mask] = np.nan\n",
    "    else:\n",
    "        ref_valid_mask = np.ones(ref_raw.shape, dtype=bool)\n",
    "    ref_decoded = ref_decoded * ref_scale + ref_offset\n",
    "    ref_valid_values = ref_decoded[ref_valid_mask & ~np.isnan(ref_decoded)]\n",
    "    \n",
    "    # Process target dataset\n",
    "    target_fill = target_encoding.get('_FillValue', target_attrs.get('_FillValue'))\n",
    "    target_scale = target_encoding.get('scale_factor', target_attrs.get('scale_factor', 1.0))\n",
    "    target_offset = target_encoding.get('add_offset', target_attrs.get('add_offset', 0.0))\n",
    "    \n",
    "    target_decoded = target_raw.astype(np.float64)\n",
    "    if target_fill is not None:\n",
    "        target_valid_mask = target_raw != target_fill\n",
    "        target_decoded[~target_valid_mask] = np.nan\n",
    "    else:\n",
    "        target_valid_mask = np.ones(target_raw.shape, dtype=bool)\n",
    "    target_decoded = target_decoded * target_scale + target_offset\n",
    "    target_valid_values = target_decoded[target_valid_mask & ~np.isnan(target_decoded)]\n",
    "    \n",
    "    # Calculate statistics for both\n",
    "    def get_stats(values, raw_values, valid_mask, decoded_values):\n",
    "        if len(values) > 0:\n",
    "            unique_vals = np.unique(values)\n",
    "            try:\n",
    "                mode_result = stats.mode(values, keepdims=False)\n",
    "                mode_val = float(mode_result.mode)\n",
    "            except:\n",
    "                mode_val = None\n",
    "                \n",
    "            if len(unique_vals) > 1:\n",
    "                diffs = np.diff(np.sort(unique_vals))\n",
    "                min_diff = float(np.min(diffs[diffs > 0])) if np.any(diffs > 0) else 0.0\n",
    "            else:\n",
    "                min_diff = 0.0\n",
    "                \n",
    "            percentiles = {f'p{p:02d}': float(np.percentile(values, p)) for p in range(0, 101, 10)}\n",
    "            \n",
    "            return {\n",
    "                'min': float(np.min(values)),\n",
    "                'max': float(np.max(values)),\n",
    "                'mean': float(np.mean(values)),\n",
    "                'median': float(np.median(values)),\n",
    "                'mode': mode_val,\n",
    "                'std': float(np.std(values)),\n",
    "                'unique_values': len(unique_vals),\n",
    "                'precision': min_diff,\n",
    "                'percentiles': percentiles\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'min': None, 'max': None, 'mean': None, 'median': None,\n",
    "                'mode': None, 'std': None, 'unique_values': 0, 'precision': None,\n",
    "                'percentiles': {}\n",
    "            }\n",
    "    \n",
    "    ref_stats = get_stats(ref_valid_values, ref_raw, ref_valid_mask, ref_decoded)\n",
    "    target_stats = get_stats(target_valid_values, target_raw, target_valid_mask, target_decoded)\n",
    "    \n",
    "    # Calculate differences and matches\n",
    "    num_mismatched = 0\n",
    "    max_diff = 0.0\n",
    "    min_diff = 0.0\n",
    "    mean_diff = 0.0\n",
    "    median_diff = 0.0\n",
    "    mode_diff = None\n",
    "    \n",
    "    if len(ref_valid_values) > 0 and len(target_valid_values) > 0:\n",
    "        # Only compare where both have valid data\n",
    "        common_mask = ref_valid_mask & target_valid_mask\n",
    "        if np.any(common_mask):\n",
    "            ref_common = ref_decoded[common_mask]\n",
    "            target_common = target_decoded[common_mask]\n",
    "            diffs = np.abs(ref_common - target_common)\n",
    "            max_diff = float(np.max(diffs))\n",
    "            min_diff = float(np.min(diffs))\n",
    "            mean_diff = float(np.mean(diffs))\n",
    "            median_diff = float(np.median(diffs))\n",
    "            num_mismatched = int(np.sum(diffs > tolerance))\n",
    "            \n",
    "            # Calculate mode of differences\n",
    "            try:\n",
    "                mode_result = stats.mode(diffs, keepdims=False)\n",
    "                mode_diff = float(mode_result.mode)\n",
    "            except:\n",
    "                mode_diff = None\n",
    "    \n",
    "    # Check values outside expected range for both datasets\n",
    "    target_values_outside_range = 0\n",
    "    target_mode_outside_range = None\n",
    "    ref_values_outside_range = 0\n",
    "    ref_mode_outside_range = None\n",
    "    \n",
    "    if var_name in expected_ranges:\n",
    "        min_exp, max_exp = expected_ranges[var_name]\n",
    "        \n",
    "        # Check target dataset\n",
    "        if len(target_valid_values) > 0:\n",
    "            outside_mask = (target_valid_values < min_exp) | (target_valid_values > max_exp)\n",
    "            target_values_outside_range = int(np.sum(outside_mask))\n",
    "            if target_values_outside_range > 0:\n",
    "                outside_values = target_valid_values[outside_mask]\n",
    "                try:\n",
    "                    mode_result = stats.mode(outside_values, keepdims=False)\n",
    "                    target_mode_outside_range = float(mode_result.mode)\n",
    "                except:\n",
    "                    target_mode_outside_range = None\n",
    "        \n",
    "        # Check reference dataset\n",
    "        if len(ref_valid_values) > 0:\n",
    "            outside_mask = (ref_valid_values < min_exp) | (ref_valid_values > max_exp)\n",
    "            ref_values_outside_range = int(np.sum(outside_mask))\n",
    "            if ref_values_outside_range > 0:\n",
    "                outside_values = ref_valid_values[outside_mask]\n",
    "                try:\n",
    "                    mode_result = stats.mode(outside_values, keepdims=False)\n",
    "                    ref_mode_outside_range = float(mode_result.mode)\n",
    "                except:\n",
    "                    ref_mode_outside_range = None\n",
    "    \n",
    "    # Build comparison table\n",
    "    comparison = {\n",
    "        'dimensions': {\n",
    "            'target_dataset': str({dim: var_target.sizes[dim] for dim in var_target.dims}),\n",
    "            'reference_dataset': str({dim: var_ref.sizes[dim] for dim in var_ref.dims}),\n",
    "            'match': var_target.dims == var_ref.dims and all(var_target.sizes[dim] == var_ref.sizes[dim] for dim in var_target.dims if dim in var_ref.dims)\n",
    "        },\n",
    "        'chunk_dimensions': {\n",
    "            'target_dataset': str(target_encoding.get('preferred_chunks', 'Not chunked')),\n",
    "            'reference_dataset': str(ref_encoding.get('preferred_chunks', 'Not chunked')),\n",
    "            'match': target_encoding.get('preferred_chunks') == ref_encoding.get('preferred_chunks')\n",
    "        },\n",
    "        'dtype': {\n",
    "            'target_dataset': str(target_decoded.dtype),\n",
    "            'reference_dataset': str(ref_decoded.dtype),\n",
    "            'match': target_decoded.dtype == ref_decoded.dtype\n",
    "        },\n",
    "        'encoded_dtype': {\n",
    "            'target_dataset': str(target_raw.dtype),\n",
    "            'reference_dataset': str(ref_raw.dtype), \n",
    "            'match': target_raw.dtype == ref_raw.dtype\n",
    "        },\n",
    "        'nodata_value': {\n",
    "            'target_dataset': target_fill,\n",
    "            'reference_dataset': ref_fill,\n",
    "            'match': target_fill == ref_fill\n",
    "        },\n",
    "        'encoded_nodata_value': {\n",
    "            'target_dataset': target_fill,\n",
    "            'reference_dataset': ref_fill,\n",
    "            'match': target_fill == ref_fill\n",
    "        },\n",
    "        'scale_factor': {\n",
    "            'target_dataset': target_scale,\n",
    "            'reference_dataset': ref_scale,\n",
    "            'match': abs(target_scale - ref_scale) < tolerance\n",
    "        },\n",
    "        'add_offset': {\n",
    "            'target_dataset': target_offset,\n",
    "            'reference_dataset': ref_offset,\n",
    "            'match': abs(target_offset - ref_offset) < tolerance\n",
    "        },\n",
    "        'total_pixels': {\n",
    "            'target_dataset': target_raw.size,\n",
    "            'reference_dataset': ref_raw.size,\n",
    "            'match': target_raw.size == ref_raw.size\n",
    "        },\n",
    "        'nan_pixels': {\n",
    "            'target_dataset': int(np.count_nonzero(~target_valid_mask)),\n",
    "            'reference_dataset': int(np.count_nonzero(~ref_valid_mask)),\n",
    "            'match': np.count_nonzero(~target_valid_mask) == np.count_nonzero(~ref_valid_mask)\n",
    "        },\n",
    "        'unique_values': {\n",
    "            'target_dataset': target_stats['unique_values'],\n",
    "            'reference_dataset': ref_stats['unique_values'],\n",
    "            'match': target_stats['unique_values'] == ref_stats['unique_values']\n",
    "        },\n",
    "        'precision': {\n",
    "            'target_dataset': target_stats['precision'],\n",
    "            'reference_dataset': ref_stats['precision'],\n",
    "            'match': abs((target_stats['precision'] or 0) - (ref_stats['precision'] or 0)) < tolerance\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add statistical comparisons\n",
    "    for stat in ['min', 'max', 'mean', 'median', 'mode', 'std']:\n",
    "        target_val = target_stats[stat]\n",
    "        ref_val = ref_stats[stat]\n",
    "        if target_val is not None and ref_val is not None:\n",
    "            match = abs(target_val - ref_val) < tolerance\n",
    "        else:\n",
    "            match = target_val == ref_val\n",
    "        \n",
    "        comparison[stat] = {\n",
    "            'target_dataset': target_val,\n",
    "            'reference_dataset': ref_val,\n",
    "            'match': match\n",
    "        }\n",
    "    \n",
    "    # Add percentiles\n",
    "    for p in range(0, 101, 10):\n",
    "        p_key = f'p{p:02d}'\n",
    "        target_val = target_stats['percentiles'].get(p_key)\n",
    "        ref_val = ref_stats['percentiles'].get(p_key)\n",
    "        if target_val is not None and ref_val is not None:\n",
    "            match = abs(target_val - ref_val) < tolerance\n",
    "        else:\n",
    "            match = target_val == ref_val\n",
    "            \n",
    "        comparison[f'percentile_{p:02d}'] = {\n",
    "            'target_dataset': target_val,\n",
    "            'reference_dataset': ref_val,\n",
    "            'match': match\n",
    "        }\n",
    "    \n",
    "    # Add expected range metrics\n",
    "    comparison['num_outside_expected_range'] = {\n",
    "        'target_dataset': target_values_outside_range,\n",
    "        'reference_dataset': ref_values_outside_range, \n",
    "        'match': target_values_outside_range == 0 and ref_values_outside_range == 0\n",
    "    }\n",
    "    comparison['mode_outside_expected_range'] = {\n",
    "        'target_dataset': target_mode_outside_range,\n",
    "        'reference_dataset': ref_mode_outside_range,\n",
    "        'match': target_mode_outside_range is None and ref_mode_outside_range is None\n",
    "    }\n",
    "    \n",
    "    # For median and MAD variables, add correctness check for both datasets\n",
    "    # Use the decoded datasets that were passed in\n",
    "    if ('median' in var_name or 'mad' in var_name) and tile_ds_target is not None and tile_ds_ref is not None:\n",
    "        # Check aggregation correctness using the decoded datasets\n",
    "        target_aggregation_correct, target_aggregation_diff = check_aggregation_correctness(\n",
    "            var_name, tile_ds_target[var_name], tile_ds_target, tolerance\n",
    "        )\n",
    "        ref_aggregation_correct, ref_aggregation_diff = check_aggregation_correctness(\n",
    "            var_name, tile_ds_ref[var_name], tile_ds_ref, tolerance\n",
    "        )\n",
    "        comparison['aggregation_correct'] = {\n",
    "            'target_dataset': target_aggregation_correct,\n",
    "            'reference_dataset': ref_aggregation_correct,\n",
    "            'match': target_aggregation_correct and ref_aggregation_correct\n",
    "        }\n",
    "        comparison['aggregation_difference'] = {\n",
    "            'target_dataset': target_aggregation_diff,\n",
    "            'reference_dataset': ref_aggregation_diff,\n",
    "            'match': (target_aggregation_diff or 0) < tolerance and (ref_aggregation_diff or 0) < tolerance\n",
    "        }\n",
    "    \n",
    "    # Add comparison metrics at the end (with visual separation)\n",
    "    comparison['_separator'] = {\n",
    "        'target_dataset': '--- COMPARISON METRICS ---',\n",
    "        'reference_dataset': '--- COMPARISON METRICS ---',\n",
    "        'match': True\n",
    "    }\n",
    "    comparison['num_values_mismatch'] = {\n",
    "        'target_dataset': num_mismatched,\n",
    "        'reference_dataset': '-',\n",
    "        'match': num_mismatched == 0\n",
    "    }\n",
    "    comparison['min_difference'] = {\n",
    "        'target_dataset': min_diff,\n",
    "        'reference_dataset': '-',\n",
    "        'match': min_diff < tolerance\n",
    "    }\n",
    "    comparison['max_difference'] = {\n",
    "        'target_dataset': max_diff,\n",
    "        'reference_dataset': '-',\n",
    "        'match': max_diff < tolerance\n",
    "    }\n",
    "    comparison['mean_difference'] = {\n",
    "        'target_dataset': mean_diff,\n",
    "        'reference_dataset': '-',\n",
    "        'match': mean_diff < tolerance\n",
    "    }\n",
    "    comparison['median_difference'] = {\n",
    "        'target_dataset': median_diff,\n",
    "        'reference_dataset': '-',\n",
    "        'match': median_diff < tolerance\n",
    "    }\n",
    "    comparison['mode_difference'] = {\n",
    "        'target_dataset': mode_diff,\n",
    "        'reference_dataset': '-',\n",
    "        'match': (mode_diff or 0) < tolerance\n",
    "    }\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "\n",
    "def check_aggregation_correctness(var_name, stored_aggregation, source_dataset, tolerance=1e-6):\n",
    "    \"\"\"Check if stored aggregated values match what we calculate from source data\n",
    "    \n",
    "    Args:\n",
    "        var_name: Name of the aggregated variable (e.g., 'runoff_onset_median')\n",
    "        stored_aggregation: The stored aggregated xarray DataArray to validate (should be decoded)\n",
    "        source_dataset: The xarray dataset containing the source multi-year data (should be decoded)\n",
    "        tolerance: Tolerance for numerical differences\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Determine the base variable name for the aggregation\n",
    "    if 'runoff_onset_median' in var_name:\n",
    "        base_var = 'runoff_onset'\n",
    "        agg_type = 'median'\n",
    "    elif 'runoff_onset_mad' in var_name:\n",
    "        base_var = 'runoff_onset'\n",
    "        agg_type = 'mad'\n",
    "    elif 'temporal_resolution_median' in var_name:\n",
    "        base_var = 'temporal_resolution'\n",
    "        agg_type = 'median'\n",
    "    else:\n",
    "        # For unknown aggregation types, can't validate\n",
    "        return True, 0.0  # Assume correct if we can't validate\n",
    "    \n",
    "    # Check if the base variable exists in the dataset\n",
    "    if base_var not in source_dataset.data_vars:\n",
    "        # Base variable not available, can't validate\n",
    "        return True, 0.0  # Assume correct if we can't validate\n",
    "    \n",
    "    try:\n",
    "        # Get the source data (should have water_year dimension)\n",
    "        source_data = source_dataset[base_var]\n",
    "        \n",
    "        # Check if water_year dimension exists\n",
    "        if 'water_year' not in source_data.dims:\n",
    "            # No water_year dimension, can't validate\n",
    "            return True, 0.0  # Assume correct if we can't validate\n",
    "        \n",
    "        # Calculate the expected aggregation along water_year dimension using xarray\n",
    "        # Data should already be decoded since we opened with mask_and_scale=True\n",
    "        if agg_type == 'median':\n",
    "            # Calculate median along water_year dimension\n",
    "            expected_aggregation = source_data.median(dim='water_year', skipna=True)\n",
    "        elif agg_type == 'mad':\n",
    "            # Calculate MAD (median absolute deviation) along water_year dimension\n",
    "            median_vals = source_data.median(dim='water_year', skipna=True)\n",
    "            # MAD = median(|x - median(x)|)\n",
    "            abs_deviations = np.abs(source_data - median_vals)\n",
    "            expected_aggregation = abs_deviations.median(dim='water_year', skipna=True)\n",
    "        else:\n",
    "            return True, 0.0  # Unknown aggregation type\n",
    "        \n",
    "        # Now compare: abs(stored_aggregation - expected_aggregation) < tolerance\n",
    "        # Work directly with xarray DataArrays (both should be decoded)\n",
    "        differences = np.abs(stored_aggregation - expected_aggregation)\n",
    "        \n",
    "        # Get the maximum difference (convert to numpy for this operation)\n",
    "        max_difference = float(differences.max().values)\n",
    "        \n",
    "        # Check if ALL differences are within tolerance\n",
    "        # Only count valid (non-NaN) pixels\n",
    "        valid_diffs = differences.where(~np.isnan(differences))\n",
    "        num_mismatched = int((valid_diffs > tolerance).sum().values)\n",
    "        \n",
    "        # Consider correct if ALL valid values match within tolerance\n",
    "        is_correct = num_mismatched == 0\n",
    "        \n",
    "        return is_correct, max_difference\n",
    "        \n",
    "    except Exception as e:\n",
    "        # If anything fails, assume correct (can't validate)\n",
    "        print(f\"Warning: Could not validate aggregation for {var_name}: {e}\")\n",
    "        return True, 0.0\n",
    "\n",
    "\n",
    "def print_validation_summary(results):\n",
    "    \"\"\"Print validation results in table format\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"❌ ERROR: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    if 'dataset_configs' in results:\n",
    "        # Dataset comparison summary\n",
    "        config_info = results['dataset_configs']\n",
    "        print(\"=\"*80)\n",
    "        print(\"🔍 DATASET COMPARISON VALIDATION\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Target Dataset: {config_info['target_dataset']}\")\n",
    "        print(f\"Reference Dataset: {config_info['reference_dataset']}\")\n",
    "        print(f\"Target Processed Tiles: {config_info['target_processed_tiles']}\")\n",
    "        print(f\"Reference Processed Tiles: {config_info['reference_processed_tiles']}\")\n",
    "        print(f\"Common Tiles: {config_info['common_tiles']}\")\n",
    "        print(f\"Tiles Checked: {config_info['tiles_checked']}\")\n",
    "        print()\n",
    "        \n",
    "        # Print tables for each tile\n",
    "        for tile_id, tile_data in results['tile_comparisons'].items():\n",
    "            print(f\"📍 {tile_id.upper()}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            for var_name, var_comparison in tile_data['variables'].items():\n",
    "                print(f\"\\n📊 Variable: {var_name}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                # Create table data\n",
    "                table_data = []\n",
    "                for metric, values in var_comparison.items():\n",
    "                    if metric == '_separator':\n",
    "                        # Add a visual separator\n",
    "                        table_data.append({\n",
    "                            'Metric': '',\n",
    "                            'Target Dataset': '',\n",
    "                            'Reference Dataset': '',\n",
    "                            'Match?': ''\n",
    "                        })\n",
    "                        table_data.append({\n",
    "                            'Metric': '--- COMPARISON METRICS ---',\n",
    "                            'Target Dataset': '--- COMPARISON METRICS ---', \n",
    "                            'Reference Dataset': '--- COMPARISON METRICS ---',\n",
    "                            'Match?': ''\n",
    "                        })\n",
    "                    else:\n",
    "                        match_symbol = \"✅\" if values['match'] else \"❌\" if values['match'] is not None else \"?\"\n",
    "                        table_data.append({\n",
    "                            'Metric': metric,\n",
    "                            'Target Dataset': str(values['target_dataset']) if values['target_dataset'] is not None else 'None',\n",
    "                            'Reference Dataset': str(values['reference_dataset']) if values['reference_dataset'] is not None else 'None',\n",
    "                            'Match?': match_symbol\n",
    "                        })\n",
    "                \n",
    "                # Create and display DataFrame with wider columns\n",
    "                df = pd.DataFrame(table_data)\n",
    "                pd.set_option('display.max_colwidth', 50)\n",
    "                pd.set_option('display.width', 120)\n",
    "                print(df.to_string(index=False, max_colwidth=50, justify='left'))\n",
    "                print()\n",
    "    \n",
    "    else:\n",
    "        print(\"❌ ERROR: Unexpected results structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd173d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_validation_results(results, config_path_target, config_path_reference, output_dir=\"tile_data/quality_check\"):\n",
    "    \"\"\"Save validation results to JSON file with organized directory structure\"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Extract version info from config paths\n",
    "    def extract_version(config_path):\n",
    "        # Extract version from path like '../config/global_config_v7.txt'\n",
    "        filename = os.path.basename(config_path)\n",
    "        if 'global_config_v' in filename:\n",
    "            version = filename.split('global_config_v')[1].split('.')[0]\n",
    "            return f\"v{version}\"\n",
    "        return \"unknown\"\n",
    "    \n",
    "    target_version = extract_version(config_path_target)\n",
    "    reference_version = extract_version(config_path_reference)\n",
    "    \n",
    "    # Create output directory structure\n",
    "    comparison_dir = f\"quality_check_{reference_version}_{target_version}\"\n",
    "    full_output_dir = Path(output_dir) / comparison_dir\n",
    "    full_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"validation_results_{reference_version}_{target_version}_{timestamp}.json\"\n",
    "    output_path = full_output_dir / filename\n",
    "    \n",
    "    # Add metadata to results\n",
    "    results_with_metadata = {\n",
    "        \"metadata\": {\n",
    "            \"comparison_name\": f\"{reference_version} vs {target_version}\",\n",
    "            \"reference_version\": reference_version,\n",
    "            \"target_version\": target_version,\n",
    "            \"reference_config_path\": config_path_reference,\n",
    "            \"target_config_path\": config_path_target,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"output_path\": str(output_path)\n",
    "        },\n",
    "        \"validation_results\": results\n",
    "    }\n",
    "    \n",
    "    # Custom JSON encoder for handling numpy types\n",
    "    class NumpyEncoder(json.JSONEncoder):\n",
    "        def default(self, obj):\n",
    "            import numpy as np\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, (np.bool_, bool)):\n",
    "                return bool(obj)\n",
    "            return super().default(obj)\n",
    "    \n",
    "    # Save to JSON\n",
    "    try:\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results_with_metadata, f, indent=2, cls=NumpyEncoder)\n",
    "        \n",
    "        print(f\"✅ Validation results saved to: {output_path}\")\n",
    "        print(f\"📁 Directory: {full_output_dir}\")\n",
    "        return str(output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving results: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_validation_results(json_path):\n",
    "    \"\"\"Load validation results from JSON file\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    try:\n",
    "        json_path = Path(json_path)\n",
    "        if not json_path.exists():\n",
    "            print(f\"❌ File not found: {json_path}\")\n",
    "            return None\n",
    "            \n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        print(f\"✅ Loaded validation results from: {json_path}\")\n",
    "        \n",
    "        # Print metadata if available\n",
    "        if \"metadata\" in data:\n",
    "            metadata = data[\"metadata\"]\n",
    "            print(f\"📊 Comparison: {metadata.get('comparison_name', 'Unknown')}\")\n",
    "            print(f\"📅 Timestamp: {metadata.get('timestamp', 'Unknown')}\")\n",
    "            print(f\"🎯 Target: {metadata.get('target_version', 'Unknown')}\")\n",
    "            print(f\"📋 Reference: {metadata.get('reference_version', 'Unknown')}\")\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading results: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def list_validation_results(output_dir=\"tile_data/quality_check\"):\n",
    "    \"\"\"List all available validation result files\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    if not output_path.exists():\n",
    "        print(f\"❌ Directory not found: {output_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Find all JSON files\n",
    "    json_files = list(output_path.rglob(\"validation_results_*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"❌ No validation result files found in {output_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"📁 Found {len(json_files)} validation result files:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    file_info = []\n",
    "    for json_file in sorted(json_files):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            metadata = data.get(\"metadata\", {})\n",
    "            comparison_name = metadata.get(\"comparison_name\", \"Unknown\")\n",
    "            timestamp = metadata.get(\"timestamp\", \"Unknown\")\n",
    "            \n",
    "            # Extract tiles checked from results\n",
    "            validation_results = data.get(\"validation_results\", {})\n",
    "            tiles_checked = validation_results.get(\"dataset_configs\", {}).get(\"tiles_checked\", 0)\n",
    "            \n",
    "            relative_path = json_file.relative_to(output_path)\n",
    "            \n",
    "            print(f\"📄 {relative_path}\")\n",
    "            print(f\"   🔍 Comparison: {comparison_name}\")\n",
    "            print(f\"   📅 Created: {timestamp}\")\n",
    "            print(f\"   🎯 Tiles Checked: {tiles_checked}\")\n",
    "            print()\n",
    "            \n",
    "            file_info.append({\n",
    "                \"path\": str(json_file),\n",
    "                \"relative_path\": str(relative_path),\n",
    "                \"comparison_name\": comparison_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"tiles_checked\": tiles_checked\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {json_file}: {e}\")\n",
    "    \n",
    "    return file_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ca188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset comparison\n",
    "comparison_results = validate_dataset_comparison(\n",
    "    reference_config_path='../config/global_config_v7.txt',\n",
    "    target_config_path='../config/global_config_v8.txt',\n",
    "    n_tiles_to_check=3,\n",
    "    tolerance=1e-3  # Configurable tolerance for matching\n",
    ")\n",
    "print_validation_summary(comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the validation results\n",
    "saved_path = save_validation_results(\n",
    "    comparison_results,\n",
    "    config_path_target='../config/global_config_v8.txt',\n",
    "    config_path_reference='../config/global_config_v7.txt',\n",
    "    output_dir=\"tile_data/quality_check\"  # Adjust path as needed\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"📋 USAGE EXAMPLES:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show how to list available files\n",
    "print(\"\\n🔍 To list all validation files:\")\n",
    "print(\"list_validation_results('tile_data/quality_check')\")\n",
    "\n",
    "if saved_path:\n",
    "    print(f\"\\n📂 To load this specific result:\")\n",
    "    print(f\"loaded_data = load_validation_results('{saved_path}')\")\n",
    "    print(f\"validation_results = loaded_data['validation_results']\")\n",
    "    print(f\"print_validation_summary(validation_results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291dbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate listing available files\n",
    "print(\"\\n🔍 LISTING AVAILABLE VALIDATION FILES:\")\n",
    "print(\"=\"*60)\n",
    "available_files = list_validation_results('tile_data/quality_check')\n",
    "\n",
    "# Demonstrate loading the results back\n",
    "if saved_path:\n",
    "    print(f\"\\n📂 LOADING RESULTS FROM: {saved_path}\")\n",
    "    print(\"=\"*60)\n",
    "    loaded_data = load_validation_results(saved_path)\n",
    "    \n",
    "    if loaded_data:\n",
    "        print(f\"\\n🎯 Loaded data contains:\")\n",
    "        print(f\"   - metadata: {list(loaded_data.get('metadata', {}).keys())}\")\n",
    "        print(f\"   - validation_results: {list(loaded_data.get('validation_results', {}).keys())}\")\n",
    "        \n",
    "        # You can now use the loaded data with print_validation_summary\n",
    "        # print_validation_summary(loaded_data['validation_results'])\n",
    "        print(f\"\\n✅ Ready to use with: print_validation_summary(loaded_data['validation_results'])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec815792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_validation_summary(loaded_data['validation_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eed3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
