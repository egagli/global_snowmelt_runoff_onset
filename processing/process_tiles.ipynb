{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global snowmelt runoff onset processing\n",
    "\n",
    "This notebook implements the core processing pipeline for detecting snowmelt runoff onset timing from Sentinel-1 SAR data at a global scale. The methodology detects the timing of minimum backscatter values, which correspond to snowmelt runoff onset.\n",
    "\n",
    "\n",
    "## Processing pipeline\n",
    "1. **Data acquisition**: Acquire Sentinel-1 RTC data from Microsoft Planetary Computer\n",
    "1. **Snow masking**: Apply spatiotemporal snow cover constraints  \n",
    "1. **Quality filtering**: Remove scenes with insufficient temporal sampling\n",
    "1. **Calculate temporal resolution**: Calculate temporal resolution on the filtered dataset\n",
    "1. **Runoff detection**: Calculate minimum backscatter timing per orbit\n",
    "1. **Aggregate statistics**: Compute 10-year median and MAD for runoff onset, 10-year median for temporal resolution\n",
    "1. **Output generation**: Write results to global zarr store\n",
    "\n",
    "## Last checks!!\n",
    "\n",
    "Before large-scale processing...\n",
    "\n",
    "1. use [egagli/MODIS_seasonal_snow_mask](https://github.com/egagli/MODIS_seasonal_snow_mask) to create the seasonal snow cover dataset (contains: snow appearance date, snow disappearnce date, max consecutive number of snow cover days, all per water year) \n",
    "1. check if config file looks good, make sure to update paths with version number!!!!\n",
    "1. cloud credentials updated if needed (i.e. `config/sas_token.txt` and `ee_key.json`) (note to self: use [egagli/azure_authentication](https://github.com/egagli/azure_authentication) to get new sas_token weekly)\n",
    "1. `select_tiles_to_process.ipynb` ran and `tile_data/global_tiles_with_seasonal_snow.geojson` created\n",
    "1. `create_zarr_store.ipynb` ran and zarr_store exists on cloud storage and can be read\n",
    "1. coiled is working and using spot instances and price isn't too high--check usage stats too!!\n",
    "1. check tiles are being output to `tile_data/tile_results_vX.csv` and tiles are showing success\n",
    "1. check tiles are being processed with `view_maps.ipynb`, check all variables\n",
    "1. validate on test tiles (check automatic weather station tile subset below)\n",
    "1. check failed tiles, potentially adjust cluster settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easysnowdata\n",
    "import pystac_client\n",
    "import tqdm\n",
    "import planetary_computer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import odc.stac\n",
    "import time\n",
    "import dask\n",
    "import dask.distributed\n",
    "import coiled\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "from global_snowmelt_runoff_onset.config import Config, Tile\n",
    "import global_snowmelt_runoff_onset.processing as processing\n",
    "import flox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config/global_config_v7.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start up the coiled cluster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                         n_workers=6,\n",
    "                         worker_memory=\"32 GB\", #coiled.list_instance_types(backend=\"azure\")\n",
    "                         worker_cpu=4,\n",
    "                         #worker_options={\"nthreads\": 1},\n",
    "                         #worker_options={\"nthreads\": 32},# 16 8 4 oversubscribe?\n",
    "                         #scheduler_memory=\"128 GB\",\n",
    "                         #scheduler_memory=\"64 GB\",\n",
    "                         spot_policy=\"spot\", # spot usually\n",
    "                         environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                         workspace=\"uwtacolab\", # azure\n",
    "                         \n",
    "                         )\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "#use the following config for the problem tiles, otherwise 4 and 32, 8 and 32\n",
    "                        #  worker_memory=\"64 GB\", \n",
    "                        #  worker_cpu=8,\n",
    "\n",
    "odc.stac.configure_rio(cloud_defaults=True, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile processing function\n",
    "\n",
    "The `process_tile` function implements the complete processing pipeline for a single spatial tile:\n",
    "\n",
    "### Key processing steps:\n",
    "\n",
    "1. **Sentinel-1 data retrieval**\n",
    "   - Retrieve Sentinel-1 RTC data from Microsoft Planetary Computer\n",
    "   - Organizes by satellite orbit and adds water year coordinates\n",
    "   - Applies optimized chunking for memory management\n",
    "\n",
    "2. **Snow cover masking** \n",
    "   - Retrieves [custom MODIS-derived seasonal snow data](https://github.com/egagli/MODIS_seasonal_snow_mask) per water year: appearance, disappearance, and maximum number of consecutive snow cover days\n",
    "   - Defines pixels with seasonal snow coverage\n",
    "   - Sets temporal detection windows from snow accumulation to disappearance\n",
    "\n",
    "3. **Quality filtering**\n",
    "   - Removes bad scenes and border noise artifacts\n",
    "   - Filters pixels with insufficient temporal sampling\n",
    "   - Calculates maximum temporal gaps per orbit\n",
    "\n",
    "4. **Runoff onset detection**\n",
    "   - Identifies minimum backscatter timing per orbit/polarization\n",
    "   - Aggregates using median for robustness\n",
    "   - Converts to day-of-water-year format\n",
    "\n",
    "5. **Aggregations**\n",
    "   - Computes median and MAD across water years\n",
    "   - Calculates temporal resolution metrics\n",
    "\n",
    "6. **Data output**\n",
    "   - Writes results to global zarr store\n",
    "   - Updates processing status tracking\n",
    "   - Manages memory cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile: Tile):\n",
    "    odc.stac.configure_rio(cloud_defaults=True)\n",
    "    tile.start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        print(f\"Getting data for tile ({tile.row},{tile.col}).\")\n",
    "\n",
    "        s1_rtc_ds = processing.get_sentinel1_rtc(\n",
    "            geobox=tile.geobox,\n",
    "            bands=config.bands,\n",
    "            start_date=config.start_date,\n",
    "            end_date=config.end_date,\n",
    "            chunks_read=config.chunks_s1_read,\n",
    "            fail_on_error=True,\n",
    "        )\n",
    "\n",
    "        s1_rtc_ds[\"vv\"] = s1_rtc_ds[\"vv\"].chunk(config.chunks_s1_process).persist()\n",
    "        print(\"Data retrieved.\")\n",
    "\n",
    "        tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "        spatiotemporal_snow_cover_mask_ds = processing.get_spatiotemporal_snow_cover_mask(\n",
    "            ds=s1_rtc_ds,\n",
    "            bbox_gdf=tile.bbox_gdf,\n",
    "            seasonal_snow_mask_store=config.seasonal_snow_mask_store,\n",
    "            extend_search_window_beyond_SDD_days=config.extend_search_window_beyond_SDD_days,\n",
    "            min_consec_snow_days_for_seasonal_snow=config.min_consec_snow_days_for_seasonal_snow,\n",
    "            reproject_method=\"rasterio\", #rasterio\n",
    "        ).persist()\n",
    "\n",
    "        if config.mountain_snow_only:\n",
    "            gmba_clipped_gdf = processing.get_gmba_mountain_inventory(tile.bbox_gdf)\n",
    "        else:\n",
    "            gmba_clipped_gdf = None\n",
    "\n",
    "        print(\"Applying masks...\")\n",
    "        s1_rtc_masked_ds = processing.apply_all_masks(\n",
    "            s1_rtc_ds=s1_rtc_ds,\n",
    "            gmba_clipped_gdf=gmba_clipped_gdf,\n",
    "            spatiotemporal_snow_cover_mask_ds=spatiotemporal_snow_cover_mask_ds,\n",
    "            water_years=config.water_years,\n",
    "        )\n",
    "\n",
    "        print(\"Removing bad scenes and border noise...\")\n",
    "        s1_rtc_masked_ds = processing.remove_bad_scenes_and_border_noise(\n",
    "            s1_rtc_masked_ds, config.low_backscatter_threshold\n",
    "        )\n",
    "        print(\"Bad scenes and border noise removed.\")\n",
    "\n",
    "        print(\"Filtering by acquisitions and gaps...\")\n",
    "        s1_rtc_masked_filtered_ds = s1_rtc_masked_ds.groupby(\"water_year\").map(\n",
    "            lambda group: processing.filter_insufficient_pixels_per_orbit(\n",
    "                s1_rtc_masked_ds=group,\n",
    "                spatiotemporal_snow_cover_mask_ds=spatiotemporal_snow_cover_mask_ds,\n",
    "                min_monthly_acquisitions=config.min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit=config.max_allowed_days_gap_per_orbit,\n",
    "            )\n",
    "        ).persist()\n",
    "        print(\"Filtering completed.\")\n",
    "\n",
    "        print(\"Calculating temporal resolution...\")\n",
    "        temporal_resolution_da = processing.get_temporal_resolution(\n",
    "            s1_rtc_masked_filtered_ds, spatiotemporal_snow_cover_mask_ds\n",
    "        ).persist()\n",
    "\n",
    "        tile_median_temporal_resolution = temporal_resolution_da.median(\n",
    "            dim=[\"latitude\", \"longitude\"]\n",
    "        )\n",
    "        tile_pixel_count = temporal_resolution_da.count(dim=[\"latitude\", \"longitude\"])\n",
    "\n",
    "        for water_year in config.water_years:\n",
    "            if water_year in tile_median_temporal_resolution.water_year:\n",
    "                temporal_resolution = tile_median_temporal_resolution.sel(\n",
    "                    water_year=water_year\n",
    "                ).values\n",
    "                setattr(tile, f\"tr_{water_year}\", round(float(temporal_resolution), 3))\n",
    "\n",
    "            if water_year in tile_pixel_count.water_year:\n",
    "                pixel_count = tile_pixel_count.sel(water_year=water_year).values\n",
    "                setattr(tile, f\"pix_ct_{water_year}\", int(pixel_count))\n",
    "\n",
    "        print(\"Temporal resolution calculated.\")\n",
    "\n",
    "        print(\"Calculating runoff onsets...\")\n",
    "        runoff_onsets_da = s1_rtc_masked_filtered_ds.groupby(\"water_year\").apply(\n",
    "            processing.calculate_runoff_onset,\n",
    "            returned_dates_format=\"dowy\",\n",
    "            return_constituent_runoff_onsets=False,\n",
    "        )\n",
    "        print(\"Runoff onsets calculated.\")\n",
    "\n",
    "        tile.runoff_onsets_dims = dict(runoff_onsets_da.sizes)\n",
    "\n",
    "        # Calculate median and MAD\n",
    "        median_da, mad_da = processing.median_and_mad_with_min_obs(\n",
    "            da=runoff_onsets_da,\n",
    "            dim=\"water_year\",\n",
    "            min_count=config.min_years_for_median_std\n",
    "        )\n",
    "\n",
    "        # Calculate median temporal resolution\n",
    "        median_temporal_resolution_da = processing.median_with_min_obs(\n",
    "            da=temporal_resolution_da,\n",
    "            dim=\"water_year\",\n",
    "            min_count=config.min_years_for_median_std\n",
    "        )\n",
    "\n",
    "        # Create dataset\n",
    "        runoff_onsets_ds = processing.dataarrays_to_dataset(\n",
    "            runoff_onsets_da=runoff_onsets_da,\n",
    "            median_da=median_da,\n",
    "            mad_da=mad_da,\n",
    "            water_years=config.water_years,\n",
    "            temporal_resolution_da=temporal_resolution_da,\n",
    "            median_temporal_resolution_da=median_temporal_resolution_da,\n",
    "        )\n",
    "\n",
    "        print(\"Median and MAD calculated, converted to dataset.\")\n",
    "\n",
    "        # Reindex to global coordinates\n",
    "        global_ds = xr.open_zarr(config.global_runoff_store, consolidated=True)\n",
    "        print(\"Global dataset opened.\")\n",
    "        global_subset_ds = global_ds.sel(\n",
    "            latitude=runoff_onsets_ds.latitude,\n",
    "            longitude=runoff_onsets_ds.longitude,\n",
    "            method=\"nearest\",\n",
    "        )\n",
    "        print(\"Global dataset subsetted.\")\n",
    "        runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(\n",
    "            latitude=global_subset_ds.latitude, longitude=global_subset_ds.longitude\n",
    "        )\n",
    "        print(\"Dataset reindexed.\")\n",
    "\n",
    "        # Write to Zarr\n",
    "        runoff_onsets_reindexed_ds.drop_vars(\"spatial_ref\").chunk(\n",
    "            config.chunks_zarr_output\n",
    "        ).to_zarr(\n",
    "            config.global_runoff_store, region=\"auto\", mode=\"r+\", consolidated=True\n",
    "        )\n",
    "        print(\"Dataset written to Zarr.\")\n",
    "\n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = True\n",
    "\n",
    "        # Clean up memory\n",
    "        del (\n",
    "            s1_rtc_ds,\n",
    "            spatiotemporal_snow_cover_mask_ds,\n",
    "            s1_rtc_masked_ds,\n",
    "            s1_rtc_masked_filtered_ds,\n",
    "            temporal_resolution_da,\n",
    "            runoff_onsets_da,\n",
    "            runoff_onsets_ds,\n",
    "            global_subset_ds,\n",
    "            runoff_onsets_reindexed_ds,\n",
    "            median_da,\n",
    "            mad_da,\n",
    "            median_temporal_resolution_da,\n",
    "            tile_median_temporal_resolution,\n",
    "            tile_pixel_count,\n",
    "            gmba_clipped_gdf,\n",
    "            global_ds,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        tile.error_messages.append(str(e))\n",
    "        tile.error_messages.append(traceback.format_exc())\n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = False\n",
    "\n",
    "    return tile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a single tile\n",
    "\n",
    "For testing, individual tiles can be processed to verify the pipeline before large-scale deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles = config.get_list_of_tiles(which='all')\n",
    "# tile=tiles[0]\n",
    "tile = config.get_tile(23,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(process_tile, tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.result().success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_result = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    [[getattr(computed_result, f) for f in config.fields]],\n",
    "    columns=config.fields,\n",
    ")\n",
    "df\n",
    "# rio 250 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    # print the temporal resolutions\n",
    "   if col.startswith('tr_'):\n",
    "       print(f\"Temporal resolution for {col}: {df[col].values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile selection and testing\n",
    "\n",
    "Select tiles for processing based on different criteria:\n",
    "- **`'all'`**: all global coverage\n",
    "- **`'processed'`**: successfully completed tiles  \n",
    "- **`'failed'`**: tiles that encountered errors\n",
    "- **`'unprocessed'`**: tiles not yet attempted\n",
    "- **`'unprocessed_and_failed'`**: tiles needing processing or reprocessing. Unless you have a specific need / debugging, you should probably use this one.\n",
    "- **`'unprocessed_and_failed_weather_stations'`**: unprocessed/failed tiles that contain automatic weather stations. Useful for validation.\n",
    "\n",
    "\n",
    "Provide one of these arguments to `config.get_list_of_tiles(which=)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiles = config.get_list_of_tiles(which='unprocessed_and_failed')\n",
    "tiles = config.get_list_of_tiles(which='unprocessed_and_failed_weather_stations') # run this to process the tiles with weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "\n",
    "for tile_batch in tqdm.tqdm(tile_batches, total=len(tile_batches)):\n",
    "\n",
    "    futures = [client.submit(process_tile, tile) for tile in tile_batch] #, retries=0\n",
    "\n",
    "    successful_tiles = []\n",
    "\n",
    "    try:\n",
    "        for future, computed_result in dask.distributed.as_completed(futures, with_results=True, timeout=1600):\n",
    "            successful_tiles.append(computed_result)\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                [[getattr(computed_result, f) for f in config.fields]],\n",
    "                columns=config.fields,\n",
    "            )\n",
    "            df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "            print(f'Tile ({computed_result.row},{computed_result.col}) completed')\n",
    "    except Exception as e:\n",
    "        for tile in tile_batch:\n",
    "            if tile.index not in [computed_tile.index for computed_tile in successful_tiles]:\n",
    "\n",
    "                df = pd.DataFrame([[getattr(tile, f) for f in config.fields]],columns=config.fields,)\n",
    "                df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "\n",
    "                print(f'Tile ({tile.row},{tile.col}) failed')\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
