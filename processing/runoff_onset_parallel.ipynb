{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create global snowmelt runoff onset product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easysnowdata\n",
    "import pystac_client\n",
    "import tqdm\n",
    "import planetary_computer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "import odc.stac\n",
    "import time\n",
    "import dask\n",
    "import dask.distributed\n",
    "import coiled\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "from global_snowmelt_runoff_onset.config import Config, Tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config/global_config.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel1_rtc(geobox):\n",
    "\n",
    "    items = (\n",
    "        pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",modifier=planetary_computer.sign_inplace)\n",
    "        .search(\n",
    "            intersects=geobox.geographic_extent,\n",
    "            collections=[\"sentinel-1-rtc\"],\n",
    "            datetime=(config.start_date, config.end_date),\n",
    "        )\n",
    "        .item_collection()\n",
    "    )\n",
    "\n",
    "    #chunks_read = {'latitude':2048,'longitude':2048,'time':100}\n",
    "    chunks_read = {'latitude':512,'longitude':512,'time':20}\n",
    "    #chunks_read = {'latitude':256,'longitude':256,'time':100}\n",
    "\n",
    "    #chunks_read = {'latitude':1024,'longitude':1024,'time':-1}\n",
    "    #chunks_read = {'latitude':1024,'longitude':1024,'time':1}\n",
    "    #chunks_read = {'latitude':512,'longitude':512,'time':1}\n",
    "\n",
    "    load_params = {\n",
    "        \"items\": items,\n",
    "        \"bands\": [\"vv\"],\n",
    "        \"nodata\": -32768,\n",
    "        \"chunks\": config.chunks_read, #config.chunks_read\n",
    "        \"groupby\": \"sat:absolute_orbit\",\n",
    "        \"geobox\":geobox,\n",
    "        \"resampling\": \"bilinear\",\n",
    "        \"fail_on_error\":False\n",
    "    }\n",
    "\n",
    "\n",
    "    s1_rtc_ds = odc.stac.load(**load_params).sortby(\"time\")#.chunk(chunks_read)#.chunk(chunks_compute) # rechunk?\n",
    "\n",
    "    metadata = gpd.GeoDataFrame.from_features(items, \"epsg:4326\")\n",
    "\n",
    "    metadata_groupby_gdf = (\n",
    "        metadata.groupby([\"sat:absolute_orbit\"]).first().sort_values(\"datetime\")\n",
    "    )\n",
    "\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"sat:orbit_state\": (\"time\", metadata_groupby_gdf[\"sat:orbit_state\"]),\n",
    "        \"sat:relative_orbit\": (\"time\", metadata_groupby_gdf[\"sat:relative_orbit\"].astype(\"int16\"))\n",
    "    })\n",
    "\n",
    "    #s1_rtc_ds = s1_rtc_ds.drop_vars(['hh','hv'],errors='ignore')\n",
    "\n",
    "    epsg = s1_rtc_ds.rio.estimate_utm_crs().to_epsg()\n",
    "    hemisphere = 'northern' if epsg < 32700 else 'southern'\n",
    "\n",
    "    s1_rtc_ds.attrs['hemisphere'] = hemisphere\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"water_year\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere))),\n",
    "        \"DOWY\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)))\n",
    "    })        \n",
    "\n",
    "    return s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_all_masks(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds):\n",
    "\n",
    "    s1_rtc_ds = remove_unwanted_water_years(s1_rtc_ds)\n",
    "\n",
    "    center_lat = (s1_rtc_ds.rio.bounds()[1]+s1_rtc_ds.rio.bounds()[3])/2\n",
    "    if np.absolute(center_lat) < 3:\n",
    "        s1_rtc_ds = remove_equator_crossing(s1_rtc_ds)\n",
    "        \n",
    "    s1_rtc_ds = s1_rtc_ds.rio.clip_box(*gmba_clipped_gdf.total_bounds,crs=gmba_clipped_gdf.crs)\n",
    "    s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry,drop=True) # does this compute?\n",
    "\n",
    "    s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "    \n",
    "    return s1_rtc_masked_ds\n",
    "\n",
    "def remove_unwanted_water_years(s1_rtc_ds):\n",
    "    s1_rtc_ds = s1_rtc_ds.sel(time=s1_rtc_ds.water_year.isin(config.water_years))\n",
    "    return s1_rtc_ds\n",
    "\n",
    "\n",
    "def remove_equator_crossing(s1_rtc_ds):\n",
    "    if s1_rtc_ds.attrs['hemisphere'] == 'northern':\n",
    "        mask = s1_rtc_ds.latitude >= 0\n",
    "    else:\n",
    "        mask = s1_rtc_ds.latitude < 0\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.where(mask)\n",
    "    return s1_rtc_ds\n",
    "\n",
    "def get_gmba_mountain_inventory(bbox_gdf):\n",
    "    url = (f\"https://data.earthenv.org/mountains/standard/GMBA_Inventory_v2.0_standard_300.zip\")\n",
    "    gmba_gdf = gpd.read_file(\"zip+\" + url)\n",
    "    gmba_clipped_gdf = gpd.clip(gmba_gdf, bbox_gdf)\n",
    "    return gmba_clipped_gdf\n",
    "\n",
    "def get_custom_seasonal_snow_mask(s1_rtc_ds,bbox_gdf,geobox):\n",
    "    seasonal_snow_mask = xr.open_zarr(config.seasonal_snow_mask_store, consolidated=True, decode_coords='all') \n",
    "    seasonal_snow_mask_clip_ds = seasonal_snow_mask.rio.clip_box(*bbox_gdf.total_bounds,crs='EPSG:4326') # clip to correct box, maybe use total_bounds and then use crs \n",
    "    seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=slice(0,5)).max(dim='time')).rename({'x':'longitude','y':'latitude'}) # if S1 scene at t=0 isn't full, does this get rid of mask values?\n",
    "    #seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.odc.reproject(geobox)#.rename({'x':'longitude','y':'latitude'})\n",
    "    return seasonal_snow_mask_matched_ds\n",
    "\n",
    "\n",
    "def apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds):\n",
    "    s1_rtc_masked_ds = s1_rtc_ds.groupby('water_year').map(lambda group: apply_mask_for_year(group, seasonal_snow_mask_matched_ds))\n",
    "    s1_rtc_masked_ds.rio.write_crs(s1_rtc_ds.rio.crs,inplace=True)\n",
    "    return s1_rtc_masked_ds\n",
    "\n",
    "def apply_mask_for_year(group, seasonal_snow_mask_matched_ds):\n",
    "\n",
    "    year = group.water_year.values[0]\n",
    "\n",
    "\n",
    "    if year not in seasonal_snow_mask_matched_ds.water_year:\n",
    "        print(f\"Warning: water_year {year} not found in seasonal_snow_mask_matched_ds\")\n",
    "        return group.where(False) \n",
    "\n",
    "    sad_mask = group['DOWY'] >= seasonal_snow_mask_matched_ds['SAD_DOWY'].sel(water_year=year)\n",
    "    sdd_mask = group['DOWY'] <= seasonal_snow_mask_matched_ds['SDD_DOWY'].sel(water_year=year)\n",
    "    consec_mask = seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=year) >= 56\n",
    "    combined_mask = sad_mask & sdd_mask & consec_mask\n",
    "    return group.where(combined_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xr_datetime_to_DOWY(date_da, hemisphere=\"northern\"):\n",
    "    \"\"\"\n",
    "    Converts an xarray DataArray containing datetime objects to the Day of Water Year (DOWY).\n",
    "\n",
    "    Parameters:\n",
    "    date (xr.DataArray): An xarray DataArray with datetime64 data type.\n",
    "    hemisphere (str): 'northern' or 'southern'\n",
    "\n",
    "    Returns:\n",
    "    xr.DataArray: An xarray DataArray containing the DOWY for each datetime in the input DataArray.\n",
    "    \"\"\"\n",
    "\n",
    "    if date_da.attrs.get(\"any_valid_date\") is not None:\n",
    "        any_valid_date = pd.to_datetime(date_da.attrs[\"any_valid_date\"])\n",
    "    else:\n",
    "        any_valid_date = pd.to_datetime(date_da.sel(x=0, y=0, method=\"nearest\").values)\n",
    "\n",
    "    start_of_water_year = easysnowdata.utils.get_water_year_start(\n",
    "        any_valid_date, hemisphere=hemisphere\n",
    "    )\n",
    "\n",
    "    return xr.apply_ufunc(\n",
    "        lambda x: (x - start_of_water_year).days + 1,  # dt accessor?\n",
    "        date_da,\n",
    "        input_core_dims=[[]],\n",
    "        vectorize=True,\n",
    "        dask=\"parallelized\",  # try allowed also\n",
    "        output_dtypes=[float],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runoff_onset(\n",
    "    s1_rtc_ds: xr.Dataset,\n",
    "    min_monthly_acquisitions: int,\n",
    "    max_allowed_days_gap_per_orbit: int,\n",
    "    consec_snow_days_da: xr.DataArray,\n",
    "    return_constituent_runoff_onsets: bool = False,\n",
    "    returned_dates_format: str = \"dowy\",\n",
    "    low_backscatter_threshold: float = 0.001,\n",
    "    report_temporal_res: bool = False,\n",
    "):\n",
    "\n",
    "\n",
    "    s1_rtc_ds = remove_bad_scenes_and_border_noise(s1_rtc_ds, low_backscatter_threshold)\n",
    "\n",
    "    #pixelwise_counts_per_orbit_and_polarization_ds = (count_acquisitions_per_orbit_and_polarization(s1_rtc_ds))  # this should be for melt, keeping general for now to integrate modis data\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds, max_days_gap_per_orbit_da = count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds)\n",
    "\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds = (calculate_backscatter_min_per_orbit(s1_rtc_ds))\n",
    "\n",
    "\n",
    "    if report_temporal_res:\n",
    "        constituent_runoff_onsets_da, temporal_resolution, pixel_count = ( # , temporal_resolution, pixel_count\n",
    "            filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "                backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "                pixelwise_counts_per_orbit_and_polarization_ds,\n",
    "                max_days_gap_per_orbit_da,\n",
    "                consec_snow_days_da,\n",
    "                min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit,\n",
    "                report_temporal_res\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        constituent_runoff_onsets_da = ( # , temporal_resolution, pixel_count\n",
    "            filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "                backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "                pixelwise_counts_per_orbit_and_polarization_ds,\n",
    "                max_days_gap_per_orbit_da,\n",
    "                consec_snow_days_da,\n",
    "                min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit,\n",
    "                report_temporal_res\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if return_constituent_runoff_onsets == False:\n",
    "        runoff_onset_da = calculate_runoff_onset_from_constituent_runoff_onsets(constituent_runoff_onsets_da)\n",
    "    else:\n",
    "        runoff_onset_da = constituent_runoff_onsets_da\n",
    "\n",
    "\n",
    "    if returned_dates_format == \"dowy\":\n",
    "\n",
    "        hemisphere = (\n",
    "            \"northern\"\n",
    "            if s1_rtc_ds.rio.estimate_utm_crs().to_epsg() < 32700\n",
    "            else \"southern\"\n",
    "        )\n",
    "        month_start = 10 if hemisphere == \"northern\" else 4\n",
    "        print(\n",
    "            f\"Area is in the {hemisphere} hemisphere. Water year starts in month {month_start}.\"\n",
    "        )\n",
    "        runoff_onset_da.attrs[\"any_valid_date\"] = s1_rtc_ds.time[0].values\n",
    "        runoff_onset_da = xr_datetime_to_DOWY(runoff_onset_da, hemisphere=hemisphere)\n",
    "\n",
    "    elif returned_dates_format == \"doy\":\n",
    "        runoff_onset_da = runoff_onset_da.dt.dayofyear\n",
    "    elif returned_dates_format == \"datetime64\":\n",
    "        runoff_onset_da = runoff_onset_da\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'returned_dates_format must be either \"doy\", \"dowy\", or \"datetime64\".'\n",
    "        )\n",
    "\n",
    "    if report_temporal_res:\n",
    "        return runoff_onset_da, temporal_resolution, pixel_count  \n",
    "    else:\n",
    "        return runoff_onset_da\n",
    "\n",
    "def remove_bad_scenes_and_border_noise(da, threshold):\n",
    "    cutoff_date = np.datetime64('2018-03-14')\n",
    "    \n",
    "    original_crs = da.rio.crs\n",
    "    \n",
    "    result = xr.where(\n",
    "        da.time < cutoff_date,\n",
    "        da.where(da > threshold),\n",
    "        da.where(da > 0)\n",
    "    )\n",
    "    \n",
    "    result.rio.write_crs(original_crs, inplace=True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# def count_acquisitions_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "#     print(\"Calculating pixelwise counts per orbit and polarization...\")\n",
    "#     pixelwise_counts_per_orbit_and_polarization = s1_rtc_ds.groupby(\n",
    "#         \"sat:relative_orbit\"\n",
    "#     ).count(dim=\"time\", engine='flox')\n",
    "#     return pixelwise_counts_per_orbit_and_polarization\n",
    "\n",
    "def count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "    print(\"Calculating pixelwise counts and maximum gaps per orbit and polarization...\")\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds = s1_rtc_ds.groupby(\"sat:relative_orbit\").count(dim=\"time\") #, engine='flox'\n",
    "    \n",
    "    def calc_max_gap(group):\n",
    "            times = group.time.sortby('time')\n",
    "            if len(times) == 1: # if only one scene in this group, set gap to very large number so it won't be calculated\n",
    "                return times.count()*9999\n",
    "            gaps = times.diff(dim='time').max().dt.days\n",
    "            return gaps\n",
    "\n",
    "    max_time_gap_per_orbit_days_da = s1_rtc_ds.groupby(\"sat:relative_orbit\").map(calc_max_gap)\n",
    "    \n",
    "    return pixelwise_counts_per_orbit_and_polarization_ds, max_time_gap_per_orbit_days_da\n",
    "\n",
    "\n",
    "def calculate_backscatter_min_per_orbit(s1_rtc_ds: xr.Dataset):\n",
    "    print(\"Calculating backscatter min per orbit...\")\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds = s1_rtc_ds.groupby(\n",
    "        \"sat:relative_orbit\"\n",
    "    ).map(lambda c: c.idxmin(dim=\"time\")) # maybe only map if if max-min > -1dB\n",
    "    return backscatter_min_timing_per_orbit_and_polarization_ds\n",
    "\n",
    "\n",
    "# def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#     backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "#     pixelwise_counts_per_orbit: xr.Dataset,\n",
    "#     consec_snow_days_da: xr.DataArray,\n",
    "#     min_monthly_acquisitions: int,\n",
    "# ):\n",
    "    \n",
    "#     print(f\"Filtering insufficient pixels per orbit and polarization, must have at least {min_monthly_acquisitions} per month...\")\n",
    "#     constituent_runoff_onsets_ds = (\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds.where(\n",
    "#             pixelwise_counts_per_orbit >= (min_monthly_acquisitions*(consec_snow_days_da/30))\n",
    "#         )\n",
    "#     )\n",
    "#     return constituent_runoff_onsets_ds\n",
    "\n",
    "def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "    backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "    pixelwise_counts_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "    max_days_gap_per_orbit_da: xr.DataArray,\n",
    "    consec_snow_days_da: xr.DataArray,\n",
    "    min_monthly_acquisitions: int,\n",
    "    max_allowed_days_gap_per_orbit: int,\n",
    "    report_temporal_res: bool,\n",
    "):\n",
    "    print(f\"Filtering insufficient pixels per orbit and polarization...\")\n",
    "\n",
    "    #pixelwise_counts_per_orbit_and_polarization_ds = pixelwise_counts_per_orbit_and_polarization_ds.persist()\n",
    "    insufficient_mask = (pixelwise_counts_per_orbit_and_polarization_ds >= (min_monthly_acquisitions*(consec_snow_days_da/30))) & (max_days_gap_per_orbit_da <= max_allowed_days_gap_per_orbit) & (pixelwise_counts_per_orbit_and_polarization_ds>0)\n",
    "    \n",
    "\n",
    "    constituent_runoff_onsets_ds = backscatter_min_timing_per_orbit_and_polarization_ds.where(insufficient_mask)\n",
    "    constituent_runoff_onsets_da = constituent_runoff_onsets_ds.to_dataarray(dim=\"polarization\")\n",
    "\n",
    "    if not report_temporal_res:\n",
    "        return constituent_runoff_onsets_da\n",
    "    else:\n",
    "        temporal_resolution_da = consec_snow_days_da / (pixelwise_counts_per_orbit_and_polarization_ds.where(insufficient_mask)['vv'].sum(dim='sat:relative_orbit').where(lambda x: x>0))\n",
    "        temporal_resolution = temporal_resolution_da.mean(dim=['latitude','longitude'],skipna=True)\n",
    "        pixel_count = temporal_resolution_da.count(dim=['latitude','longitude'])\n",
    "        return constituent_runoff_onsets_da, temporal_resolution, pixel_count\n",
    "\n",
    "\n",
    "def calculate_runoff_onset_from_constituent_runoff_onsets(constituent_runoff_onsets_da: xr.DataArray,):\n",
    "    print(\"Calculating runoff onset from constituent runoff onsets...\")\n",
    "    runoff_onset_da = (\n",
    "        constituent_runoff_onsets_da.astype(\"int64\")\n",
    "        .where(lambda x: x > 0)\n",
    "        .median(dim=[\"sat:relative_orbit\", \"polarization\"], skipna=True)\n",
    "        .astype(\"datetime64[ns]\")\n",
    "    )  #\n",
    "    return runoff_onset_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_runoff_onset_wrapper(ds, consec_snow_days_da, min_monthly_acquisitions, max_allowed_days_gap_per_orbit, returned_dates_format, return_constituent_runoff_onsets, low_backscatter_threshold, report_temporal_res, tile):\n",
    "    \n",
    "    water_year = ds.water_year.values[0]\n",
    "\n",
    "    print(f'calculating for WY {water_year}...')\n",
    "\n",
    "    if water_year not in consec_snow_days_da.water_year:\n",
    "        print(f\"Warning: water_year {water_year} not found in consec_snow_days_da\")\n",
    "        consec_snow_days_slice = consec_snow_days_da.sel(water_year=water_year, method='nearest').where(False,other=9999) # if water year does not exist in the consec_snow_days_da, set to 9999 so no values are calculated in calculate_runoff_onset...\n",
    "    else:\n",
    "        consec_snow_days_slice = consec_snow_days_da.sel(water_year=water_year)\n",
    "    \n",
    "    if report_temporal_res:\n",
    "        runoff_onset_da, temporal_resolution, pixel_count = calculate_runoff_onset( #, temporal_resolution, pixel_count\n",
    "            ds,\n",
    "            consec_snow_days_da=consec_snow_days_slice,\n",
    "            min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "            max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "            returned_dates_format=returned_dates_format,\n",
    "            return_constituent_runoff_onsets=return_constituent_runoff_onsets,\n",
    "            low_backscatter_threshold=low_backscatter_threshold,\n",
    "            report_temporal_res=report_temporal_res,\n",
    "        )\n",
    "\n",
    "        setattr(tile, f'tr_{water_year}', round(float(temporal_resolution),3))\n",
    "        setattr(tile, f'pix_ct_{water_year}', int(pixel_count))\n",
    "    else:\n",
    "        runoff_onset_da = calculate_runoff_onset( #, temporal_resolution, pixel_count\n",
    "            ds,\n",
    "            consec_snow_days_da=consec_snow_days_slice,\n",
    "            min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "            max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "            returned_dates_format=returned_dates_format,\n",
    "            return_constituent_runoff_onsets=return_constituent_runoff_onsets,\n",
    "            low_backscatter_threshold=low_backscatter_threshold,\n",
    "            report_temporal_res=report_temporal_res,\n",
    "        )\n",
    "    \n",
    "    return runoff_onset_da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataarrays_to_dataset(runoff_onsets_da, median_da, std_da):\n",
    "\n",
    "    runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset').round().astype('uint16')\n",
    "    runoff_onsets_ds = runoff_onsets_ds.reindex(water_year=config.water_years)\n",
    "    runoff_onsets_ds['runoff_onset_median'] = median_da.round().astype('uint16')\n",
    "    runoff_onsets_ds['runoff_onset_std'] = std_da\n",
    "    \n",
    "    return runoff_onsets_ds\n",
    "\n",
    "def median_and_std_with_min_obs(da, dim, min_count):\n",
    "    count_mask = da.notnull().sum(dim=dim) >= min_count\n",
    "    \n",
    "    median = da.where(count_mask).median(dim=dim)\n",
    "    std = da.where((count_mask) & (median>0)).std(dim=dim)\n",
    "    \n",
    "    return median, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tile(tile: Tile):\n",
    "    tile.start_time = time.time()\n",
    "    geobox = tile.geobox\n",
    "    bbox_gdf = tile.bbox_gdf\n",
    "    \n",
    "    try:\n",
    "\n",
    "        print(f'Getting data for tile ({tile.row},{tile.col}).')\n",
    "        s1_rtc_ds = get_sentinel1_rtc(geobox).compute()\n",
    "        print('Data retrieved.')\n",
    "\n",
    "        \n",
    "\n",
    "        #s1_rtc_ds = s1_rtc_ds.persist()\n",
    "        \n",
    "        tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "        # if tile.s1_rtc_ds_dims['time'] > 1700:\n",
    "        #     tile.error_messages.append('Over 1700 scenes, process with more worker RAM')\n",
    "        #     tile.total_time = time.time() - tile.start_time\n",
    "        #     tile.success = False\n",
    "        #     return tile\n",
    "        \n",
    "        #s1_rtc_ds = s1_rtc_ds.rio.clip_box(*gmba_clipped_gdf.total_bounds,crs=gmba_clipped_gdf.crs).persist() # clip to correct box, maybe use total_bounds and then use crs\n",
    "\n",
    "        seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds, bbox_gdf, geobox)\n",
    "        gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "        \n",
    "        print('Applying masks...')\n",
    "        s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds, gmba_clipped_gdf, seasonal_snow_mask_matched_ds)\n",
    "        print('Masks applied.')\n",
    "\n",
    "        print('Calculating runoff onsets...')\n",
    "        runoff_onsets_da = (\n",
    "            s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "            .apply(\n",
    "                calculate_runoff_onset_wrapper,\n",
    "                consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "                min_monthly_acquisitions=config.min_monthly_acquisitions,\n",
    "                max_allowed_days_gap_per_orbit=config.max_allowed_days_gap_per_orbit,\n",
    "                returned_dates_format=\"dowy\",\n",
    "                return_constituent_runoff_onsets=False,\n",
    "                low_backscatter_threshold=config.low_backscatter_threshold,\n",
    "                report_temporal_res=False,\n",
    "                tile=tile,\n",
    "            )\n",
    "        )\n",
    "        print('Runoff onsets calculated.')\n",
    "        \n",
    "        tile.runoff_onsets_dims = dict(runoff_onsets_da.sizes)\n",
    "        median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', config.min_years_for_median_std)\n",
    "        runoff_onsets_ds = dataarrays_to_dataset(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "        print('Median and std calculated, converted to dataset.')\n",
    "\n",
    "        global_store = config.global_runoff_store\n",
    "        \n",
    "        global_ds = xr.open_zarr(global_store, consolidated=True)\n",
    "        print('Global dataset opened.')\n",
    "        global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude, longitude=runoff_onsets_ds.longitude, method='nearest')\n",
    "        print('Global dataset subsetted.')\n",
    "        runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude, longitude=global_subset_ds.longitude)\n",
    "        print('Dataset reindexed.')\n",
    "        # Write to Zarr\n",
    "        runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk(config.chunks_write).to_zarr(\n",
    "            global_store, region=\"auto\", mode=\"r+\", consolidated=True\n",
    "        )\n",
    "        print('Dataset written to Zarr.')\n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = True\n",
    "    except Exception as e:\n",
    "        tile.error_messages.append(str(e))\n",
    "        tile.error_messages.append(traceback.format_exc())\n",
    "        tile.total_time = time.time() - tile.start_time\n",
    "        tile.success = False\n",
    "    \n",
    "    return tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coiled.list_instance_types(backend=\"azure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                         #shutdown_on_close=False,\n",
    "                         #wait_for_workers=True,\n",
    "                         #n_workers=[41,170], # 170\n",
    "                         #n_workers=[31,86],\n",
    "                         n_workers=41,\n",
    "                         #n_workers=8,\n",
    "                         #n_workers=10,\n",
    "                         worker_memory=\"64 GB\", #coiled.list_instance_types(backend=\"azure\")\n",
    "                         worker_cpu=8,\n",
    "                         #worker_options={\"nthreads\": 32},# 16 8 4 oversubscribe?\n",
    "                         #scheduler_memory=\"128 GB\",\n",
    "                         scheduler_memory=\"128 GB\",\n",
    "                         spot_policy=\"spot\", # spot usually\n",
    "                         #software=\"sar_snowmelt_timing\",\n",
    "                         environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                         #container=\"mcr.microsoft.com/planetary-computer/python:latest\",\n",
    "                         workspace=\"uwtacolab\",\n",
    "                         \n",
    "                         )\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "#use the following config for the problem tiles, otherwise 4 and 32, 8 and 32\n",
    "                        #  worker_memory=\"64 GB\", \n",
    "                        #  worker_cpu=8,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK CLIP_BOX issue before running again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odc.stac.configure_rio(cloud_defaults=True, client=client)\n",
    "odc.stac.configure_rio(cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = config.fields\n",
    "tiles = config.get_list_of_tiles(which='unprocessed_and_failed')\n",
    "#tiles.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentinel1_rtc(geobox):\n",
    "\n",
    "    items = (\n",
    "        pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",modifier=planetary_computer.sign_inplace)\n",
    "        .search(\n",
    "            intersects=geobox.geographic_extent,\n",
    "            collections=[\"sentinel-1-rtc\"],\n",
    "            datetime=(config.start_date, config.end_date),\n",
    "        )\n",
    "        .item_collection()\n",
    "    )\n",
    "\n",
    "    #chunks_read = {'latitude':2048,'longitude':2048,'time':100}\n",
    "    chunks_read = {'latitude':512,'longitude':512,'time':20}\n",
    "    #chunks_read = {'latitude':256,'longitude':256,'time':100}\n",
    "\n",
    "    #chunks_read = {'latitude':1024,'longitude':1024,'time':-1}\n",
    "    #chunks_read = {'latitude':1024,'longitude':1024,'time':1}\n",
    "    #chunks_read = {'latitude':512,'longitude':512,'time':1}\n",
    "\n",
    "    load_params = {\n",
    "        \"items\": items,\n",
    "        \"bands\": [\"vv\"],\n",
    "        \"nodata\": -32768,\n",
    "        \"chunks\": config.chunks_read, #config.chunks_read\n",
    "        \"groupby\": \"sat:absolute_orbit\",\n",
    "        \"geobox\":geobox,\n",
    "        \"resampling\": \"bilinear\",\n",
    "        \"fail_on_error\":False\n",
    "    }\n",
    "\n",
    "\n",
    "    s1_rtc_ds = odc.stac.load(**load_params).sortby(\"time\")#.chunk(chunks_read)#.chunk(chunks_compute) # rechunk?\n",
    "\n",
    "    metadata = gpd.GeoDataFrame.from_features(items, \"epsg:4326\")\n",
    "\n",
    "    metadata_groupby_gdf = (\n",
    "        metadata.groupby([\"sat:absolute_orbit\"]).first().sort_values(\"datetime\")\n",
    "    )\n",
    "\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"sat:orbit_state\": (\"time\", metadata_groupby_gdf[\"sat:orbit_state\"]),\n",
    "        \"sat:relative_orbit\": (\"time\", metadata_groupby_gdf[\"sat:relative_orbit\"].astype(\"int16\"))\n",
    "    })\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.drop_vars(['hh','hv'],errors='ignore')\n",
    "\n",
    "    epsg = s1_rtc_ds.rio.estimate_utm_crs().to_epsg()\n",
    "    hemisphere = 'northern' if epsg < 32700 else 'southern'\n",
    "\n",
    "    s1_rtc_ds.attrs['hemisphere'] = hemisphere\n",
    "\n",
    "    s1_rtc_ds = s1_rtc_ds.assign_coords(\n",
    "    {\n",
    "        \"water_year\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_WY(x, hemisphere=hemisphere))),\n",
    "        \"DOWY\": (\"time\", pd.to_datetime(s1_rtc_ds.time).map(lambda x: easysnowdata.utils.datetime_to_DOWY(x, hemisphere=hemisphere)))\n",
    "    })        \n",
    "\n",
    "    return s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geobox = tiles[-1].geobox\n",
    "geobox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    \"or\": [\n",
    "        {\"sar:polarizations\": {\"eq\": [\"VV\"]}},\n",
    "        {\"sar:polarizations\": {\"eq\": [\"VV\", \"VH\"]}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "query = {\n",
    "    \"sar:polarizations\": {\"in\": [\"VV\"]}  # May not work in all STAC implementations\n",
    "}\n",
    "\n",
    "query = {\n",
    "    \"sar:polarizations\": {\"eq\": [\"VV\",\"VH\"]}  # May not work in all STAC implementations\n",
    "}\n",
    "\n",
    "items = (\n",
    "    pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",modifier=planetary_computer.sign_inplace)\n",
    "    .search(\n",
    "        intersects=geobox.geographic_extent,\n",
    "        collections=[\"sentinel-1-rtc\"],\n",
    "        datetime=(config.start_date, config.end_date),\n",
    "        #query=query,\n",
    "    )\n",
    "    .item_collection()\n",
    ")\n",
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params = {\n",
    "    \"items\": items,\n",
    "    \"bands\": [\"vv\"],\n",
    "    \"nodata\": -32768,\n",
    "    \"chunks\": config.chunks_read, #config.chunks_read\n",
    "    \"groupby\": \"sat:absolute_orbit\",\n",
    "    \"geobox\":geobox,\n",
    "    \"resampling\": \"bilinear\",\n",
    "    \"fail_on_error\":True\n",
    "}\n",
    "load_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rtc_ds = odc.stac.load(**load_params).sortby(\"time\")\n",
    "s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_rtc_ds = s1_rtc_ds.compute()\n",
    "s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile = get_sentinel1_rtc(tiles[-1].geobox)\n",
    "test_tile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile_computed = test_tile.compute()\n",
    "test_tile_computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tile.chunk({'latitude': 256, 'longitude': 256, 'time':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tile(tiles[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "\n",
    "for tile_batch in tqdm.tqdm(tile_batches, total=len(tile_batches)):\n",
    "\n",
    "    futures = [client.submit(process_tile, tile) for tile in tile_batch] #, retries=0\n",
    "\n",
    "    successful_tiles = []\n",
    "\n",
    "    try:\n",
    "        for future, computed_result in dask.distributed.as_completed(futures, with_results=True, timeout=1600):\n",
    "            successful_tiles.append(computed_result)\n",
    "\n",
    "            df = pd.DataFrame(\n",
    "                [[getattr(computed_result, f) for f in fields]],\n",
    "                columns=fields,\n",
    "            )\n",
    "            df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "            print(f'Tile ({computed_result.row},{computed_result.col}) completed')\n",
    "    except Exception as e:\n",
    "        for tile in tile_batch:\n",
    "            if tile.index not in [computed_tile.index for computed_tile in successful_tiles]:\n",
    "\n",
    "                df = pd.DataFrame([[getattr(tile, f) for f in fields]],columns=fields,)\n",
    "                df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "\n",
    "                print(f'Tile ({tile.row},{tile.col}) failed')\n",
    "                print(e)\n",
    "                print(traceback.format_exc())\n",
    "    \n",
    "    \n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 5\n",
    "# tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "\n",
    "# for tile_batch in tqdm.tqdm(tile_batches, total=len(tile_batches)):\n",
    "\n",
    "#     batch_results = [dask.delayed(process_tile)(tile) for tile in tile_batch]\n",
    "\n",
    "#     try:    \n",
    "#         computed_results = dask.compute(*batch_results)\n",
    "#     except:\n",
    "#         computed_results = tile_batch\n",
    "    \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in computed_results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "\n",
    "#     df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "    \n",
    "#     client.restart()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 5\n",
    "# tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "\n",
    "# for tile_batch in tqdm.tqdm(tile_batches, total=len(tile_batches)):\n",
    "\n",
    "#     futures = []\n",
    "#     for tile in tile_batch:\n",
    "#         futures.append(client.submit(process_tile, tile, retries=0))\n",
    "\n",
    "\n",
    "#     try:\n",
    "#         dask.distributed.wait(futures, timeout=1000, return_when=\"ALL_COMPLETED\")\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print('Error waiting for futures to complete')\n",
    "#         for future in futures:\n",
    "#             if future.status != 'finished':\n",
    "#                 future.cancel()\n",
    "\n",
    "#     computed_results = client.gather(futures,errors='skip')\n",
    "\n",
    "#     for tile in tile_batch:\n",
    "#         if tile.index not in [computed_tile.index for computed_tile in computed_results]:\n",
    "#             computed_results.append(tile)\n",
    "    \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in computed_results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "\n",
    "#     df.to_csv(config.tile_results_path, mode='a', header=False, index=False) # header=True if starting over\n",
    "    \n",
    "#     client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import as_completed\n",
    "\n",
    "# fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\n",
    "#             \"tr_2015\", \"tr_2016\", \"tr_2017\", \"tr_2018\", \"tr_2019\", \"tr_2020\", \"tr_2021\", \"tr_2022\", \"tr_2023\",\"tr_2024\",\n",
    "#             \"pix_ct_2015\",\"pix_ct_2016\",\"pix_ct_2017\",\"pix_ct_2018\",\"pix_ct_2019\",\"pix_ct_2020\",\"pix_ct_2021\",\"pix_ct_2022\",\"pix_ct_2023\",\"pix_ct_2024\",\n",
    "#             \"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "# futures = [client.submit(process_tile, tile, retries=0) for tile in tiles]\n",
    "\n",
    "# with tqdm.tqdm(total=len(tiles)) as pbar:\n",
    "#     for future in as_completed(futures):\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             df = pd.DataFrame([[getattr(result, f) for f in fields]], columns=fields)\n",
    "#         except Exception as e:\n",
    "#             # Handle the error, possibly by adding the original tile to the results\n",
    "#             error_tile = next(tile for tile in tiles if tile.index == future.key.split('-')[1])\n",
    "#             error_tile.error_messages = str(e)\n",
    "#             df = pd.DataFrame([[getattr(error_tile, f) for f in fields]], columns=fields)\n",
    "\n",
    "#         df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#         header_written = True\n",
    "#         pbar.update(1)\n",
    "\n",
    "#     client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True,decode_coords='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile = Tile(16,204)\n",
    "# tile = Tile(16,203)\n",
    "# futures = []\n",
    "# futures.append(client.submit(process_tile, tile, retries=1))\n",
    "# futures\n",
    "# computed_results = client.gather(futures,errors='skip')\n",
    "# computed_results[0].error_messages\n",
    "# computed_results[0].runoff_onsets_dims\n",
    "# test_ds = global_ds.rio.clip_box(*tile.get_geobox().boundingbox,crs='EPSG:4326')\n",
    "# test_ds\n",
    "# test_ds['runoff_onset'].sum(dim=['latitude','longitude']).values\n",
    "# view_tile(tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_ds['runoff_onset_median'].odc.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.scale(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiles = [Tile(row,col) for row,col in zip(valid_tiles_gdf.row,valid_tiles_gdf.col)]\n",
    "# tile = tiles[6]\n",
    "# geobox = tile.get_geobox()\n",
    "# geobox.explore()\n",
    "# bbox = geobox.boundingbox\n",
    "# bbox_geometry = shapely.geometry.box(bbox.left, bbox.bottom, bbox.right, bbox.top)\n",
    "# bbox_gdf = gpd.GeoDataFrame(geometry=[bbox_geometry], crs=geobox.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "# s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "# s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry)\n",
    "# s1_rtc_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seasonal_snow_mask_clip_ds = get_custom_seasonal_snow_mask(bbox_gdf)\n",
    "# seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=0)).rename({'x':'longitude','y':'latitude'})\n",
    "# #seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.odc.reproject(geobox).persist()#.rename({'x':'longitude','y':'latitude'})\n",
    "# seasonal_snow_mask_matched_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "# s1_rtc_masked_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_rtc_masked_vv_WY2019_ds = s1_rtc_masked_ds['vv'].sel(time=slice('2018-10-01','2019-09-30')).compute()\n",
    "# s1_rtc_masked_vv_WY2019_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_rtc_masked_vv_WY2019_ds.plot.imshow(col='time',col_wrap=6,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1_rtc_masked_vv_WY2019_ds.where(lambda x:x>0.001).plot.imshow(col='time',col_wrap=6,vmin=0,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runoff_onsets = (\n",
    "# s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "# .apply(\n",
    "#     calculate_runoff_onset_wrapper,\n",
    "#     consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "#     min_monthly_acquisitions=1, #one or two\n",
    "#     returned_dates_format=\"dowy\",\n",
    "#     return_constituent_runoff_onsets=True,\n",
    "#     low_backscatter_threshold=0.001#0.001\n",
    "# ))\n",
    "#runoff_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runoff_onsets.sel(water_year=2019).plot.imshow(col='sat:relative_orbit',row='polarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getsizeof(futures[0].result())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# runoff_onsets\n",
    "# runoff_onsets_computed = runoff_onsets.to_dataset(name='runoff_onset').persist()# add .persist here?\n",
    "# runoff_onsets_computed\n",
    "# runoff_onsets_computed['runoff_onset_median'] = median_with_min_obs(runoff_onsets_computed['runoff_onset'], 'water_year', 3)\n",
    "# runoff_onsets_computed\n",
    "# runoff_onsets_computed['runoff_onset'].plot.imshow(col='water_year',col_wrap=3)\n",
    "# f,ax=plt.subplots(figsize=(12,12))\n",
    "# runoff_onsets_computed['runoff_onset_median'].plot.imshow(ax=ax)\n",
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True) #consolidated=False if processed tiles not showing up\n",
    "# global_ds\n",
    "# global_subset = global_ds.sel(latitude=runoff_onsets_computed.latitude,longitude=runoff_onsets_computed.longitude,method='nearest')\n",
    "# global_subset\n",
    "# runoff_onsets_computed_reindexed = runoff_onsets_computed.round().astype('uint16').assign_coords(latitude=global_subset.latitude,longitude=global_subset.longitude)\n",
    "# runoff_onsets_computed_reindexed\n",
    "# runoff_onsets_computed_reindexed.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048})#.to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True)\n",
    "# cluster.scale(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing equator\n",
    "# # \n",
    "# # client.restart()\n",
    "\n",
    "# test_tiles_gdf = valid_tiles_gdf[(valid_tiles_gdf['row'] == 54) | (valid_tiles_gdf['row'] == 55)]\n",
    "\n",
    "# test_tiles = [Tile(row,col) for row,col in zip(test_tiles_gdf.row,test_tiles_gdf.col)]\n",
    "\n",
    "# futures = []\n",
    "\n",
    "# for tile in test_tiles:\n",
    "#     future = client.submit(process_tile,tile)\n",
    "#     #future = process_tile.submit(tile)\n",
    "#     futures.append(future)\n",
    "\n",
    "#     test_tiles_gdf.explore()\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "\n",
    "# for result in results:\n",
    "#     print(f'tile {result.index} success: {result.success}, time: {result.total_time}, errors: {result.error_messages}, s1_rtc_ds_dims: {result.s1_rtc_ds_dims}, s1_rtc_masked_ds_dims: {result.s1_rtc_masked_ds_dims}, runoff_onsets_dims: {result.runoff_onsets_dims}')\n",
    "\n",
    "# global_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snowmelt_runoff_onset/global.zarr\")\n",
    "# global_ds = xr.open_zarr(global_store, consolidated=True,decode_coords='all')\n",
    "\n",
    "\n",
    "# def view_tile(tile: Tile, global_ds: xr.Dataset):\n",
    "\n",
    "\n",
    "#     test_ds = global_ds.rio.clip_box(*tile.get_geobox().boundingbox,crs='EPSG:4326')\n",
    "\n",
    "#     f,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "#     test_ds['runoff_onset_median'].plot.imshow(ax=ax)\n",
    "\n",
    "#     test_ds['runoff_onset'].plot.imshow(col='water_year',col_wrap=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@coiled.function(cpu=4, memory='32 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", threads_per_worker=-1)\n",
    "# #@coiled.function(cpu=4, spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\")\n",
    "# #, name=f\"process_tile_batch_{batch_number}\"\n",
    "# #@dask.delayed#, threads_per_worker=-1\n",
    "# #odc.stac.configure_rio(cloud_defaults=True)\n",
    "# #@coiled.function(n_workers=80, cpu=4, memory='32 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", name='mem_test')\n",
    "# def process_tile(tile : Tile):\n",
    "\n",
    "\n",
    "#     tile.start_time = time.time()\n",
    "\n",
    "#     geobox = tile.geobox\n",
    "#     bbox_gdf = tile.bbox_gdf\n",
    "    \n",
    "#     #odc.stac.configure_rio(cloud_defaults=True)\n",
    "\n",
    "#     try:\n",
    "\n",
    "\n",
    "#         #s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "#         s1_rtc_ds = dask.delayed(get_sentinel1_rtc)(geobox)\n",
    "\n",
    "\n",
    "#         #tile.s1_rtc_ds = s1_rtc_ds\n",
    "        \n",
    "#         #tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "#         #seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds,bbox_gdf)\n",
    "#         seasonal_snow_mask_matched_ds = dask.delayed(get_custom_seasonal_snow_mask)(s1_rtc_ds,bbox_gdf)\n",
    "\n",
    "#         #gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "#         gmba_clipped_gdf = dask.delayed(get_gmba_mountain_inventory)(bbox_gdf)\n",
    "\n",
    "\n",
    "#         #s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "#         s1_rtc_masked_ds = dask.delayed(apply_all_masks)(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "\n",
    "        \n",
    "#         #tile.s1_rtc_masked_ds_dims = dict(s1_rtc_masked_ds.sizes)\n",
    "\n",
    "#         runoff_onsets_da = (\n",
    "#         s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "#         .apply(\n",
    "#             calculate_runoff_onset_wrapper,\n",
    "#             consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "#             min_monthly_acquisitions=2, #one or two\n",
    "#             returned_dates_format=\"dowy\",\n",
    "#             return_constituent_runoff_onsets=False,\n",
    "#             low_backscatter_threshold=0.001,\n",
    "#         ))\n",
    "        \n",
    "#         #tile.runoff_onsets = runoff_onsets\n",
    "        \n",
    "#         #runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset')# add .persist here?\n",
    "\n",
    "\n",
    "#         median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', 5)\n",
    "\n",
    "#         runoff_onsets_ds = dask.delayed(dataarrays_to_dataset)(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "#         #runoff_onsets_ds['runoff_onset_median'] = median_da\n",
    "        \n",
    "#         #runoff_onsets_ds = runoff_onsets_ds.round().astype('uint16')\n",
    "\n",
    "#         #runoff_onsets_ds['runoff_onset_std'] = std_da.astype('float32')\n",
    "\n",
    "#         #tile.runoff_onsets_dims = dict(runoff_onsets_ds.sizes)\n",
    "\n",
    "#         #with dask.config.set(pool=ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         #    runoff_onsets_computed = runoff_onsets_computed.compute() # use compute instead, then if stilll sloe remove thread pool saturation\n",
    "        \n",
    "#         #del s1_rtc_ds, seasonal_snow_mask_clip_ds, seasonal_snow_mask_matched_ds, s1_rtc_masked_ds, runoff_onsets #gmba_clipped_gdf,\n",
    "#         #gc.collect()\n",
    "\n",
    "#         global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude,longitude=runoff_onsets_ds.longitude,method='nearest')\n",
    "\n",
    "#         runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude,longitude=global_subset_ds.longitude)\n",
    "        \n",
    "#         #with dask.config.set(pool=ThreadPoolExecutor(16), scheduler=\"threads\"):\n",
    "#         runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True, compute=True)\n",
    "\n",
    "#         #del runoff_onsets_computed, global_store, global_ds, global_subset, runoff_onsets_computed_reindexed\n",
    "#         #gc.collect()\n",
    "\n",
    "#         #del seasonal_snow_mask_matched_ds\n",
    "\n",
    "#         tile.total_time = time.time() - tile.start_time\n",
    "#         tile.success = True\n",
    "\n",
    "#     except Exception as e:\n",
    "#         #gc.collect()\n",
    "#         tile.error_messages.append(str(e))\n",
    "#         tile.error_messages.append(traceback.format_exc())\n",
    "#         tile.total_time = time.time() - tile.start_time\n",
    "#         tile.success = False\n",
    "\n",
    "#     return tile\n",
    "\n",
    "\n",
    "\n",
    "# def apply_all_masks(s1_rtc_ds,bbox_gdf):\n",
    "\n",
    "#     s1_rtc_ds = remove_unwanted_water_years(s1_rtc_ds)\n",
    "\n",
    "#     center_lat = (s1_rtc_ds.rio.bounds()[1]+s1_rtc_ds.rio.bounds()[3])/2\n",
    "\n",
    "#     if np.absolute(center_lat) < 3:\n",
    "#         s1_rtc_ds = remove_equator_crossing(s1_rtc_ds)\n",
    "        \n",
    "#     gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "#     s1_rtc_ds = s1_rtc_ds.rio.clip(gmba_clipped_gdf.geometry)\n",
    "    \n",
    "#     seasonal_snow_mask_clip_ds = get_custom_seasonal_snow_mask(bbox_gdf)\n",
    "#     seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.rio.reproject_match(s1_rtc_ds.isel(time=0)).rename({'x':'longitude','y':'latitude'})\n",
    "#     #seasonal_snow_mask_matched_ds = seasonal_snow_mask_clip_ds.odc.reproject(geobox)\n",
    "\n",
    "\n",
    "#     s1_rtc_masked_ds = apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds)\n",
    "    \n",
    "#     return s1_rtc_masked_ds, seasonal_snow_mask_matched_ds\n",
    "\n",
    "# def remove_unwanted_water_years(s1_rtc_ds):\n",
    "#     s1_rtc_ds = s1_rtc_ds.sel(time=s1_rtc_ds.water_year.isin(water_years))\n",
    "#     return s1_rtc_ds\n",
    "\n",
    "\n",
    "# def remove_equator_crossing(s1_rtc_ds):\n",
    "#     if s1_rtc_ds.attrs['hemisphere'] == 'northern':\n",
    "#         mask = s1_rtc_ds.latitude >= 0\n",
    "#     else:\n",
    "#         mask = s1_rtc_ds.latitude < 0\n",
    "\n",
    "#     s1_rtc_ds = s1_rtc_ds.where(mask)\n",
    "#     return s1_rtc_ds\n",
    "\n",
    "# def get_gmba_mountain_inventory(bbox_gdf):\n",
    "#     url = (f\"https://data.earthenv.org/mountains/standard/GMBA_Inventory_v2.0_standard_300.zip\")\n",
    "#     gmba_gdf = gpd.read_file(\"zip+\" + url)\n",
    "#     return gpd.clip(gmba_gdf, bbox_gdf)\n",
    "\n",
    "# def get_custom_seasonal_snow_mask(bbox_gdf):\n",
    "#     #xmin, ymin, xmax, ymax = bbox_gdf.total_bounds\n",
    "#     mask_store = adlfs.AzureBlobFileSystem(account_name=\"snowmelt\", credential=sas_token).get_mapper(\"snowmelt/snow_mask/global_modis_snow_mask.zarr\")\n",
    "#     seasonal_snow_mask = xr.open_zarr(mask_store, consolidated=True, decode_coords='all') \n",
    "#     seasonal_snow_mask_clip = seasonal_snow_mask.rio.clip_box(*bbox_gdf.total_bounds,crs='EPSG:4326') # clip to correct box, maybe use total_bounds and then use crs \n",
    "#     return seasonal_snow_mask_clip\n",
    "\n",
    "\n",
    "# def apply_seasonal_snow_spatial_and_temporal_mask(s1_rtc_ds, seasonal_snow_mask_matched_ds):\n",
    "#     s1_rtc_masked_ds = s1_rtc_ds.groupby('water_year').map(lambda group: apply_mask_for_year(group, seasonal_snow_mask_matched_ds))\n",
    "#     s1_rtc_masked_ds.rio.write_crs(s1_rtc_ds.rio.crs,inplace=True)\n",
    "#     return s1_rtc_masked_ds\n",
    "\n",
    "# def apply_mask_for_year(group, seasonal_snow_mask_matched_ds):\n",
    "\n",
    "#     year = group.water_year.values[0]\n",
    "\n",
    "\n",
    "#     if year not in seasonal_snow_mask_matched_ds.water_year:\n",
    "#         print(f\"Warning: water_year {year} not found in seasonal_snow_mask_matched_ds\")\n",
    "#         return group.where(False) \n",
    "\n",
    "#     sad_mask = group['DOWY'] >= seasonal_snow_mask_matched_ds['SAD_DOWY'].sel(water_year=year)\n",
    "#     sdd_mask = group['DOWY'] <= seasonal_snow_mask_matched_ds['SDD_DOWY'].sel(water_year=year)\n",
    "#     consec_mask = seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=year) >= 56\n",
    "#     combined_mask = sad_mask & sdd_mask & consec_mask\n",
    "#     return group.where(combined_mask)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def xr_datetime_to_DOWY_map_blocks(date_da, hemisphere=\"northern\"):\n",
    "#     \"\"\"\n",
    "#     Converts an xarray DataArray containing datetime objects to the Day of Water Year (DOWY).\n",
    "\n",
    "#     Parameters:\n",
    "#     date (xr.DataArray): An xarray DataArray with datetime64 data type.\n",
    "#     hemisphere (str): 'northern' or 'southern'\n",
    "\n",
    "#     Returns:\n",
    "#     xr.DataArray: An xarray DataArray containing the DOWY for each datetime in the input DataArray.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Determine any valid date\n",
    "#     if date_da.attrs.get(\"any_valid_date\") is not None:\n",
    "#         any_valid_date = pd.to_datetime(date_da.attrs[\"any_valid_date\"])\n",
    "#     else:\n",
    "#         any_valid_date = pd.to_datetime(date_da.sel(x=0, y=0, method=\"nearest\").values)\n",
    "\n",
    "#     # Calculate the start of the water year\n",
    "#     start_of_water_year = easysnowdata.utils.get_water_year_start(\n",
    "#         any_valid_date, hemisphere=hemisphere\n",
    "#     )\n",
    "\n",
    "#     # Define the function to calculate DOWY for a block\n",
    "#     def calculate_dowy_block(block, start_of_water_year):\n",
    "#         # Calculate DOWY for the block\n",
    "#         dowy_block = (block - np.datetime64(start_of_water_year)).astype('timedelta64[D]').astype(int) + 1\n",
    "#         return dowy_block\n",
    "\n",
    "#     # Apply the function using map_blocks\n",
    "#     return date_da.map_blocks(\n",
    "#         calculate_dowy_block,\n",
    "#         args=(start_of_water_year,),\n",
    "#         template=date_da.astype(int)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tile = tiles[0]\n",
    "# geobox = tile.geobox\n",
    "# bbox_gdf = tile.bbox_gdf\n",
    "\n",
    "# s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "# s1_rtc_ds = dask.delayed(get_sentinel1_rtc)(geobox)\n",
    "# s1_rtc_ds\n",
    "\n",
    "\n",
    "# seasonal_snow_mask_matched_ds = dask.delayed(get_custom_seasonal_snow_mask)(s1_rtc_ds,bbox_gdf)\n",
    "# seasonal_snow_mask_matched_ds\n",
    "\n",
    "\n",
    "# gmba_clipped_gdf = dask.delayed(get_gmba_mountain_inventory)(bbox_gdf)\n",
    "# gmba_clipped_gdf\n",
    "\n",
    "\n",
    "# s1_rtc_masked_ds = dask.delayed(apply_all_masks)(s1_rtc_ds,gmba_clipped_gdf,seasonal_snow_mask_matched_ds)\n",
    "# s1_rtc_masked_ds\n",
    "\n",
    "\n",
    "# runoff_onsets_da = (\n",
    "# s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "# .apply(\n",
    "# calculate_runoff_onset_wrapper,\n",
    "# consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "# min_monthly_acquisitions=2, #one or two\n",
    "# returned_dates_format=\"doy\",\n",
    "# return_constituent_runoff_onsets=False,\n",
    "# low_backscatter_threshold=0.001,\n",
    "# ))\n",
    "\n",
    "# runoff_onsets_da\n",
    "\n",
    "\n",
    "# median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', 5)\n",
    "\n",
    "# def dataarrays_to_dataset(runoff_onsets_da, median_da, std_da):\n",
    "\n",
    "#     runoff_onsets_ds = runoff_onsets_da.to_dataset(name='runoff_onset').round().astype('uint16')\n",
    "#     runoff_onsets_ds['runoff_onset_median'] = median_da.round().astype('uint16')\n",
    "#     runoff_onsets_ds['runoff_onset_std'] = std_da\n",
    "    \n",
    "#     return runoff_onsets_ds\n",
    "\n",
    "# runoff_onsets_ds = dask.delayed(dataarrays_to_dataset)(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "# runoff_onsets_ds\n",
    "\n",
    "# runoff_onsets_computed_ds = dask.compute(runoff_onsets_ds)\n",
    "\n",
    "# runoff_onsets_computed_ds\n",
    "\n",
    "# global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude,longitude=runoff_onsets_ds.longitude,method='nearest')\n",
    "# global_subset_ds\n",
    "\n",
    "# runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude,longitude=global_subset_ds.longitude)\n",
    "# runoff_onsets_reindexed_ds\n",
    "\n",
    "# dask.compute(runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(global_store, region=\"auto\", mode=\"r+\", consolidated=True))\n",
    "\n",
    "\n",
    "# runoff_onsets_ds['runoff_onset_median'] = median_da\n",
    "\n",
    "# runoff_onsets_ds = runoff_onsets_ds.round().astype('uint16')\n",
    "\n",
    "# runoff_onsets_ds['runoff_onset_std'] = std_da.astype('float32')\n",
    "\n",
    "# computed_results = dask.compute(*results)\n",
    "\n",
    "\n",
    "# results = computed_results\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     for tile in tile_batch:\n",
    "#         result = process_tile(tile)\n",
    "#         results.append(result)\n",
    "    \n",
    "#     if i == 0:\n",
    "#         break\n",
    "    \n",
    "    \n",
    "#     for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     futures = []\n",
    "#     for tile in tile_batch:\n",
    "#         future = client.submit(process_tile,tile)\n",
    "#         #future = process_tile.submit(tile) # when trying serverless coiled.function\n",
    "#         futures.append(future)\n",
    "\n",
    "#     fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "#     #results \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "#     df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#     header_written = True\n",
    "    \n",
    "#     client.restart()\n",
    "    \n",
    "#     if i == 1:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_results_list = []\n",
    "\n",
    "# batch_size = 10\n",
    "# tile_batches = [tiles[i:i + batch_size] for i in range(0, len(tiles), batch_size)]\n",
    "\n",
    "# process_tile_serverless = coiled.function(n_workers=11, cpu=8, memory='64 GB', spot_policy=\"spot\", region=\"westeurope\", environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"}, keepalive=\"5m\", workspace=\"azure\", name='mem_test')(process_tile)\n",
    "\n",
    "\n",
    "# for i,tile_batch in tqdm.tqdm(enumerate(tile_batches),total=len(tile_batches)):\n",
    "#     futures = []\n",
    "#     for tile in tile_batch:\n",
    "#         #future = client.submit(process_tile,tile)\n",
    "#         future = process_tile_serverless.submit(tile) # when trying serverless coiled.function\n",
    "#         futures.append(future)\n",
    "\n",
    "#     results = [f.result() for f in futures]\n",
    "    \n",
    "#     full_results_list.extend(results)\n",
    "\n",
    "#     fields = (\"row\",\"col\",\"percent_valid_snow_pixels\",\"s1_rtc_ds_dims\",\"runoff_onsets_dims\",\"start_time\",\"total_time\",\"success\",\"error_messages\")\n",
    "\n",
    "    \n",
    "#     #results = client.gather(futures) \n",
    "\n",
    "#     df = pd.DataFrame(\n",
    "#         [[getattr(r, f) for f in fields] for r in results if r is not None],\n",
    "#         columns=fields,\n",
    "#     )\n",
    "#     df.to_csv('tile_results.csv', mode='a', header=not header_written, index=False)\n",
    "#     header_written = True\n",
    "    \n",
    "    \n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_acquisitions_and_max_gap_per_orbit_and_polarization(s1_rtc_ds: xr.Dataset):\n",
    "#     print(\"Calculating pixelwise counts and maximum gaps per orbit and polarization...\")\n",
    "#     pixelwise_counts_per_orbit_ds = s1_rtc_ds.groupby(\"sat:relative_orbit\").count(dim=\"time\", engine='flox')\n",
    "    \n",
    "#     def calc_max_gap(group):\n",
    "#         times = group.time.sortby('time')\n",
    "#         gaps = times.diff(dim='time').max()\n",
    "#         return gaps\n",
    "\n",
    "#     max_time_gap_per_orbit_days_da = s1_rtc_ds.groupby(\"sat:relative_orbit\").map(calc_max_gap).dt.days\n",
    "    \n",
    "#     return pixelwise_counts_per_orbit_ds, max_time_gap_per_orbit_days_da\n",
    "\n",
    "\n",
    "# def filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#     backscatter_min_timing_per_orbit_and_polarization_ds: xr.Dataset,\n",
    "#     pixelwise_counts_per_orbit_ds: xr.Dataset,\n",
    "#     max_days_gap_per_orbit_da: xr.Dataset,\n",
    "#     consec_snow_days_da: xr.DataArray,\n",
    "#     min_monthly_acquisitions: int,\n",
    "#     max_allowed_days_gap_per_orbit: int\n",
    "# ):\n",
    "#     print(f\"Filtering insufficient pixels per orbit and polarization...\")\n",
    "#     constituent_runoff_onsets_ds = (\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds.where(\n",
    "#             (pixelwise_counts_per_orbit_ds >= (min_monthly_acquisitions*(consec_snow_days_da/30))) &\n",
    "#             (max_days_gap_per_orbit_da <= max_allowed_days_gap_per_orbit)\n",
    "#         )\n",
    "#     )\n",
    "#     return constituent_runoff_onsets_ds\n",
    "\n",
    "\n",
    "# constituent_runoff_onsets_ds = (\n",
    "#     filter_insufficient_pixels_per_orbit_and_polarization(\n",
    "#         backscatter_min_timing_per_orbit_and_polarization_ds,\n",
    "#         pixelwise_counts_per_orbit_ds,\n",
    "#         max_days_gap_per_orbit_da,\n",
    "#         seasonal_snow_mask_matched_ds['max_consec_snow_days'].sel(water_year=2019),\n",
    "#         min_monthly_acquisitions,\n",
    "#         max_allowed_days_gap_per_orbit\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# constituent_runoff_onsets_da = constituent_runoff_onsets_ds.to_dataarray(dim=\"polarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_tile(tile: Tile):\n",
    "#     @dask.delayed\n",
    "#     def _process():\n",
    "#         tile.start_time = time.time()\n",
    "#         geobox = tile.geobox\n",
    "#         bbox_gdf = tile.bbox_gdf\n",
    "        \n",
    "#         try:\n",
    "\n",
    "#             s1_rtc_ds = get_sentinel1_rtc(geobox)\n",
    "#             tile.s1_rtc_ds_dims = dict(s1_rtc_ds.sizes)\n",
    "\n",
    "#             seasonal_snow_mask_matched_ds = get_custom_seasonal_snow_mask(s1_rtc_ds, bbox_gdf)\n",
    "\n",
    "#             gmba_clipped_gdf = get_gmba_mountain_inventory(bbox_gdf)\n",
    "\n",
    "#             s1_rtc_masked_ds = apply_all_masks(s1_rtc_ds, gmba_clipped_gdf, seasonal_snow_mask_matched_ds)\n",
    "\n",
    "#             runoff_onsets_da = (\n",
    "#                 s1_rtc_masked_ds.groupby(\"water_year\")\n",
    "#                 .apply(\n",
    "#                     calculate_runoff_onset_wrapper,\n",
    "#                     consec_snow_days_da=seasonal_snow_mask_matched_ds['max_consec_snow_days'],\n",
    "#                     min_monthly_acquisitions=min_monthly_acquisitions,\n",
    "#                     max_allowed_days_gap_per_orbit=max_allowed_days_gap_per_orbit,\n",
    "#                     returned_dates_format=\"dowy\",\n",
    "#                     return_constituent_runoff_onsets=False,\n",
    "#                     low_backscatter_threshold=low_backscatter_threshold,\n",
    "#                     report_temporal_res=False,\n",
    "#                     tile=tile,\n",
    "#                 )\n",
    "#             )\n",
    "            \n",
    "#             tile.runoff_onsets_dims = dict(runoff_onsets_da.sizes)\n",
    "#             median_da, std_da = median_and_std_with_min_obs(runoff_onsets_da, 'water_year', min_years_for_median_std)\n",
    "#             runoff_onsets_ds = dataarrays_to_dataset(runoff_onsets_da, median_da, std_da)\n",
    "\n",
    "#             global_store = get_global_runoff_store()\n",
    "            \n",
    "#             global_ds = xr.open_zarr(global_store, consolidated=True)\n",
    "#             global_subset_ds = global_ds.sel(latitude=runoff_onsets_ds.latitude, longitude=runoff_onsets_ds.longitude, method='nearest')\n",
    "            \n",
    "#             runoff_onsets_reindexed_ds = runoff_onsets_ds.assign_coords(latitude=global_subset_ds.latitude, longitude=global_subset_ds.longitude)\n",
    "            \n",
    "#             # Write to Zarr\n",
    "#             runoff_onsets_reindexed_ds.drop_vars('spatial_ref').chunk({\"longitude\": 2048, \"latitude\": 2048}).to_zarr(\n",
    "#                 global_store, region=\"auto\", mode=\"r+\", consolidated=True\n",
    "#             )\n",
    "            \n",
    "#             tile.total_time = time.time() - tile.start_time\n",
    "#             tile.success = True\n",
    "#         except Exception as e:\n",
    "#             tile.error_messages.append(str(e))\n",
    "#             tile.error_messages.append(traceback.format_exc())\n",
    "#             tile.total_time = time.time() - tile.start_time\n",
    "#             tile.success = False\n",
    "        \n",
    "#         return tile\n",
    "    \n",
    "#     return _process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
