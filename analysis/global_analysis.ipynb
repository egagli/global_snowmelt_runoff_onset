{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global analysis of snowmelt runoff onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import coiled\n",
    "import dask\n",
    "from global_snowmelt_runoff_onset.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config/global_config.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                        n_workers=10,\n",
    "                        worker_memory=\"64 GB\",\n",
    "                        worker_cpu=8,\n",
    "                        scheduler_memory=\"64 GB\",\n",
    "                        spot_policy=\"spot\",\n",
    "                        environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                        workspace=\"uwtacolab\",\n",
    "                        )\n",
    "\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_ds = xr.open_zarr(config.global_runoff_store, consolidated=True,decode_coords='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ddf = dd.read_parquet('snowmelt/analysis/tiles/', filesystem=config.azure_blob_fs,split_row_groups='adaptive')\n",
    "results_ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_tiles_results_df = ddf[[\"original_lat\",\"dem\",\"runoff_onset_median\",\"chili\"]].repartition(partition_size=\"256 MiB\").persist()\n",
    "all_tiles_results_df = ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles_results_df.memory_usage().compute() / 1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global analysis: linear regression and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every X increase in elevation there is a y delay\n",
    "# for every X increase in latitude there is a y delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles_results_df.corr(numeric_only=True).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create latitude and elevation bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_bin_low = 0\n",
    "dem_bin_high = 8000\n",
    "dem_bin_interval = 100\n",
    "dem_bins = np.arange(dem_bin_low,dem_bin_high+dem_bin_interval,dem_bin_interval)\n",
    "lat_bin_low = -80\n",
    "lat_bin_high = 80\n",
    "lat_bin_interval = 1\n",
    "lat_bins = np.arange(lat_bin_low,lat_bin_high+lat_bin_interval,lat_bin_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles_results_df['lat_bin'] = all_tiles_results_df['original_lat'].map_partitions(pd.cut, lat_bins)\n",
    "all_tiles_results_df['dem_bin'] = all_tiles_results_df['dem'].map_partitions(pd.cut, dem_bins)\n",
    "all_tiles_results_df = all_tiles_results_df.dropna(subset=['lat_bin','dem_bin'])\n",
    "all_tiles_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles_results_df['lat_bin'] = all_tiles_results_df['lat_bin'].apply(lambda x: x.left).astype(int)\n",
    "all_tiles_results_df['dem_bin'] = all_tiles_results_df['dem_bin'].apply(lambda x: x.left).astype(int)\n",
    "all_tiles_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median snowmelt runoff onset binned by elevation and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_latitude_and_elevation_df = all_tiles_results_df[['lat_bin','dem_bin','runoff_onset_median']].groupby(['lat_bin', 'dem_bin']).median()\n",
    "groupby_latitude_and_elevation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}):\n",
    "    groupby_latitude_and_elevation_df = groupby_latitude_and_elevation_df.compute()\n",
    "groupby_latitude_and_elevation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_onset_vs_lat_and_elev_df = groupby_latitude_and_elevation_df.reset_index().pivot(index='lat_bin', columns='dem_bin', values='runoff_onset_median').reindex(lat_bins).sort_index(ascending=False)\n",
    "runoff_onset_vs_lat_and_elev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(8,10),dpi=300)\n",
    "sns.heatmap(runoff_onset_vs_lat_and_elev_df, square=True,ax=ax, cmap='viridis', cbar_kws={'label': 'snowmelt runoff onset [DOWY]'},\n",
    "            vmin=0,vmax=365)\n",
    "\n",
    "ax.set_xlabel('elevation (m)')\n",
    "ax.set_ylabel('latitude [degrees]')\n",
    "ax.set_title('2015-2024 median date of snowmelt runoff onset\\nbinned by elevation and latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global analysis: chili / influence of shortwave radiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles_results_df['chili_class'] = 'neutral'\n",
    "all_tiles_results_df['chili_class'] = all_tiles_results_df['chili_class'].where(\n",
    "    (all_tiles_results_df['chili'] >= 0.448) & (all_tiles_results_df['chili'] <= 0.767),\n",
    "    other=all_tiles_results_df['chili'].map(lambda x: 'warm' if x > 0.767 else 'cool' if x < 0.448 else 'neutral')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = all_tiles_results_df[['lat_bin','dem_bin','chili_class','runoff_onset_median']].dropna().groupby(['lat_bin', 'dem_bin','chili_class'])['runoff_onset_median'].mean().compute()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = grouped.unstack()\n",
    "pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape to get warm and cool values\n",
    "warm_cool_ratio_df = pivot_df['warm'] / pivot_df['cool']\n",
    "warm_cool_ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_cool_ratio_df = warm_cool_ratio_df.reset_index().pivot(index='lat_bin', columns='dem_bin').reindex(lat_bins).sort_index(ascending=False)\n",
    "warm_cool_ratio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(8,10),dpi=300)\n",
    "sns.heatmap(warm_cool_ratio_df, square=True,ax=ax, cmap='PuOr', cbar_kws={'label': 'warm / cool'},robust=True)\n",
    "\n",
    "ax.set_xlabel('elevation (m)')\n",
    "ax.set_ylabel('latitude [degrees]')\n",
    "ax.set_title('Ratio of snowmelt runoff onset medians by CHILI warm/cool classification\\nbinned by elevation and latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import pyarrow.parquet as pq\n",
    "# # import glob\n",
    "# # import os\n",
    "\n",
    "# #config.azure_blob_fs.download('snowmelt/analysis/tiles/','tiles/',recursive=True)\n",
    "\n",
    "# # for filepath in glob.glob('tiles/*'):\n",
    "# #     try:\n",
    "# #         pq.ParquetFile(filepath)\n",
    "# #     except Exception as e:\n",
    "# #         print(f'Error reading {filepath}: {e}')\n",
    "# #         os.remove(filepath)\n",
    "# # ddf = dd.read_parquet('tiles/')\n",
    "\n",
    "\n",
    "\n",
    "# #df = ddf[[\"original_lat\",\"original_lon\",\"runoff_onset_median\",\"dem\",\"aspect\"]].persist()\n",
    "\n",
    "# config.azure_blob_fs.ls('snowmelt/analysis/tiles/')\n",
    "\n",
    "# ddf = dd.read_parquet('snowmelt/analysis/tiles/tile_008*', filesystem=config.azure_blob_fs)\n",
    "# ddf\n",
    "\n",
    "# for col in ddf.columns:\n",
    "#     if col == 'hemisphere':\n",
    "#         continue\n",
    "#     print(f'for {col} with data type {ddf[col].dtype}')\n",
    "#     print(f'the mean is {ddf[col].mean().compute()}')\n",
    "#     print(f'the std is {ddf[col].std().compute()}')\n",
    "#     print(f'the min is {ddf[col].min().compute()}')\n",
    "#     print(f'the max is {ddf[col].max().compute()}')\n",
    "#     print(f'the median is {ddf[col].median_approximate().compute()}')\n",
    "#     print('---------------------------------')\n",
    "\n",
    "    \n",
    "# all_tiles_results_df = ddf[[\"original_lat\",\"runoff_onset_median\",\"dem\"]].repartition(partition_size=\"256 MiB\").persist()#.compute().repartition(partition_size=\"256 MiB\")\n",
    "# all_tiles_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
