{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SAR-derived runoff onset / snow pillow-derived runoff onset comparison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.dataframe as dd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import coiled\n",
    "import rasterio\n",
    "import xyzservices.providers as xyz\n",
    "import matplotlib.cm as cm\n",
    "import xyzservices as xyz\n",
    "import easysnowdata\n",
    "from global_snowmelt_runoff_onset.config import Config\n",
    "import global_snowmelt_runoff_onset.processing as processing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config('../config/global_config_v6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StationsWUS = easysnowdata.automatic_weather_stations.StationCollection()\n",
    "StationsWUS.get_entire_data_archive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles_with_stations_gdf = gpd.sjoin(\n",
    "    config.valid_tiles_gdf,\n",
    "    StationsWUS.all_stations,\n",
    "    how='inner',\n",
    "    predicate='contains'\n",
    ")\n",
    "tiles_with_stations_gdf = tiles_with_stations_gdf.drop_duplicates(subset=['row','col'])\n",
    "tiles_with_stations_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_temporal_resolutions(gdf):\n",
    "    # Initialize results dictionary\n",
    "    yearly_means = {}\n",
    "    \n",
    "    # Calculate yearly weighted means\n",
    "    for year in range(2015, 2025):\n",
    "        tr_col = f'tr_{year}'\n",
    "        pix_col = f'pix_ct_{year}'\n",
    "        \n",
    "        if tr_col in gdf.columns and pix_col in gdf.columns:\n",
    "            # Filter out NaN values for this year\n",
    "            year_data = gdf[[tr_col, pix_col]].dropna()\n",
    "            if not year_data.empty:\n",
    "                weighted_mean = np.average(\n",
    "                    year_data[tr_col],\n",
    "                    weights=year_data[pix_col]\n",
    "                )\n",
    "                yearly_means[year] = weighted_mean\n",
    "    \n",
    "    # Calculate overall weighted mean\n",
    "    tr_cols = [f'tr_{year}' for year in range(2015, 2025)]\n",
    "    pix_cols = [f'pix_ct_{year}' for year in range(2015, 2025)]\n",
    "    \n",
    "    # Create DataFrame with all tr and pixel count values\n",
    "    all_data = pd.DataFrame({\n",
    "        'tr': gdf[tr_cols].values.flatten(),\n",
    "        'pix': gdf[pix_cols].values.flatten()\n",
    "    })\n",
    "    \n",
    "    # Remove rows where either tr or pix is NaN\n",
    "    all_data = all_data.dropna()\n",
    "    \n",
    "    overall_mean = np.average(all_data['tr'], weights=all_data['pix']) if not all_data.empty else np.nan\n",
    "    \n",
    "    return yearly_means, overall_mean\n",
    "\n",
    "# Calculate results\n",
    "yearly_means, overall_mean = calculate_temporal_resolutions(tiles_with_stations_gdf)\n",
    "\n",
    "# Print results\n",
    "for year, mean in yearly_means.items():\n",
    "    print(f\"Weighted average temporal resolution for {year}: {mean:.2f}\")\n",
    "print(f\"\\nOverall weighted average temporal resolution: {overall_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_WUS_da = StationsWUS.entire_data_archive['WTEQ'].sel(time=slice('2014-10-01','2024-09-30'))\n",
    "stations_WUS_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f,ax=plt.subplots(figsize=(20,80))\n",
    "# stations_WUS_WY2023_da = stations_WUS_da.where(stations_WUS_da.WY==2023,drop=True)\n",
    "# stations_WUS_WY2023_normalized_da = stations_WUS_WY2023_da/stations_WUS_WY2023_da.max(dim='time')\n",
    "\n",
    "\n",
    "# stations_WUS_WY2023_normalized_da.plot(ax=ax,vmin=0.5,vmax=1,cmap='gist_rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f,ax=plt.subplots(figsize=(40,200))\n",
    "\n",
    "# stations_WUS_da.sel(time=slice('2021-10-01','2022-09-30')).plot(ax=ax,vmax=1.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_WUS_da = stations_WUS_da.where(stations_WUS_da>=0)\n",
    "\n",
    "abs_diffs = np.abs(stations_WUS_da.diff(dim='time'))\n",
    "abs_diffs_forward = abs_diffs.shift(time=1)\n",
    "abs_diffs_backward = abs_diffs.shift(time=-1)\n",
    "jump_mask = (abs_diffs_forward < 0.2) & (abs_diffs_backward < 0.2)\n",
    "stations_WUS_da = stations_WUS_da.where(jump_mask)\n",
    "\n",
    "window=10\n",
    "valid_mask = ~np.isnan(stations_WUS_da)\n",
    "rolling_valid = valid_mask.rolling(time=window*2, center=True).sum()\n",
    "stations_WUS_da = stations_WUS_da.where(rolling_valid >= window)  \n",
    "\n",
    "\n",
    "def check_missing_data(group):\n",
    "    nov_to_apr_mask = group.time.dt.month.isin([11, 12, 1, 2, 3]) # not good!!!!! \n",
    "    filtered_group = group.where(nov_to_apr_mask,drop=True)\n",
    "    missing_data_counts = filtered_group.isnull().sum(dim='time')\n",
    "\n",
    "    missing_data = missing_data_counts > 30\n",
    "\n",
    "    valid_data = ~np.isnan(group)\n",
    "\n",
    "    def calc_location_gaps(location_data):\n",
    "        valid_indices = ~np.isnan(location_data)\n",
    "        if not np.any(valid_indices):\n",
    "            return True\n",
    "        valid_times = group.time.values[valid_indices]\n",
    "        gaps = np.diff(valid_times)\n",
    "        return np.any(gaps / np.timedelta64(1, 'D') > 10)\n",
    "    \n",
    "    large_gaps = xr.apply_ufunc(\n",
    "        calc_location_gaps,\n",
    "        group,\n",
    "        input_core_dims=[['time']],\n",
    "        vectorize=True,\n",
    "        output_dtypes=[bool]\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Check for proper seasonal evolution\n",
    "    if valid_data.any():\n",
    "        first_valid_idx = valid_data.argmax(dim='time')\n",
    "        last_valid_idx = valid_data[:, ::-1].argmax(dim='time')\n",
    "        first_valid = group.isel(time=first_valid_idx)\n",
    "        last_valid = group.isel(time=group.sizes['time'] - last_valid_idx - 1)\n",
    "        improper_evolution = (first_valid > 0.1) | (last_valid > 0.1)\n",
    "    else:\n",
    "        improper_evolution = True\n",
    "\n",
    "    # print(f'missing data count: {missing_data.sum().values}')\n",
    "    # print(f'large gaps count: {large_gaps.sum().values}')\n",
    "    # print(f'improper evolution count: {improper_evolution.sum().values}')\n",
    "\n",
    "    columns_to_nan = missing_data | large_gaps | improper_evolution\n",
    "    group[columns_to_nan] = np.nan\n",
    "    return group\n",
    "\n",
    "def check_seasonal_snow_swe(group):\n",
    "    # Count days with SWE >= 0.05 in each window\n",
    "    sufficient_swe = (group >= 0.05).rolling(time=60, center=True, min_periods=55).sum() #0.01 CHANGED TO 0.2, 20 CM!!!!!!!!!!!\n",
    "    # Find locations that meet the criteria in any window\n",
    "    columns_to_keep = (sufficient_swe >= 55).any(dim='time')\n",
    "    # Mask out columns that don't meet criteria\n",
    "    columns_to_nan = ~columns_to_keep\n",
    "    group[columns_to_nan] = np.nan\n",
    "    return group\n",
    "\n",
    "def find_pct_max_timing(da, pct, dim='time', skipna=True):\n",
    "    \"\"\"Find the time when SWE last crosses below a percentage of max SWE\"\"\"\n",
    "    max_val = da.max(dim=dim, skipna=skipna)\n",
    "    threshold = max_val * pct\n",
    "    # Create boolean mask of values above threshold\n",
    "    above_thresh = xr.where(da >= threshold, 1, np.nan)\n",
    "    # Find the last True value\n",
    "    return above_thresh.sel(time=slice(None, None, -1)).swap_dims({'time':'DOWY'}).idxmax(dim=\"DOWY\", skipna=True).drop_vars('WY').where(lambda x: x>0) #reversed to get last max value instead of first\n",
    "\n",
    "\n",
    "stations_WUS_da = stations_WUS_da.groupby('WY').apply(check_missing_data)\n",
    "stations_WUS_da = stations_WUS_da.groupby('WY').apply(check_seasonal_snow_swe)\n",
    "\n",
    "\n",
    "#stations_WUS_max_SWE_timing_da = stations_WUS_da.fillna(-9999).groupby(\"WY\").map(lambda x: x.sel(time=slice(None, None, -1)).idxmax(\"time\",skipna=True).DOWY.drop_vars('WY')).where(lambda x: x>0) #reversed to get last max value instead of first\n",
    "stations_WUS_max_SWE_timing_da = stations_WUS_da.groupby(\"WY\").map(lambda x: x.sel(time=slice(None, None, -1)).swap_dims({'time':'DOWY'}).idxmax(\"DOWY\",skipna=True).drop_vars('WY')).where(lambda x: x>0)\n",
    "runoff_onset_max_swe_timing_WUS_ds = stations_WUS_max_SWE_timing_da.to_dataset(name='station_max_SWE_timing')\n",
    "\n",
    "runoff_onset_max_swe_timing_WUS_ds[\"station_max_SWE_value\"] = stations_WUS_da.groupby(\"WY\").max()\n",
    "\n",
    "pct_list = [0.99, 0.95, 0.9, 0.5]\n",
    "for pct in pct_list:\n",
    "    pct_str = str(int(pct * 100))\n",
    "    runoff_onset_max_swe_timing_WUS_ds[f'station_max_SWE_{pct_str}pct_timing'] = stations_WUS_da.groupby(\"WY\").map(lambda x: find_pct_max_timing(x, pct)).where(lambda x: x>0)\n",
    "\n",
    "runoff_onset_max_swe_timing_WUS_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_onset_max_swe_timing_WUS_ds.count(dim='station').sel(buffer_radius=1000,fcf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax=plt.subplots(figsize=(10,200))\n",
    "#stations_WUS_max_SWE_timing_da.dropna(dim='station',how='any').plot(ax=ax)\n",
    "#stations_WUS_max_SWE_timing_da.plot(ax=ax)\n",
    "runoff_onset_max_swe_timing_WUS_ds['station_max_SWE_95pct_timing'].plot(ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runoff_onset_global_ds = xr.open_zarr(config.global_runoff_store, consolidated=True,decode_coords='all')\n",
    "runoff_onset_global_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations_gdf = StationsWUS.all_stations\n",
    "# stations_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_gdf = gpd.read_file('~/repos/updated_snotel_locations/snotel_stations_with_updated_locations.geojson').set_index('code')\n",
    "stations_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_gdf(stations_gdf, station_code, buffer_radius=None):\n",
    "    station_gdf = stations_gdf[stations_gdf.index==station_code]\n",
    "    station_epsg = station_gdf.estimate_utm_crs().to_epsg()\n",
    "    station_gdf = station_gdf.to_crs(epsg=station_epsg)\n",
    "    if buffer_radius:\n",
    "        station_gdf['geometry'] = station_gdf.geometry.buffer(buffer_radius)\n",
    "    return station_gdf\n",
    "\n",
    "\n",
    "\n",
    "def get_station_buffered_runoff_onset(runoff_onset_WUS_ds, station_gdf):\n",
    "\n",
    "    runoff_onset_station_ds = runoff_onset_WUS_ds.rio.clip_box(*station_gdf.total_bounds, crs=station_gdf.crs).rio.reproject(station_gdf.crs)\n",
    "    \n",
    "\n",
    "    fcf_da = easysnowdata.remote_sensing.get_forest_cover_fraction(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'),mask_nodata=True).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.bilinear)\n",
    "    runoff_onset_station_ds['fcf'] = fcf_da\n",
    "\n",
    "    dem_da = easysnowdata.topography.get_copernicus_dem(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'), resolution=30).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.bilinear)\n",
    "    runoff_onset_station_ds['dem'] = dem_da\n",
    "\n",
    "    esa_da =easysnowdata.remote_sensing.get_esa_worldcover(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'),mask_nodata=True).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.nearest)\n",
    "    runoff_onset_station_ds['worldcover'] = esa_da\n",
    "\n",
    "    runoff_onset_station_ds = runoff_onset_station_ds.rio.clip(station_gdf.geometry) \n",
    "\n",
    "    #runoff_onset_station_ds = runoff_onset_station_ds.rio.clip(station_gdf.geometry,all_touched=True) \n",
    "\n",
    "    return runoff_onset_station_ds\n",
    "\n",
    "\n",
    "def get_station_buffered_runoff_onset_delayed(station_gdf):\n",
    "\n",
    "    runoff_onset_WUS_ds = xr.open_zarr(config.global_runoff_store, consolidated=True,decode_coords='all')\n",
    "\n",
    "    runoff_onset_station_ds = runoff_onset_WUS_ds.rio.clip_box(*station_gdf.total_bounds, crs=station_gdf.crs).rio.reproject(station_gdf.crs)\n",
    "    \n",
    "\n",
    "    fcf_da = easysnowdata.remote_sensing.get_forest_cover_fraction(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'),mask_nodata=True).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.bilinear)\n",
    "    runoff_onset_station_ds['fcf'] = fcf_da\n",
    "\n",
    "    dem_da = easysnowdata.topography.get_copernicus_dem(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'), resolution=30).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.bilinear)\n",
    "    runoff_onset_station_ds['dem'] = dem_da\n",
    "\n",
    "    esa_da =easysnowdata.remote_sensing.get_esa_worldcover(runoff_onset_station_ds.rio.transform_bounds('EPSG:4326'),mask_nodata=True).rio.reproject_match(runoff_onset_station_ds,resampling=rasterio.enums.Resampling.mode)\n",
    "    runoff_onset_station_ds['worldcover'] = esa_da\n",
    "\n",
    "    runoff_onset_station_ds = runoff_onset_station_ds.rio.clip(station_gdf.geometry) \n",
    "\n",
    "    #runoff_onset_station_ds = runoff_onset_station_ds.rio.clip(station_gdf.geometry,all_touched=True) \n",
    "\n",
    "    return runoff_onset_station_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create comparison dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = coiled.Cluster(idle_timeout=\"10 minutes\",\n",
    "                         #shutdown_on_close=False,\n",
    "                         #wait_for_workers=True,\n",
    "                         #n_workers=[41,170], # 170\n",
    "                         #n_workers=[31,86],\n",
    "                         n_workers=60,\n",
    "                         #n_workers=8,\n",
    "                         #n_workers=10,\n",
    "                         worker_memory=\"8 GB\", #coiled.list_instance_types(backend=\"azure\")\n",
    "                         worker_cpu=4,\n",
    "                         #worker_options={\"nthreads\": 1},\n",
    "                         #worker_options={\"nthreads\": 32},# 16 8 4 oversubscribe?\n",
    "                         #scheduler_memory=\"128 GB\",\n",
    "                         scheduler_memory=\"16 GB\",\n",
    "                         spot_policy=\"spot\", # spot usually\n",
    "                         #software=\"sar_snowmelt_timing\",\n",
    "                         environ={\"GDAL_DISABLE_READDIR_ON_OPEN\": \"EMPTY_DIR\"},\n",
    "                         #container=\"mcr.microsoft.com/planetary-computer/python:latest\",\n",
    "                         workspace=\"uwtacolab\",\n",
    "                         \n",
    "                         )\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "#use the following config for the problem tiles, otherwise 4 and 32, 8 and 32\n",
    "                        #  worker_memory=\"64 GB\", \n",
    "                        #  worker_cpu=8,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcf_thresh_values = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "buffer_radii = [100, 200, 300, 500, 1000]\n",
    "\n",
    "# Initialize output arrays\n",
    "runoff_onset_timing_WUS_ds = xr.DataArray(\n",
    "    data=np.nan,\n",
    "    dims=[\"station\",\"WY\",\"buffer_radius\",\"fcf\"],\n",
    "    coords={\n",
    "        \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "        \"WY\": runoff_onset_max_swe_timing_WUS_ds.WY,\n",
    "        \"buffer_radius\": buffer_radii,\n",
    "        \"fcf\": fcf_thresh_values\n",
    "    },\n",
    "    name=\"runoff_onset_timing\"\n",
    ")\n",
    "\n",
    "frac_valid_fcf_WUS_ds = xr.DataArray(\n",
    "    data=np.nan,\n",
    "    dims=[\"station\",\"buffer_radius\",\"fcf\"],\n",
    "    coords={\n",
    "        \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "        \"buffer_radius\": buffer_radii,\n",
    "        \"fcf\": fcf_thresh_values\n",
    "    },\n",
    "    name=\"pixel_count\"\n",
    ")\n",
    "\n",
    "mean_fcf_WUS_ds = xr.DataArray(\n",
    "    data=np.nan,\n",
    "    dims=[\"station\",\"buffer_radius\",\"fcf\"],\n",
    "    coords={\n",
    "        \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "        \"buffer_radius\": buffer_radii,\n",
    "        \"fcf\": fcf_thresh_values\n",
    "    },\n",
    "    name=\"mean_fcf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_station_buffer(station_code):\n",
    "    \"\"\"Process a single station-buffer combination and return results\"\"\"\n",
    "    # try:\n",
    "    results = []\n",
    "    max_buffer = max(buffer_radii)\n",
    "    station_gdf_max = get_station_gdf(stations_gdf, station_code, max_buffer)\n",
    "    runoff_onset_station_ds = get_station_buffered_runoff_onset_delayed(station_gdf_max).compute()\n",
    "\n",
    "    for buffer_radius in buffer_radii:\n",
    "        station_gdf = get_station_gdf(stations_gdf,station_code, buffer_radius)\n",
    "        clipped_ds = runoff_onset_station_ds.rio.clip(station_gdf.geometry)\n",
    "        \n",
    "        \n",
    "        for fcf_thresh in fcf_thresh_values:\n",
    "            # if we want to mask out tree cover from ESA worldcover\n",
    "            #masked_data = clipped_ds['runoff_onset'].where(clipped_ds['fcf'] <= fcf_thresh).where((clipped_ds['worldcover']!=80) & (clipped_ds['worldcover']!=50) & (clipped_ds['worldcover']!=10))\n",
    "            masked_data = clipped_ds['runoff_onset'].where(clipped_ds['fcf'] <= fcf_thresh).where((clipped_ds['worldcover']!=80) & (clipped_ds['worldcover']!=50))\n",
    "\n",
    "            result = {\n",
    "                'station': station_code,\n",
    "                'buffer_radius': buffer_radius,\n",
    "                'fcf': fcf_thresh,\n",
    "                'runoff_onset': masked_data.median(dim=['x', 'y']).rename({'water_year':'WY'}),\n",
    "                # 'frac_valid': (clipped_ds['fcf'].where(clipped_ds['fcf'] <= fcf_thresh).where((clipped_ds['worldcover']!=80) & (clipped_ds['worldcover']!=50) & (clipped_ds['worldcover']!=10)).count(dim=['x', 'y'])/\n",
    "                #             clipped_ds['fcf'].count(dim=['x', 'y'])),\n",
    "                'frac_valid': (clipped_ds['fcf'].where(clipped_ds['fcf'] <= fcf_thresh).where((clipped_ds['worldcover']!=80) & (clipped_ds['worldcover']!=50)).count(dim=['x', 'y'])/\n",
    "                            clipped_ds['fcf'].count(dim=['x', 'y'])),\n",
    "                'mean_fcf': clipped_ds['fcf'].where(clipped_ds['fcf'] <= fcf_thresh).mean(dim=['x', 'y'])\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error processing station {station_code}: {e}\")\n",
    "    #     return None\n",
    "\n",
    "\n",
    "futures = [client.submit(process_station_buffer, station_code, retries=100) for station_code in runoff_onset_max_swe_timing_WUS_ds.station.values]\n",
    "futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stati = [future.status for future in futures]\n",
    "np.unique(stati,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.gather(futures)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill arrays with results\n",
    "for result_group in results:\n",
    "    if result_group is not None:\n",
    "        for result in result_group:\n",
    "            station = result['station']\n",
    "            buffer_radius = result['buffer_radius']\n",
    "            fcf = result['fcf']\n",
    "            runoff_onset_timing_WUS_ds.loc[dict(station=station, buffer_radius=buffer_radius, fcf=fcf)] = result['runoff_onset']\n",
    "            frac_valid_fcf_WUS_ds.loc[dict(station=station, buffer_radius=buffer_radius, fcf=fcf)] = result['frac_valid']\n",
    "            mean_fcf_WUS_ds.loc[dict(station=station, buffer_radius=buffer_radius, fcf=fcf)] = result['mean_fcf']\n",
    "\n",
    "# Final assignments\n",
    "runoff_onset_max_swe_timing_WUS_ds = runoff_onset_max_swe_timing_WUS_ds.assign(\n",
    "    runoff_onset_timing=runoff_onset_timing_WUS_ds,\n",
    "    frac_valid_fcf=frac_valid_fcf_WUS_ds,\n",
    "    mean_fcf=mean_fcf_WUS_ds\n",
    ")\n",
    "\n",
    "runoff_onset_max_swe_timing_WUS_ds['sar_minus_stations'] = (\n",
    "    runoff_onset_max_swe_timing_WUS_ds['runoff_onset_timing'] - \n",
    "    runoff_onset_max_swe_timing_WUS_ds['station_max_SWE_timing']\n",
    ")\n",
    "\n",
    "for pct in pct_list:\n",
    "    pct_str = str(int(pct * 100))\n",
    "    runoff_onset_max_swe_timing_WUS_ds[f'sar_minus_{pct_str}pct'] = (\n",
    "        runoff_onset_max_swe_timing_WUS_ds['runoff_onset_timing'] - runoff_onset_max_swe_timing_WUS_ds[f'station_max_SWE_{pct_str}pct_timing'])\n",
    "    \n",
    "\n",
    "runoff_onset_max_swe_timing_WUS_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runoff_onset_max_swe_timing_WUS_ds.drop_vars('geometry').to_netcdf(f'station_comparison/snotel_sar_differences_UPDATED_SNOTEL_LOCATIONS_v5.nc')\n",
    "#runoff_onset_max_swe_timing_WUS_ds.drop_vars('geometry').to_netcdf(f'station_comparison/snotel_sar_differences_treemask_UPDATED_SNOTEL_LOCATIONS_andreq20cmSWEfor60days_v5.nc')\n",
    "runoff_onset_max_swe_timing_WUS_ds.drop_vars('geometry').to_netcdf(f'comparison_datasets/snotel_sar_differences_UPDATED_SNOTEL_LOCATIONS_andreq5cmSWEfor60days_v6.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fcf_thresh_values = [10,20,30,40,50,60,70,80,90,100]\n",
    "# buffer_radii = [50,100,300,500,1000]\n",
    "\n",
    "# def process_single_station(station_code):\n",
    "#     fcf_thresh_values = [10,20,30,40,50,60,70,80,90,100]\n",
    "#     buffer_radii = [50,100,300,500,1000]\n",
    "#     \"\"\"Process all buffer/fcf combinations for a single station\"\"\"\n",
    "#     station_results = {\n",
    "#         'runoff_onset': {},\n",
    "#         'frac_valid': {},\n",
    "#         'mean_fcf': {}\n",
    "#     }\n",
    "    \n",
    "#     runoff_onset_global_ds = xr.open_zarr(config.global_runoff_store, consolidated=True,decode_coords='all')\n",
    "    \n",
    "#     try:\n",
    "#         for buffer_radius in buffer_radii:\n",
    "#             station_gdf = get_station_gdf(station_code, buffer_radius)\n",
    "#             runoff_onset_station_ds = get_station_buffered_runoff_onset(\n",
    "#                 runoff_onset_global_ds, \n",
    "#                 station_gdf\n",
    "#             )\n",
    "            \n",
    "#             for fcf_thresh in fcf_thresh_values:\n",
    "#                 key = (station_code, buffer_radius, fcf_thresh)\n",
    "                \n",
    "#                 # Calculate metrics for this combination\n",
    "#                 runoff_onset = runoff_onset_station_ds['runoff_onset'].where(\n",
    "#                     runoff_onset_station_ds['fcf'] <= fcf_thresh\n",
    "#                 ).median(dim=['x', 'y']).rename({'water_year':'WY'})\n",
    "                \n",
    "#                 frac_valid = (runoff_onset_station_ds['fcf'].where(\n",
    "#                     runoff_onset_station_ds['fcf'] <= fcf_thresh\n",
    "#                 ).count(dim=['x', 'y'])/runoff_onset_station_ds['fcf'].count(dim=['x', 'y']))\n",
    "                \n",
    "#                 mean_fcf = runoff_onset_station_ds['fcf'].where(\n",
    "#                     runoff_onset_station_ds['fcf'] <= fcf_thresh\n",
    "#                 ).mean(dim=['x', 'y'])\n",
    "                \n",
    "#                 station_results['runoff_onset'][key] = runoff_onset\n",
    "#                 station_results['frac_valid'][key] = frac_valid\n",
    "#                 station_results['mean_fcf'][key] = mean_fcf\n",
    "                \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing station {station_code}: {e}\")\n",
    "        \n",
    "#     return station_results\n",
    "\n",
    "\n",
    "# station_codes = list(runoff_onset_max_swe_timing_WUS_ds.station.values)\n",
    "\n",
    "# # futures = []\n",
    "# # for station_code in runoff_onset_max_swe_timing_WUS_ds.station.values:\n",
    "# #     future = client.submit(\n",
    "# #         process_single_station,\n",
    "# #         station_code,\n",
    "# #     )\n",
    "# #     futures.append(future)\n",
    "\n",
    "# futures = [client.submit(process_single_station, station_code) for station_code in station_codes]\n",
    "\n",
    "# # Gather results\n",
    "# print(f\"Computing {len(futures)} station tasks in parallel...\")\n",
    "# results = client.gather(futures)\n",
    "# results\n",
    "\n",
    "\n",
    "# # Create output arrays with same structure as before\n",
    "# runoff_onset_timing = xr.DataArray(\n",
    "#     data=np.nan,\n",
    "#     dims=[\"station\", \"WY\", \"buffer_radius\", \"fcf\"],\n",
    "#     coords={\n",
    "#         \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "#         \"WY\": runoff_onset_max_swe_timing_WUS_ds.WY,\n",
    "#         \"buffer_radius\": buffer_radii,\n",
    "#         \"fcf\": fcf_thresh_values\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# frac_valid_fcf = xr.DataArray(\n",
    "#     data=np.nan,\n",
    "#     dims=[\"station\", \"buffer_radius\", \"fcf\"],\n",
    "#     coords={\n",
    "#         \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "#         \"buffer_radius\": buffer_radii,\n",
    "#         \"fcf\": fcf_thresh_values\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# mean_fcf = xr.DataArray(\n",
    "#     data=np.nan,\n",
    "#     dims=[\"station\", \"buffer_radius\", \"fcf\"],\n",
    "#     coords={\n",
    "#         \"station\": runoff_onset_max_swe_timing_WUS_ds.station,\n",
    "#         \"buffer_radius\": buffer_radii,\n",
    "#         \"fcf\": fcf_thresh_values\n",
    "#     }\n",
    "# )\n",
    "\n",
    "\n",
    "# # Populate arrays from results\n",
    "# for station_result in results:\n",
    "#     for key, onset in station_result['runoff_onset'].items():\n",
    "#         station, buffer, fcf = key\n",
    "#         runoff_onset_timing.loc[dict(station=station, buffer_radius=buffer, fcf=fcf)] = onset\n",
    "#         frac_valid_fcf.loc[dict(station=station, buffer_radius=buffer, fcf=fcf)] = station_result['frac_valid'][key]\n",
    "#         mean_fcf.loc[dict(station=station, buffer_radius=buffer, fcf=fcf)] = station_result['mean_fcf'][key]\n",
    "\n",
    "# # Create final dataset and save\n",
    "# runoff_onset_max_swe_timing_WUS_ds = runoff_onset_max_swe_timing_WUS_ds.assign(\n",
    "#     runoff_onset_timing=runoff_onset_timing,\n",
    "#     frac_valid_fcf=frac_valid_fcf,\n",
    "#     mean_fcf=mean_fcf\n",
    "# )\n",
    "\n",
    "# runoff_onset_max_swe_timing_WUS_ds['sar_minus_stations'] = (\n",
    "#     runoff_onset_max_swe_timing_WUS_ds['runoff_onset_timing'] - \n",
    "#     runoff_onset_max_swe_timing_WUS_ds['station_max_SWE_timing']\n",
    "# )\n",
    "\n",
    "# runoff_onset_max_swe_timing_WUS_ds\n",
    "\n",
    "# runoff_onset_max_swe_timing_WUS_ds.drop_vars('geometry').to_netcdf('snotel_sar_differences_vv_v4.nc')\n",
    "\n",
    "\n",
    "# fcf_thresh_values = [10,20,30,40,50,60,70,80,90,100]\n",
    "# buffer_radii = [100,200,300,500,1000]\n",
    "\n",
    "# runoff_onset_timing_WUS_ds = xr.DataArray(\n",
    "#     data=np.nan,  # Initialize with NaNs\n",
    "#     dims=[\"station\",\"WY\",\"buffer_radius\",\"fcf\"],\n",
    "#     coords={\"station\": runoff_onset_max_swe_timing_WUS_ds.station, \"WY\": runoff_onset_max_swe_timing_WUS_ds.WY, \"buffer_radius\": buffer_radii, \"fcf\": fcf_thresh_values},\n",
    "#     name=\"runoff_onset_timing\"\n",
    "# )\n",
    "\n",
    "# frac_valid_fcf_WUS_ds = xr.DataArray(\n",
    "#     data=np.nan,  # Initialize with NaNs\n",
    "#     dims=[\"station\",\"buffer_radius\",\"fcf\"],\n",
    "#     coords={\"station\": runoff_onset_max_swe_timing_WUS_ds.station, \"buffer_radius\": buffer_radii, \"fcf\": fcf_thresh_values},\n",
    "#     name=\"pixel_count\"\n",
    "# )\n",
    "\n",
    "# mean_fcf_WUS_ds = xr.DataArray(\n",
    "#     data=np.nan,  # Initialize with NaNs\n",
    "#     dims=[\"station\",\"buffer_radius\",\"fcf\"],\n",
    "#     coords={\"station\": runoff_onset_max_swe_timing_WUS_ds.station, \"buffer_radius\": buffer_radii, \"fcf\": fcf_thresh_values},\n",
    "#     name=\"mean_fcf\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Iterate over each station\n",
    "# for station_code in tqdm.tqdm(runoff_onset_max_swe_timing_WUS_ds.station.values):\n",
    "#     print(f'Processing station {station_code}')\n",
    "#     for buffer_radius in buffer_radii:\n",
    "#         print(f'Processing buffer radius {buffer_radius}')\n",
    "#         try: \n",
    "#             station_gdf = get_station_gdf(station_code, buffer_radius)\n",
    "\n",
    "#             runoff_onset_station_ds = get_station_buffered_runoff_onset(runoff_onset_global_ds, station_gdf)\n",
    "#             runoff_onset_station_ds = runoff_onset_station_ds.where(runoff_onset_station_ds['worldcover']!=80)\n",
    "            \n",
    "#             # Assign the calculated timing to the new DataArray\n",
    "#             for fcf_thresh in fcf_thresh_values:\n",
    "#                 runoff_onset_timing_WUS_ds.loc[dict(station=station_code,buffer_radius=buffer_radius,fcf=fcf_thresh)] = runoff_onset_station_ds['runoff_onset'].where(runoff_onset_station_ds['fcf'] <= fcf_thresh).median(dim=['x', 'y']).rename({'water_year':'WY'})\n",
    "#                 frac_valid_fcf_WUS_ds.loc[dict(station=station_code,buffer_radius=buffer_radius,fcf=fcf_thresh)] = (runoff_onset_station_ds['fcf'].where(runoff_onset_station_ds['fcf'] <= fcf_thresh).count(dim=['x', 'y'])/runoff_onset_station_ds['fcf'].count(dim=['x', 'y']))\n",
    "#                 mean_fcf_WUS_ds.loc[dict(station=station_code,buffer_radius=buffer_radius,fcf=fcf_thresh)] = runoff_onset_station_ds['fcf'].where(runoff_onset_station_ds['fcf'] <= fcf_thresh).mean(dim=['x', 'y'])\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "\n",
    "# runoff_onset_max_swe_timing_WUS_ds = runoff_onset_max_swe_timing_WUS_ds.assign(runoff_onset_timing=runoff_onset_timing_WUS_ds)\n",
    "# runoff_onset_max_swe_timing_WUS_ds = runoff_onset_max_swe_timing_WUS_ds.assign(frac_valid_fcf=frac_valid_fcf_WUS_ds)\n",
    "# runoff_onset_max_swe_timing_WUS_ds = runoff_onset_max_swe_timing_WUS_ds.assign(mean_fcf=mean_fcf_WUS_ds)\n",
    "# runoff_onset_max_swe_timing_WUS_ds['sar_minus_stations'] = runoff_onset_max_swe_timing_WUS_ds['runoff_onset_timing'] - runoff_onset_max_swe_timing_WUS_ds['station_max_SWE_timing']\n",
    "# runoff_onset_max_swe_timing_WUS_ds.drop_vars('geometry').to_netcdf(f'snotel_sar_differences_vv_v4.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_snowmelt_runoff_onset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
