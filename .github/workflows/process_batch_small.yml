name: Process Batch Small (â‰¤256 tiles)

on:
  workflow_dispatch:
    inputs:
      which_tiles_to_process:
        description: 'Which tiles to process'
        required: false
        default: 'unprocessed'
        type: choice
        options:
        - all
        - processed
        - failed
        - unprocessed
        - unprocessed_and_failed
        - unprocessed_and_failed_weather_stations
      how_many:
        description: 'How many tiles to process (max 256 for this workflow)'
        required: false
        default: '10'
        type: string
      config_file:
        description: 'Config file to use (e.g., global_config_v9.txt)'
        required: false
        default: 'global_config_v9.txt'
        type: string
      batch_index:
        description: 'Batch index for reusable workflow calls (keep as -1)'
        required: false
        default: '-1'
        type: string

  # Must duplicate inputs for workflow_call (https://github.com/orgs/community/discussions/39357)
  workflow_call:
    inputs:
      which_tiles_to_process:
        type: string
        required: true
      how_many:
        type: string
        required: true
      config_file:
        type: string
        required: true
      batch_index:
        type: string
        required: false
        default: '-1'

env:
  AZURE_STORAGE_SAS_TOKEN: ${{ secrets.AZURE_STORAGE_SAS_TOKEN }}
  AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}

jobs:
  generate-tile-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.get-tiles.outputs.matrix }}
      tile-count: ${{ steps.get-tiles.outputs.count }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        miniforge-variant: Miniforge3
        channels: conda-forge,defaults
        channel-priority: true
        activate-environment: global_snowmelt_runoff_onset_actions
        environment-file: environment_github_actions.yml
        
    - name: Install package
      shell: bash -l {0}
      run: |
        pip install -e .
        
    - name: Get tiles for batch processing
      id: get-tiles
      shell: bash -l {0}
      run: |
        BATCH_INDEX="${{ inputs.batch_index }}"
        
        COUNT=$(python processing/scripts/get_tiles_for_batch.py \
          --config-file ${{ inputs.config_file }} \
          --which-tiles ${{ inputs.which_tiles_to_process }} \
          --how-many ${{ inputs.how_many }} \
          --batch-index $BATCH_INDEX \
          --output count)
        echo "count=$COUNT" >> $GITHUB_OUTPUT
        
        # Check if user should use Large Batch workflow instead (only for workflow_dispatch)
        if [ "$BATCH_INDEX" = "-1" ] && [ $COUNT -gt 256 ]; then
          echo "âš ï¸  WARNING: You're trying to process $COUNT tiles, but this workflow is limited to 256 tiles."
          echo "ðŸ”„ Consider using the 'Process Batch Large' workflow instead for better performance."
          echo "âŒ This workflow will fail due to GitHub Actions matrix job limits."
          exit 1
        fi
        
        MATRIX=$(python processing/scripts/get_tiles_for_batch.py \
          --config-file ${{ inputs.config_file }} \
          --which-tiles ${{ inputs.which_tiles_to_process }} \
          --how-many ${{ inputs.how_many }} \
          --batch-index $BATCH_INDEX \
          --output json)
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
        
        echo "âœ… Processing $COUNT tiles with Small Batch workflow"

  process-tiles:
    needs: generate-tile-matrix
    if: fromJson(needs.generate-tile-matrix.outputs.tile-count) > 0
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        tile: ${{ fromJson(needs.generate-tile-matrix.outputs.matrix) }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        miniforge-variant: Miniforge3
        channels: conda-forge,defaults
        channel-priority: true
        activate-environment: global_snowmelt_runoff_onset_actions
        environment-file: environment_github_actions.yml
        
    - name: Install package
      shell: bash -l {0}
      run: |
        pip install -e .
        
    - name: Process tile
      id: process_tile
      shell: bash -l {0}
      run: |
        python processing/scripts/process_single_tile.py \
          --tile-row ${{ matrix.tile.row }} \
          --tile-col ${{ matrix.tile.col }} \
          --config-file ${{ inputs.config_file }}
          
    - name: Create failure CSV if processing failed
      if: failure() && steps.process_tile.outcome == 'failure'
      shell: bash -l {0}
      run: |
        python -c "
        import sys
        from pathlib import Path
        
        # Add the parent directory to Python path to import our modules
        sys.path.append(str(Path.cwd()))
        
        from global_snowmelt_runoff_onset.config import Config
        import pandas as pd
        import os
        from datetime import datetime
        
        # Load config to get proper field structure
        config_file = '${{ inputs.config_file }}'
        config = Config(f'config/{config_file}')
        
        # Create a failure tile object with the proper structure
        tile = config.get_tile(${{ matrix.tile.row }}, ${{ matrix.tile.col }})
        tile.start_time = datetime.now().isoformat()
        tile.total_time = 0.0
        tile.success = False
        tile.error_messages = ['GitHub Actions runner crashed or timed out']
        
        # Create row data using the same logic as successful saves
        def clean_value(value):
            import math
            if value is None:
                return ''
            if isinstance(value, float) and math.isnan(value):
                return ''
            if isinstance(value, str) and value.lower() == 'nan':
                return ''
            return value
        
        row_data = {field: clean_value(getattr(tile, field, None)) for field in config.fields}
        
        # Create output directory
        output_dir = 'processing/tile_data/github_workflow_results'
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract version from config file name
        config_version = config_file.replace('global_config_', '').replace('.txt', '')
        
        # Save CSV using same logic as successful case
        csv_path = f'{output_dir}/tile_{tile.row}_{tile.col}_{config_version}.csv'
        df = pd.DataFrame([row_data])
        df.to_csv(csv_path, index=False)
        print(f'Created failure CSV: {csv_path}')
        "
          
    - name: Upload tile result as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: batch-tile-result-${{ matrix.tile.row }}-${{ matrix.tile.col }}
        path: processing/tile_data/github_workflow_results/tile_${{ matrix.tile.row }}_${{ matrix.tile.col }}_*.csv
        retention-days: 30
        if-no-files-found: ignore
