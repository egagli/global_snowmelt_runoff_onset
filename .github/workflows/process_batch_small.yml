name: Process Batch Small (≤256 tiles)

on:
  workflow_dispatch:
    inputs:
      which_tiles_to_process:
        description: 'Which tiles to process'
        required: false
        default: 'unprocessed'
        type: choice
        options:
        - all
        - processed
        - failed
        - unprocessed
        - unprocessed_and_failed
        - unprocessed_and_failed_weather_stations
      how_many:
        description: 'How many tiles to process (max 256 for this workflow)'
        required: false
        default: '10'
        type: string
      config_file:
        description: 'Config file to use (e.g., global_config_v9.txt)'
        required: false
        default: 'global_config_v9.txt'
        type: string
      batch_index:
        description: 'Batch index for reusable workflow calls (keep as -1)'
        required: false
        default: '-1'
        type: string

  # Must duplicate inputs for workflow_call (https://github.com/orgs/community/discussions/39357)
  workflow_call:
    inputs:
      which_tiles_to_process:
        type: string
        required: true
      how_many:
        type: string
        required: true
      config_file:
        type: string
        required: true
      batch_index:
        type: string
        required: false
        default: '-1'

env:
  AZURE_STORAGE_SAS_TOKEN: ${{ secrets.AZURE_STORAGE_SAS_TOKEN }}
  AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}

jobs:
  generate-tile-matrix:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.get-tiles.outputs.matrix }}
      tile-count: ${{ steps.get-tiles.outputs.count }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        miniforge-variant: Miniforge3
        channels: conda-forge,defaults
        channel-priority: true
        activate-environment: global_snowmelt_runoff_onset_actions
        environment-file: environment_github_actions.yml
        
    - name: Install package
      shell: bash -l {0}
      run: |
        pip install -e .
        
    - name: Get tiles for batch processing
      id: get-tiles
      shell: bash -l {0}
      run: |
        BATCH_INDEX="${{ inputs.batch_index }}"
        
        COUNT=$(python processing/scripts/get_tiles_for_batch.py \
          --config-file ${{ inputs.config_file }} \
          --which-tiles ${{ inputs.which_tiles_to_process }} \
          --how-many ${{ inputs.how_many }} \
          --batch-index $BATCH_INDEX \
          --output count)
        echo "count=$COUNT" >> $GITHUB_OUTPUT
        
        # Check if user should use Large Batch workflow instead (only for workflow_dispatch)
        if [ "$BATCH_INDEX" = "-1" ] && [ $COUNT -gt 256 ]; then
          echo "⚠️  WARNING: You're trying to process $COUNT tiles, but this workflow is limited to 256 tiles."
          echo "🔄 Consider using the 'Process Batch Large' workflow instead for better performance."
          echo "❌ This workflow will fail due to GitHub Actions matrix job limits."
          exit 1
        fi
        
        MATRIX=$(python processing/scripts/get_tiles_for_batch.py \
          --config-file ${{ inputs.config_file }} \
          --which-tiles ${{ inputs.which_tiles_to_process }} \
          --how-many ${{ inputs.how_many }} \
          --batch-index $BATCH_INDEX \
          --output json)
        echo "matrix=$MATRIX" >> $GITHUB_OUTPUT
        
        echo "✅ Processing $COUNT tiles with Small Batch workflow"

  process-tiles:
    needs: generate-tile-matrix
    if: fromJson(needs.generate-tile-matrix.outputs.tile-count) > 0
    runs-on: ubuntu-latest
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        tile: ${{ fromJson(needs.generate-tile-matrix.outputs.matrix) }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Miniforge
      uses: conda-incubator/setup-miniconda@v3
      with:
        miniforge-version: latest
        miniforge-variant: Miniforge3
        channels: conda-forge,defaults
        channel-priority: true
        activate-environment: global_snowmelt_runoff_onset_actions
        environment-file: environment_github_actions.yml
        
    - name: Install package
      shell: bash -l {0}
      run: |
        pip install -e .
        
    - name: Process tile
      id: process_tile
      shell: bash -l {0}
      run: |
        python processing/scripts/process_single_tile.py \
          --tile-row ${{ matrix.tile.row }} \
          --tile-col ${{ matrix.tile.col }} \
          --config-file ${{ inputs.config_file }}
          
    - name: Create failure CSV if processing failed
      if: failure() && steps.process_tile.outcome == 'failure'
      shell: bash -l {0}
      run: |
        python -c "
        import sys
        from pathlib import Path
        
        # Add the parent directory to Python path to import our modules
        sys.path.append(str(Path.cwd()))
        
        from global_snowmelt_runoff_onset.config import Config
        import pandas as pd
        import os
        from datetime import datetime
        
        # Load config to get proper field structure
        config_file = '${{ inputs.config_file }}'
        config = Config(f'config/{config_file}')
        
        # Create a failure tile object with the proper structure
        tile = config.get_tile(${{ matrix.tile.row }}, ${{ matrix.tile.col }})
        tile.start_time = datetime.now().isoformat()
        tile.total_time = 0.0
        tile.success = False
        tile.error_messages = ['GitHub Actions runner crashed or timed out']
        
        # Create row data using the same logic as successful saves
        def clean_value(value):
            import math
            if value is None:
                return ''
            if isinstance(value, float) and math.isnan(value):
                return ''
            if isinstance(value, str) and value.lower() == 'nan':
                return ''
            return value
        
        row_data = {field: clean_value(getattr(tile, field, None)) for field in config.fields}
        
        # Create output directory
        output_dir = 'processing/tile_data/github_workflow_results'
        os.makedirs(output_dir, exist_ok=True)
        
        # Extract version from config file name
        config_version = config_file.replace('global_config_', '').replace('.txt', '')
        
        # Save CSV using same logic as successful case
        csv_path = f'{output_dir}/tile_{tile.row}_{tile.col}_{config_version}.csv'
        df = pd.DataFrame([row_data])
        df.to_csv(csv_path, index=False)
        print(f'Created failure CSV: {csv_path}')
        "
          
    - name: Upload tile result as artifact
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: batch-tile-result-${{ matrix.tile.row }}-${{ matrix.tile.col }}
        path: processing/tile_data/github_workflow_results/tile_${{ matrix.tile.row }}_${{ matrix.tile.col }}_*.csv
        retention-days: 30
        if-no-files-found: ignore
