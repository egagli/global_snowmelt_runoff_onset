name: Consolidate Tile Results

on:
  workflow_dispatch:
    inputs:
      days_back:
        description: 'How many days back to look for artifacts'
        required: false
        default: '7'
        type: string
      config_version:
        description: 'Config version (e.g., v9)'
        required: false
        default: 'v9'
        type: string

env:
  AZURE_STORAGE_SAS_TOKEN: ${{ secrets.AZURE_STORAGE_SAS_TOKEN }}
  AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}

permissions:
  contents: write
  actions: read  # Need this to download artifacts

jobs:
  consolidate-results:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install pandas requests
        
    - name: Download and consolidate artifacts
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python << 'EOF'
        import os
        import requests
        import json
        import pandas as pd
        from pathlib import Path
        import zipfile
        from datetime import datetime, timedelta
        import tempfile
        import shutil
        
        # Configuration
        repo = "${{ github.repository }}"
        token = os.environ['GITHUB_TOKEN']
        days_back = int("${{ github.event.inputs.days_back || '7' }}")
        config_version = "${{ github.event.inputs.config_version || 'v9' }}"
        
        # Calculate date threshold
        threshold_date = datetime.now() - timedelta(days=days_back)
        
        # GitHub API headers
        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        print(f"üîç Looking for tile result artifacts from the last {days_back} days...")
        
        # Get all artifacts from the repository
        artifacts_url = f"https://api.github.com/repos/{repo}/actions/artifacts"
        response = requests.get(artifacts_url, headers=headers)
        
        if response.status_code != 200:
            print(f"‚ùå Failed to fetch artifacts: {response.status_code}")
            print(response.text)
            exit(1)
        
        artifacts_data = response.json()
        
        # Filter for tile result artifacts
        tile_artifacts = []
        for artifact in artifacts_data['artifacts']:
            artifact_name = artifact['name']
            created_at = datetime.strptime(artifact['created_at'], '%Y-%m-%dT%H:%M:%SZ')
            
            # Check if it's a tile result artifact and within date range
            if (artifact_name.startswith('tile-result-') or 
                artifact_name.startswith('batch-tile-result-')) and created_at > threshold_date:
                tile_artifacts.append(artifact)
        
        print(f"üì¶ Found {len(tile_artifacts)} tile result artifacts")
        
        if not tile_artifacts:
            print("‚úÖ No new artifacts to consolidate")
            exit(0)
        
        # Create temp directory for downloads
        temp_dir = tempfile.mkdtemp()
        consolidated_data = []
        processed_tiles = set()
        
        try:
            for i, artifact in enumerate(tile_artifacts):
                print(f"üì• Downloading artifact {i+1}/{len(tile_artifacts)}: {artifact['name']}")
                
                # Download artifact
                download_url = artifact['archive_download_url']
                download_response = requests.get(download_url, headers=headers)
                
                if download_response.status_code == 200:
                    # Save and extract zip file
                    zip_path = os.path.join(temp_dir, f"{artifact['name']}.zip")
                    with open(zip_path, 'wb') as f:
                        f.write(download_response.content)
                    
                    # Extract and process CSV files
                    extract_dir = os.path.join(temp_dir, artifact['name'])
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_dir)
                    
                    # Find CSV files in extracted content
                    csv_files = list(Path(extract_dir).rglob('*.csv'))
                    
                    for csv_file in csv_files:
                        try:
                            df = pd.read_csv(csv_file)
                            if not df.empty:
                                # Extract tile coordinates from filename or data
                                for _, row in df.iterrows():
                                    tile_key = (row.get('row'), row.get('col'))
                                    if tile_key not in processed_tiles:
                                        consolidated_data.append(row.to_dict())
                                        processed_tiles.add(tile_key)
                                print(f"  ‚úÖ Processed {len(df)} records from {csv_file.name}")
                        except Exception as e:
                            print(f"  ‚ö†Ô∏è  Error processing {csv_file}: {e}")
                else:
                    print(f"  ‚ùå Failed to download {artifact['name']}: {download_response.status_code}")
        
        finally:
            # Clean up temp directory
            shutil.rmtree(temp_dir)
        
        if not consolidated_data:
            print("‚úÖ No new data to consolidate")
            exit(0)
        
        print(f"üìä Consolidated {len(consolidated_data)} unique tile results")
        
        # Create consolidated DataFrame
        new_df = pd.DataFrame(consolidated_data)
        
        # Path to main results CSV
        main_csv_path = f"processing/tile_data/tile_results_{config_version}.csv"
        
        # Load existing data if it exists
        if os.path.exists(main_csv_path):
            try:
                existing_df = pd.read_csv(main_csv_path)
                print(f"üìã Loaded existing CSV with {len(existing_df)} records")
                
                # Remove duplicates - keep the newest version of each tile
                combined_df = pd.concat([existing_df, new_df], ignore_index=True)
                
                # Sort by processing time if available, otherwise by row order
                if 'total_time' in combined_df.columns:
                    combined_df = combined_df.sort_values('total_time', ascending=False)
                
                # Keep only the latest result for each tile
                final_df = combined_df.drop_duplicates(subset=['row', 'col'], keep='first')
                
                print(f"üîÑ After deduplication: {len(final_df)} total records")
            except Exception as e:
                print(f"‚ö†Ô∏è  Error reading existing CSV: {e}")
                final_df = new_df
        else:
            print("üìÑ Creating new main CSV file")
            final_df = new_df
        
        # Create output directory if it doesn't exist
        os.makedirs(os.path.dirname(main_csv_path), exist_ok=True)
        
        # Save consolidated results
        final_df.to_csv(main_csv_path, index=False)
        print(f"üíæ Saved consolidated results to {main_csv_path}")
        
        # Create summary
        success_count = len(final_df[final_df.get('success', False) == True])
        failure_count = len(final_df[final_df.get('success', False) == False])
        
        print(f"""
        üìà CONSOLIDATION SUMMARY:
        ========================
        ‚Ä¢ Total tiles: {len(final_df)}
        ‚Ä¢ Successful: {success_count}
        ‚Ä¢ Failed: {failure_count}
        ‚Ä¢ Success rate: {success_count/len(final_df)*100:.1f}%
        ‚Ä¢ New artifacts processed: {len(tile_artifacts)}
        ‚Ä¢ New records added: {len(consolidated_data)}
        """)
        
        # Save summary to file for commit message
        with open('consolidation_summary.txt', 'w') as f:
            f.write(f"Consolidated {len(consolidated_data)} new tile results from {len(tile_artifacts)} artifacts\n")
            f.write(f"Total tiles: {len(final_df)} (Success: {success_count}, Failed: {failure_count})")
        
        EOF
        
    - name: Commit consolidated results
      run: |
        # Check if there are changes to commit
        if [[ -n "$(git status --porcelain processing/tile_data/tile_results_*.csv)" ]]; then
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action (Consolidation)"
          
          # Read summary for commit message
          if [ -f consolidation_summary.txt ]; then
            SUMMARY=$(cat consolidation_summary.txt)
          else
            SUMMARY="Consolidated tile results"
          fi
          
          git add processing/tile_data/tile_results_*.csv
          git commit -m "üîÑ $SUMMARY"
          
          # Push with retry logic
          for i in {1..3}; do
            if git push; then
              echo "‚úÖ Successfully pushed consolidated results"
              break
            else
              echo "‚ö†Ô∏è  Push failed (attempt $i/3), retrying in 5 seconds..."
              sleep 5
              git pull --rebase origin main || true
            fi
            
            if [ $i -eq 3 ]; then
              echo "‚ùå Failed to push after 3 attempts"
              exit 1
            fi
          done
        else
          echo "‚úÖ No changes to commit - all artifacts already consolidated"
        fi
        
    - name: Cleanup old artifacts (optional)
      if: github.event_name == 'schedule'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        python << 'EOF'
        import os
        import requests
        from datetime import datetime, timedelta
        
        # Only run cleanup on scheduled runs, and only delete very old artifacts
        repo = "${{ github.repository }}"
        token = os.environ['GITHUB_TOKEN']
        
        # Delete artifacts older than 30 days
        threshold_date = datetime.now() - timedelta(days=30)
        
        headers = {
            'Authorization': f'token {token}',
            'Accept': 'application/vnd.github.v3+json'
        }
        
        # Get all artifacts
        artifacts_url = f"https://api.github.com/repos/{repo}/actions/artifacts"
        response = requests.get(artifacts_url, headers=headers)
        
        if response.status_code == 200:
            artifacts_data = response.json()
            deleted_count = 0
            
            for artifact in artifacts_data['artifacts']:
                if artifact['name'].startswith(('tile-result-', 'batch-tile-result-')):
                    created_at = datetime.strptime(artifact['created_at'], '%Y-%m-%dT%H:%M:%SZ')
                    
                    if created_at < threshold_date:
                        delete_url = f"https://api.github.com/repos/{repo}/actions/artifacts/{artifact['id']}"
                        delete_response = requests.delete(delete_url, headers=headers)
                        
                        if delete_response.status_code == 204:
                            deleted_count += 1
                            print(f"üóëÔ∏è  Deleted old artifact: {artifact['name']}")
            
            print(f"üßπ Cleaned up {deleted_count} old artifacts")
        EOF
